\documentclass[12pt,ngerman,]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{eurosym}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
  \newcommand{\euro}{€}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\PassOptionsToPackage{usenames,dvipsnames}{color} % color is loaded by hyperref
\hypersetup{unicode=true,
            pdftitle={Praxis der Datenanalyse},
            pdfauthor={Sebastian Sauer. Mit Beiträgen von Oliver Gansser, Matthias Gehrke, Karsten Lübke und Norman Markgraf},
            colorlinks=true,
            linkcolor=Maroon,
            citecolor=Blue,
            urlcolor=Blue,
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[shorthands=off,main=ngerman]{babel}
\else
  \usepackage{polyglossia}
  \setmainlanguage[]{german}
\fi
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\usepackage[normalem]{ulem}
% avoid problems with \sout in headers with hyperref:
\pdfstringdefDisableCommands{\renewcommand{\sout}{}}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}
  \title{Praxis der Datenanalyse}
  \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
\subtitle{Skript zum Modul}
  \author{Sebastian Sauer. Mit Beiträgen von Oliver Gansser, Matthias Gehrke,
Karsten Lübke und Norman Markgraf}
  \preauthor{\centering\large\emph}
  \postauthor{\par}
  \predate{\centering\large\emph}
  \postdate{\par}
  \date{06 July, 2017}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage[bf,singlelinecheck=off]{caption}

%ses:
\pagestyle{headings}

%\setmainfont[UprightFeatures={SmallCapsFont=AlegreyaSC-Regular}]{Alegreya}

\usepackage{framed,color}
\definecolor{shadecolor}{RGB}{248,248,248}

\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}

%\renewenvironment{quote}{\begin{VF}}{\end{VF}}
\let\oldhref\href
\renewcommand{\href}[2]{#2\footnote{\url{#1}}}

\ifxetex
  \usepackage{letltxmacro}
  \setlength{\XeTeXLinkMargin}{1pt}
  \LetLtxMacro\SavedIncludeGraphics\includegraphics
  \def\includegraphics#1#{% #1 catches optional stuff (star/opt. arg.)
    \IncludeGraphicsAux{#1}%
  }%
  \newcommand*{\IncludeGraphicsAux}[2]{%
    \XeTeXLinkBox{%
      \SavedIncludeGraphics#1{#2}%
    }%
  }%
\fi

% Hier müsste noch das Chapter mit rein! Sonst gibt es Das nicht in der Ausgabe!
% Daher habe ich das auskommentiert! (NM)
%\renewcommand{\thesection}{\arabic{section}}  % ses


\makeatletter
\newenvironment{kframe}{%
\medskip{}
\setlength{\fboxsep}{.8em}
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\renewenvironment{Shaded}{\begin{kframe}}{\end{kframe}}

\newenvironment{rmdblock}[1]
  {
  \begin{itemize}
  \renewcommand{\labelitemi}{
    \raisebox{-.7\height}[0pt][0pt]{
      {\setkeys{Gin}{width=3em,keepaspectratio}\includegraphics{images/icons/#1}}
    }
  }
  \setlength{\fboxsep}{1em}
  \begin{kframe}
  \item
  }
  {
  \end{kframe}
  \end{itemize}
  }
\newenvironment{rmdnote}
  {\begin{rmdblock}{note}}
  {\end{rmdblock}}
\newenvironment{rmdcaution}
  {\begin{rmdblock}{caution}}
  {\end{rmdblock}}
\newenvironment{rmdimportant}
  {\begin{rmdblock}{important}}
  {\end{rmdblock}}
\newenvironment{rmdtip}
  {\begin{rmdblock}{tip}}
  {\end{rmdblock}}
\newenvironment{rmdwarning}
  {\begin{rmdblock}{warning}}
  {\end{rmdblock}}
\newenvironment{rmdpseudocode}
  {\begin{rmdblock}{pseudocode}}
  {\end{rmdblock}}
\newenvironment{rmdexercises}
  {\begin{rmdblock}{exercises}}
  {\end{rmdblock}}
\newenvironment{rmdlove}
  {\begin{rmdblock}{love}}
  {\end{rmdblock}}

\usepackage{makeidx}  % ses
\makeindex  % ses

\urlstyle{tt}

\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}


%NM:
\def\@makechapterhead#1{%
  \vspace*{50\p@}%
  {\parindent \z@ \raggedright \normalfont
  \ifnum \c@secnumdepth >\m@ne
    \if@mainmatter
        \huge\scshape\bfseries\scshape \@chapapp\space \thechapter
        \par\nobreak
        \vskip 20\p@
    \fi
  \fi
  \interlinepenalty\@M
  \Huge \scshape\bfseries\scshape #1\par\nobreak
  \vskip 40\p@
}}

\makeatother


% NM: Zu einem frontmatter gehört auch ein mainmatter, appendix und backmatter! ;-) - Habe ich daher mal eingefügt!
\frontmatter

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\let\BeginKnitrBlock\begin \let\EndKnitrBlock\end
\begin{document}
\maketitle

{
\hypersetup{linkcolor=black}
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoftables
\listoffigures
\newpage

\setcounter{chapter}{0}\chapter{Vorwort}

\begin{center}\includegraphics[width=0.5\linewidth]{images/icons/OER_Logo_201706_gruen} \end{center}

\begin{center}\includegraphics[width=0.1\linewidth]{images/licence} \end{center}

Statistik heute; was ist das? Sicherlich haben sich die Schwerpunkte von
``gestern'' zu ``heute'' verschoben. Wenig überraschend spielt der
Computer eine immer größere Rolle; die Daten werden vielseitiger und
massiger. Entsprechend sind neue Verfahren nötig - und vorhanden, in
Teilen - um auf diese neue Situation einzugehen. Einige Verfahren werden
daher weniger wichtig, z.B. der p-Wert oder der t-Test. Allerdings wird
vielfach, zumeist, noch die Verfahren gelehrt und verwendet, die für die
erste Hälfte des 20. Jahrhunderts entwickelt wurden. Eine Zeit, in der
kleine Daten, ohne Hilfe von Computern und basierend auf einer kleinen
Theoriefamilie im Rampenlicht standen (Cobb
\protect\hyperlink{ref-cobb2007introductory}{2007}). Die Zeiten haben
sich geändert!

\begin{center}\includegraphics[width=0.7\linewidth]{images/vorwort/Forschung_frueher_heute} \end{center}

Zu Themen, die heute zu den dynamischten Gebieten der Datenanalyse
gehören, die aber früher keine große Rolle spielten, gehören (Hardin
u.~a. \protect\hyperlink{ref-hardin2015data}{2015}):

\begin{itemize}
\tightlist
\item
  Nutzung von Datenbanken und anderen Data Warehouses
\item
  Daten aus dem Internet automatisch einlesen (``scraping'')
\item
  Genanalysen mit Tausenden von Variablen
\item
  Gesichtserkennung
\end{itemize}

Sie werden in diesem Kurs einige praktische Aspekte der modernen
Datenanalyse lernen. Ziel ist es, Sie - in Grundzügen - mit der Art und
Weise vertraut zu machen, wie angewandte Statistik bei führenden
Organisationen und Praktikern verwendet wird\footnote{Statistiker, die
  dabei als Vorbild Pate standen sind: Roger D. Peng:
  \url{http://www.biostat.jhsph.edu/~rpeng/}, Hadley Wickham:
  \url{http://hadley.nz}, Jennifer Bryan:
  \url{https://github.com/jennybc}}.

Es ist ein Grundlagenkurs; das didaktische Konzept beruht auf einem
induktiven, intuitiven Lehr-Lern-Ansatz. Formeln und mathematische
Hintergründe such man meist vergebens (tja).

Im Gegensatz zu anderen Statistik-Büchern steht hier die Umsetzung mit R
stark im Vordergrund. Dies hat pragmatische Gründe: Möchte man Daten
einer statistischen Analyse unterziehen, so muss man sie zumeist erst
aufbereiten; oft mühselig aufbereiten. Selten kann man den Luxus
genießen, einfach ``nur'', nach Herzenslust sozusagen, ein Feuerwerk an
multivariater Statistik abzubrennen. Zuvor gilt es, die Daten
aufzubereiten, umzuformen, zu prüfen und zusammenzufassen. Diesem Teil
ist hier recht ausführlich Rechnung getragen.

``Statistical thinking'' sollte, so eine verbreitete Idee, im Zentrum
oder als Ziel einer Statistik-Ausbildung stehen (Wild und Pfannkuch
\protect\hyperlink{ref-wild1999statistical}{1999}). Es ist die Hoffnung
der Autoren dieses Skripts, dass das praktische Arbeiten (im Gegensatz
zu einer theoretischen Fokus) zur Entwicklung einer Kompetenz im
statistischen Denken beiträgt.

Außerdem spielt in diesem Kurs die Visualisierung von Daten eine große
Rolle. Zum einen könnte der Grund einfach sein, dass Diagramme
ansprechen und gefallen (einigen Menschen). Zum anderen bieten Diagramme
bei umfangreichen Daten Einsichten, die sonst leicht wortwörtlich
überersehen würden.

\begin{quote}
Dieser Kurs zielt auf die praktischen Aspekte der Analyse von Daten ab:
``wie mache ich es?''; mathematische und philosophische Hintergründe
werden vernachlässigt bzw. auf einschlägige Literatur verwiesen.
\end{quote}

Dieses Skript ist publiziert unter
\href{https://creativecommons.org/licenses/by-nc-sa/3.0/de/}{CC-BY-NC-SA
3.0 DE}.

\begin{center}\includegraphics[width=0.1\linewidth]{images/licence} \end{center}

Sebastian Sauer

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Herausgeber: FOM Hochschule für Oekonomie \& Management
gemeinnützige GmbH}

Dieses Skript dient als Begleitmaterial zum Modul ``Praxis der
Datenanalyse'' des Masterstudiengangs ``Wirtschaftspsychologie \&
Consulting'' der FOM Hochschule für Oekonomie \& Management.

FOM. Die Hochschule. Für Berufstätige. Die mit bundesweit über 42.000
Studierenden größte private Hochschule Deutschlands führt seit 1993
Studiengang für Berufstätige durch, die einen staatlich und
international anerkannten Hochschulabschluss (Bachelor/Master) erlangen
wollen. Weitere Informationen finden Sie unter
\textless{}www.fom.de\textgreater{}

\newpage

\chapter{Organisatorisches}\label{organisatorisches}

\begin{center}\includegraphics[width=0.3\linewidth]{images/FOM} \end{center}

\begin{center}\includegraphics[width=0.1\linewidth]{images/licence} \end{center}

\section{Modulziele}\label{modulziele}

\BeginKnitrBlock{rmdcaution}
Die Studierenden können nach erfolgreichem Abschluss des Moduls:

\begin{itemize}
\tightlist
\item
  den Ablauf eines Projekts aus der Datenanalyse in wesentlichen
  Schritten nachvollziehen,
\item
  Daten aufbereiten und ansprechend visualisieren,
\item
  Inferenzstatistik anwenden und kritisch hinterfragen,
\item
  klassische Vorhersagemethoden (Regression) anwenden,
\item
  moderne Methoden der angewandten Datenanalyse anwenden (z.B.
  Textmining),
\item
  betriebswirtschaftliche Fragestellungen mittels datengetriebener
  Vorhersagemodellen beantworten.
\end{itemize}
\EndKnitrBlock{rmdcaution}

\section{Themen pro Termin}\label{themen-pro-termin}

Für dieses Modul sind 44 UE für Lehre eingeplant, aufgeteilt in 11
Termine (vgl. \ref{tab:termin-themen}).

Folgende Abfolge von Themen sind pro Termin vorgeschlagen:

Tabelle \ref{tab:termin-themen} ordnet die Themen des Moduls den
Terminen (1-11) zu.

\begin{table}

\caption{\label{tab:termin-themen}Zuordnung von Themen zu Terminen}
\centering
\begin{tabular}[t]{r|l}
\hline
Termin & Thema (Kapitel)\\
\hline
1 & Organisatorisches\\
\hline
1 & Einführung\\
\hline
1 & Rahmen\\
\hline
1 & Daten einlesen\\
\hline
2 & Datenjudo\\
\hline
3 & Daten visualisieren\\
\hline
4 & Fallstudie (z.B. zu 'movies')\\
\hline
5 & Daten modellieren\\
\hline
5 & Der p-Wert\\
\hline
6 & Lineare Regression - metrisch\\
\hline
7 & Lineare Regression - kategorial\\
\hline
8 & Fallstudie (z.B. zu 'titanic' und 'affairs')\\
\hline
9 & Vertiefung 1: Textmining oder Clusteranalyse\\
\hline
10 & Vertiefung 2: Dimensionsreduktion\\
\hline
11 & Wiederholung\\
\hline
\end{tabular}
\end{table}

\section{Vorerfahrung}\label{vorerfahrung}

Bei den Studierenden werden folgende Themen als bekannt vorausgesetzt:

\begin{itemize}
\tightlist
\item
  Deskriptive Statistik
\item
  Inferenzstatistik
\item
  Grundlagen R
\item
  Grundlagen der Datenvisualisierung
\end{itemize}

\section{Prüfung}\label{prufung}

\subsection{Prüfungshinweise}\label{prufungshinweise}

\begin{itemize}
\tightlist
\item
  Die Prüfung besteht aus zwei Teilen

  \begin{itemize}
  \tightlist
  \item
    einer Klausur (50\% der Teilnote)
  \item
    einer Datenanalyse (50\% der Teilnote).
  \end{itemize}
\end{itemize}

\emph{Prüfungsrelevant} ist der gesamte Stoff aus dem Skript und dem
Unterricht mit folgenden Ausnahmen:

\begin{itemize}
\tightlist
\item
  Inhalte/Abschnitte, die als ``nicht klausurrelevant'' gekennzeichnet
  sind,
\item
  Inhalte/Abschnitte, die als ``Vertiefung'' gekennzeichnet sind,
\item
  Fallstudien (nur für Klausuren nicht prüfungsrelevant),
\item
  die Inhalte von Links,
\item
  die Inhalte von Fußnoten,
\item
  die Kapitel \emph{Vorwort}, \emph{Organisatorisches} und
  \emph{Anhang}.
\end{itemize}

Alle Hinweise zur Prüfung gelten nur insoweit nicht anders vom Dozenten
festgelegt.

\subsection{Klausur}\label{klausur}

\begin{itemize}
\item
  Die Klausur besteht fast oder komplett aus Multiple-Choice
  (MC-)-Aufgaben mit mehreren Antwortoptionen (sofern nicht anders vom
  Dozenten vorgegeben).
\item
  Die (maximale) Anzahl der richtigen Aussagen ist pro Aufgabe
  angegeben. Werden mehr Aussagen als ``richtig'' angekreuzt als
  angegeben, so wird die Aufgabe mit 0 Punkten beurteilt. Ansonsten
  werden Teilpunkte für jede Aufgabe vergeben.
\item
  Jede Aussage gilt ceteris paribus (unter sonst gleichen Umständen).
  Aussagen der Art ``A ist B'' (z.B. ``Menschen sind sterblich'') sind
  \emph{nur} dann als richtig auszuwählen, wenn die Aussage \emph{immer}
  richtig ist.
\item
  Im Zweifel ist eine Aussage auf den Stoff, so wie im Unterricht
  behandelt, zu beziehen. Werden in Aussagen Zahlen abgefragt, so sind
  Antworten auch dann richtig, wenn die vorgeschlagene Antwort ab der 1.
  Dezimale von der wahren Antwort abweicht (einigermaßen genaue Aussagen
  werden als richtig akzeptiert). Bei Fragen zu R-Syntax spielen Aspekte
  wie Enter-Taste o.ä. bei der Beantwortung der Frage keine Rolle; diese
  Aspekte dürfen zu ignorieren.
\item
  Jede Aussage einer MC-Aufgabe ist entweder richtig oder falsch (aber
  nicht beides oder keines).
\item
  Die MC-Aufgaben sind nur mit Kreuzen zu beantworten; Text wird bei der
  Korrektur nicht berücksichtigt.
\item
  Bei Nachholklausuren gelten die selben Inhalte (inkl. Schwerpunkte)
  wie bei der Standard-Klausur, sofern nicht anderweitig angegeben.
\item
  I.d.R. sind nur Klausurpapier und ein nicht-programmierbarer
  Taschenrechner als Hilfsmittel zulässig.
\item
  Die Musterlösungen zu offenen Fragen sind elektronisch hinterlegt.
\end{itemize}

\subsection{Datenanalyse}\label{datenanalyse}

\subsubsection{Hinweise}\label{hinweise}

\begin{itemize}
\item
  Wenden Sie die passenden, im Modul eingeführten statistischen
  Verfahren an.
\item
  Werten Sie die Daten mit R aus; R-Syntax soll verwendet und im
  Hauptteil dokumentiert werden.
\item
  In der Wahl des Datensatzes sind Sie frei, mit folgender Ausnahme: Im
  Unterricht besprochene Datensätze dürfen nicht als Prüfungsleistung
  eingereicht werden (vgl. Abschnitt \ref{daten}).
\item
  Der (Original-)Name des Datensatzes (sowie ggf. Link) ist bei der
  Anmeldung anzugeben.
\item
  Gruppenarbeiten sind nicht zulässig.
\item
  Hat sich jemand schon für einen Datensatz angemeldet, so darf dieser
  Datensatz nicht mehr gewählt werden (``first come, first serve'').
\item
  Fundorte für Datensätze sind z.B.
  \href{http://www.stat.ufl.edu/~winner/datasets.html}{hier},
  \href{http://archive.ics.uci.edu/ml/datasets.html}{hier},
  \href{https://www.kaggle.com/datasets}{hier} und
  \href{http://vincentarelbundock.github.io/Rdatasets/datasets.html}{hier};
  im Internet finden sich viele Datensätze\footnote{Googeln Sie mal nach
    ``open datasets'' o.ä.}.
\item
  Schreiben Sie Ihre Ergebnisse in einer Ausarbeitung zusammen; der
  Umfang der Ausarbeitung umfasst ca. \emph{1500 Wörter} (nur Hauptteil;
  d.h. exklusive Deckblatt, Verzeichnisse, Anhang etc.).
\item
  Untersuchen Sie 2-3 Hypothesen.
\item
  Denken Sie daran, Name, Matrikelnummer, Modulname etc. anzugeben
  (Deckblatt). Bei der Gestaltung des Layout entscheiden Sie selbständig
  bitte nach Zweckmäßigkeit (und Ästhetik).
\item
  Fügen Sie keine Erklärungen oder Definitionen von statistischen
  Verfahren an.
\end{itemize}

\subsubsection{Gliederungsvorschlag zur
Datenanalyse}\label{gliederungsvorschlag-zur-datenanalyse}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Datensatz

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    Beschreibung

    \begin{itemize}
    \tightlist
    \item
      Name
    \item
      Hintergrund (Themengebiet, Theorien, Relevanz), ca. 100 Wörter
    \item
      Dimension (Zeilen*Spalten)
    \item
      Zitation (wenn vorhanden)
    \item
      sonstige Hinweise (z.B. Datenqualität, Entstehung des Datensatzes)
    \end{itemize}
  \item
    Variablendeskription (nur für Variablen der Hypothese)

    \begin{itemize}
    \tightlist
    \item
      Skalenniveaus
    \item
      Kontinuität (nur bei metrischen Variablen)
    \item
      R-Datentyp
    \item
      Anzahl Fälle und fehlende Werte
    \item
      Erläuterung der Variablen
    \end{itemize}
  \end{enumerate}
\item
  Deduktive Analyse

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    Hypothese(n) Beschreiben Sie die Vermutung(en), die Sie prüfen
    möchten, möglichst exakt.
  \item
    Deskriptive Statistiken

    \begin{itemize}
    \tightlist
    \item
      Berichten Sie deskriptive Statistiken für alle Variablen der
      Hypothesen.
    \item
      Berichten Sie aber nur univariate Statistiken sowie
      Subgruppenanalysen dazu.
    \item
      Berichten Sie ggf. Effektstärken.
    \end{itemize}
  \item
    Diagramme

    \begin{itemize}
    \tightlist
    \item
      Visualisieren Sie Ihre Hypothese(n) bzw. die Daten dazu, gerne aus
      mehreren Blickwinkeln.
    \end{itemize}
  \item
    Signifikanztest
  \end{enumerate}
\item
  Explorative Analyse
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Erörtern Sie interessante Einblicke, die über Ihre vorab getroffenen
  Hypothesen hinausgehen.
\item
  Diagramme können hier eine zentrale Rolle spielen.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  Diskussion

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    Zentrale Ergebnisse Fassen Sie das zentrale Ergebnisse zusammen.
  \item
    Interpretation Interpretieren Sie die Ergebnisse: Was bedeuten die
    Zahlen/Fakten, die die Rechnungen ergeben haben?
  \item
    Grenzen der Analyse

    \begin{itemize}
    \tightlist
    \item
      Schildern Sie etwaige Schwachpunkte oder Einschränkungen der
      Analyse.
    \item
      Geben Sie Anregungen für weiterführende Analysen dieses
      Datensatzes.
    \end{itemize}
  \end{enumerate}
\end{enumerate}

\section{Literatur}\label{literatur}

Zum Bestehen der Prüfung ist keine weitere Literatur formal notwendig;
allerdings ist es hilfreich, den Stoff aus unterschiedlichen
Blickwinkeln aufzuarbeiten. Dazu ist am ehesten das Buch von Wickham und
Grolemund (Wickham und Grolemund \protect\hyperlink{ref-r4ds}{2016})
hilfreich, obwohl es deutlich tiefer geht als dieses Skript.

\mainmatter

\setcounter{chapter}{0} \part{Grundlagen}

\chapter{Rahmen}\label{Rahmen}

\begin{center}\includegraphics[width=0.3\linewidth]{images/FOM} \end{center}

\begin{center}\includegraphics[width=0.1\linewidth]{images/licence} \end{center}

\BeginKnitrBlock{rmdcaution}
Lernziele:

\begin{itemize}
\tightlist
\item
  Einen Überblick über die fünf wesentliche Schritte der Datenanalyse
  gewinnen.
\item
  R und RStudio installieren können.
\item
  Einige häufige technische Probleme zu lösen wissen.
\item
  R-Pakete installieren können.
\item
  Einige grundlegende R-Funktionalitäten verstehen.
\item
  Auf die Frage ``Was ist Statistik?'' eine Antwort geben können.
\end{itemize}
\EndKnitrBlock{rmdcaution}

In diesem Skript geht es um die Praxis der Datenanalyse. Mit Rahmen ist
das ``Drumherum'' oder der Kontext der eigentlichen Datenanalyse
gemeint. Dazu gehören einige praktische Vorbereitungen und ein paar
Überlegungen. Zum Beispiel brauchen wir einen Überblick über das Thema.
Voilà (Abb. \ref{fig:fig-prozess}):

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/Rahmen/Prozess_Datenanalyse} 

}

\caption{Der Prozess der Datenanalyse}\label{fig:fig-prozess}
\end{figure}

Datenanalyse, praktisch betrachtet, kann man in fünf Schritte einteilen
(Wickham und Grolemund \protect\hyperlink{ref-r4ds}{2016}). Zuerst muss
man die Daten \emph{einlesen}, die Daten also in R (oder einer anderen
Software) verfügbar machen (laden). Fügen wir hinzu: In \emph{schöner
Form} verfügbar machen; man nennt dies auch \emph{tidy data} (hört sich
cooler an). Sobald die Daten in geeigneter Form in R geladen sind, folgt
das \emph{Aufbereiten}. Das beinhaltet Zusammenfassen, Umformen oder
Anreichern je nach Bedarf. Ein nächster wesentlicher Schritt ist das
\emph{Visualisieren} der Daten. Ein Bild sagt bekanntlich mehr als viele
Worte. Schließlich folgt das \emph{Modellieren} oder das Hypothesen
prüfen: Man überlegt sich, wie sich die Daten erklären lassen könnten.
Zu beachten ist, dass diese drei Schritte - Aufbereiten, Visualisieren,
Modellieren - keine starre Abfolge sind, sondern eher ein munteres
Hin-und-Her-Springen, ein aufbauendes Abwechseln. Der letzte Schritt ist
das \emph{Kommunizieren} der Ergebnisse der Analyse - nicht der Daten.
Niemand ist an Zahlenwüsten interessiert; es gilt, spannende Einblicke
zu vermitteln.

Der Prozess der Datenanalyse vollzieht sich nicht im luftleeren Raum,
sondern ist in einem \emph{Rahmen} eingebettet. Dieser beinhaltet
praktische Aspekte - wie Software, Datensätze - und grundsätzliche
Überlegungen - wie Ziele und Grundannahmen.

\section{Software installieren}\label{software-installieren}

Als Haupt-Analysewerkzeug nutzen wir R; daneben wird uns die sog.
``Entwicklungsumgebung'' RStudio einiges an komfortabler Funktionalität
bescheren. Eine Reihe von R-Paketen (``Packages''; d.h. Erweiterungen)
werden wir auch nutzen. R ist eine recht alte Sprache; viele Neuerungen
finden in Paketen Niederschlag, da der ``harte Kern'' von R lieber nicht
so stark geändert wird. Stellen Sie sich vor: Seit 29 Jahren nutzen Sie
eine Befehl, der Ihnen einen Mittelwert ausrechnet, sagen wir die
mittlere Anzahl von Tassen Kaffee am Tag. Und auf einmal wird der
Mittelwert anders berechnet?! Eine Welt stürzt ein! Naja, vielleicht
nicht ganz so tragisch in dem Beispiel, aber grundsätzlich sind
Änderungen in viel benutzen Befehlen potenziell problematisch. Das ist
wohl ein Grund, warum sich am ``R-Kern'' nicht so viel ändert. Die
Innovationen in R passieren in den Paketen. Und es gibt viele davon; als
ich diese Zeilen schreibe, sind es fast schon 10.000! Genauer: 9937 nach
dieser Quelle: \url{https://cran.r-project.org/web/packages/}. Übrigens
können R-Pakete auch Daten enthalten.

\subsection{R und RStudio
installieren}\label{r-und-rstudio-installieren}

\includegraphics[width=0.10000\textwidth]{images/Rahmen/Rlogo.png}
\includegraphics[width=0.20000\textwidth]{images/Rahmen/rstudiologo.png}

Sie können R unter \url{https://cran.r-project.org} herunterladen und
installieren (für Windows, Mac oder Linux). RStudio finden Sie auf der
gleichnamigen Homepage: \url{https://www.rstudio.com}; laden Sie die
``Desktop-Version'' für Ihr Betriebssystem herunter.

RStudio ist ``nur'' eine Oberfläche (``GUI'') für R, mit einer R von
praktischen Zusatzfunktionen. Die eigentlich Arbeit verrichtet das
``normale'' R, welches automatisch gestartet wird, wenn Sie RStudio
starten (sofern R installiert ist).

Die Oberfläche von RStudio sieht (unter allen Betriebssystemen etwa
gleich) so aus wie in Abbildung \ref{fig:rstudio-screenshot}
dargestellt.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/Rahmen/RStudio-Screenshot} 

}

\caption{RStudio}\label{fig:rstudio-screenshot}
\end{figure}

Das \emph{Skript-Fenster}\index{Skript-Fenster} ähnelt einem normalem
Text-Editor; praktischerweise finden Sie aber einen Button ``run'', der
die aktuelle Zeile oder die Auswahl ``abschickt'', d.h. in die Konsole
gibt, wo die Syntax ausgeführt wird. Wenn Sie ein Skript-Fenster öffnen
möchten, so können Sie das Icon
\includegraphics{images/Rahmen/new_script.png} klicken (Alternativ:
Ctrl-Shift-N oder File \textgreater{} New File \textgreater{} R Script).

Aus dem Fenster der \emph{Konsole}\index{Konsole} spricht R zu uns bzw.
wir mit R. Wird ein Befehl\index{Funktion} (synonym:
\emph{Funktion}\index{Funktion}) hier eingegeben, so führt R ihn aus. Es
ist aber viel praktischer, Befehle in das Skript-Fenster einzugeben, als
in die Konsole. Behalten Sie dieses Fenster im Blick, wenn Sie Antwort
von R erwarten.

Im Fenster \emph{Umgebung}\index{Umgebung} (engl. ``environment'') zeigt
R, welche Variablen (Objekte) vorhanden sind. Stellen Sie sich die
Umgebung wie einen Karpfenteich vor, in dem die Datensätze und andere
Objekte herumschwimmen. Was nicht in der Umgebung angezeigt wird,
existiert nicht für R.

Im Fenster rechts unten werden mehrere Informationen bereit gestellt,
z.B. werden Diagramme (Plots) dort ausgegeben. Klicken Sie mal die
anderen Reiter im Fenster rechts unten durch.

Wer Shortcuts mag, wird in RStudio überschwänglich beschenkt; der
Shortcut für die Shortcuts ist \texttt{Shift-Alt-K}.

Wenn Sie RStudio starten, startet R automatisch auch. Starten Sie daher,
wenn Sie RStudio gestartet haben, \emph{nicht} noch extra R. Damit
hätten Sie sonst zwei Instanzen von R laufen, was zu Verwirrungen (bei R
und beim Nutzer) führen kann.

\subsection{Hilfe! R startet nicht!}\label{hilfe-r-startet-nicht}

\begin{quote}
Manntje, Manntje, Timpe Te,\\
Buttje, Buttje inne See,\\
myne Fru de Ilsebill\\
will nich so, as ik wol will.
\end{quote}

\emph{Gebrüder Grimm, Märchen vom Fischer und seiner Frau\footnote{\url{https://de.wikipedia.org/wiki/Vom_Fischer_und_seiner_Frau}}}

Ihr R startet nicht oder nicht richtig? Die drei wichtigsten Heilmittel
sind:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Schließen Sie die Augen für eine Minute. Denken Sie an etwas Schönes
  und was Rs Problem sein könnte.
\item
  Schalten Sie den Rechner aus und probieren Sie es morgen noch einmal.
\item
  Googeln.
\end{enumerate}

Sorry für die schnoddrigen Tipps. Aber: Es passiert allzu leicht, dass
man \emph{Fehler} wie diese macht:

\BeginKnitrBlock{rmdcaution}
OH NO:

\begin{itemize}
\item
  install.packages(dplyr)
\item
  install.packages(``dliar'')
\item
  install.packages(``derpyler'')
\item
  install.packages(``dplyr'') \# dependencies vergessen
\item
  Keine Internet-Verbindung
\item
  library(dplyr) \# ohne vorher zu installieren
\end{itemize}
\EndKnitrBlock{rmdcaution}

Wenn R oder RStudio dann immer noch nicht starten oder nicht richtig
laufen, probieren Sie dieses:

\begin{itemize}
\item
  Sehen Sie eine Fehlermeldung, die von einem fehlenden Paket spricht
  (z.B. ``Package `Rcpp' not available'') oder davon spricht, dass ein
  Paket nicht installiert werden konnte (z.B. ``Package `Rcpp' could not
  be installed'' oder ``es gibt kein Paket namens `Rcpp'\,'' oder
  ``unable to move temporary installation XXX to YYY''), dann tun Sie
  folgendes:

  \begin{itemize}
  \tightlist
  \item
    Schließen Sie R und starten Sie es neu.
  \item
    Installieren Sie das oder die angesprochenen Pakete mit
    \texttt{install.packages("name\_des\_pakets",\ dependencies\ =\ TRUE)}
    oder mit dem entsprechenden Klick in RStudio.
  \item
    Starten Sie das entsprechende Paket mit
    \texttt{library(name\_des\_pakets)}.
  \end{itemize}
\item
  Gerade bei Windows 10 scheinen die Schreibrechte für R (und damit
  RStudio oder RCommander) eingeschränkt zu sein. Ohne Schreibrechte
  kann R aber nicht die Pakete (``packages'') installieren, die Sie für
  bestimmte R-Funktionen benötigen. Daher schließen Sie R bzw. RStudio
  und suchen Sie das Icon von R oder wenn Sie RStudio verwenden von
  RStudio. Rechtsklicken Sie das Icon und wählen Sie ``als Administrator
  ausführen''. Damit geben Sie dem Programm Schreibrechte. Jetzt können
  Sie etwaige fehlende Pakete installieren.
\item
  Ein weiterer Grund, warum R bzw. RStudio die Schreibrechte verwehrt
  werden könnten (und damit die Installation von Paketen), ist ein
  Virenscanner. Der Virenscanner sagt, nicht ganz zu Unrecht: ``Moment,
  einfach hier Software zu installieren, das geht nicht, zu
  gefährlich''. Grundsätzlich gut, in diesem Fall unnötig. Schließen Sie
  R/RStudio und schalten Sie dann den Virenscanner \emph{komplett} (!)
  aus. Öffnen Sie dann R/RStudio wieder und versuchen Sie fehlende
  Pakete zu installieren.
\end{itemize}

\subsubsection{I am an outdated model}\label{i-am-an-outdated-model}

Verwenden Sie möglichst die neueste Version von R, RStudio und Ihres
Betriebssystems. Ältere Versionen führen u.U. zu Problemen; je älter,
desto Problem\ldots{} Updaten Sie Ihre Packages regelmäßig z.B. mit
\texttt{update.packages()} oder dem Button ``Update'' bei RStudio
(Reiter \texttt{Packages}).

\subsection{Pakete}\label{pakete}

Ein Großteil der Neuentwicklungen bei R passiert in sog. `Paketen'
(engl. \emph{packages}), das sind Erweiterungen für R. Jeder, der sich
berufen fühlt, kann ein R-Paket schreiben und es zum `R-Appstore'
(\href{https://cran.r-project.org/}{CRAN}) hochladen. Von dort kann es
dann frei (frei wie in Bier) heruntergeladen werden.

Am einfachsten installiert man R-Pakete in RStudio über den Button
``Install'' im Reiter ``Packages'' (s. Abb.
\ref{fig:fig-install-packages}).

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/Rahmen/install_packages} 

}

\caption{So installiert man Pakete in RStudio}\label{fig:fig-install-packages}
\end{figure}

Ein R-Paket, welches wir gleich benötigen, heißt \texttt{devtools}.
Bitte installieren Sie es schon einmal (sofern noch nicht geschehen).
Sie können auch folgenden Befehl verwenden, um Pakete zu installieren.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"devtools"}\NormalTok{, }\DataTypeTok{dependencies =} \OtherTok{TRUE}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

Aber einfacher geht es über die RStudio-Oberfläche.

Alle Pakete außer \texttt{devtools}, die wir hier benötigen, können über
das R-Paket \texttt{prada} installiert werden. Sie müssen also nur noch
das Paket \texttt{prada}installieren. Mit dem Befehl
\texttt{install\_prada\_pckgs} (aus dem Paket \texttt{prada}) werdenn
dann ggf. eine Reihe weiterer Pakete installiert. Allerdings wohnt
\texttt{prada} nicht im R-Appstore (CRAN), sondern bei
\href{www.github.com}{Github}\footnote{einer Online-Plattform, auf der
  man Dateien bereistellen und ihre Veränderungen nachverfolgen kann}.
Um Pakete von Github zu installieren, nutzen wir diesen Befehl (Sie
müssen natürlich online sein):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{devtools}\OperatorTok{::}\KeywordTok{install_github}\NormalTok{(}\StringTok{"sebastiansauer/prada"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(prada)}
\end{Highlighting}
\end{Shaded}

Sofern Sie online sind, sollte das Paket \texttt{prada} jetzt
installiert sein. Installieren Sie jetzt alle Pakete, die für diesen
Kurs benötigt werden mit dem Befehl \texttt{install\_prada\_pckgs()}.

\BeginKnitrBlock{rmdcaution}
Beim Installieren von R-Paketen könnten Sie gefragt werden, welchen
``Mirror'' Sie verwenden möchten. Das hat folgenden Hintergrund:
R-Pakete sind in einer Art ``App-Store'', mit Namen CRAN (Comprehense R
Archive Network) gespeichert. Damit nicht ein armer, kleiner Server
überlastet wird, wenn alle Studis dieser Welt just gerade beschließen,
ein Paket herunterzuladen, gibt es viele Kopien dieses Servers - seine
Spiegelbilder (engl. ``mirrors''). Suchen Sie sich einfach einen aus,
der in der Nähe ist.

Bei der Installation von Paketen mit
\texttt{install.packages("name\_des\_pakets")} sollte stets der
Parameter \texttt{dependencies\ =\ TRUE} angefügt werden. Also
\texttt{install.packages("name\_des\_pakets",\ dependencies\ =\ TRUE)}.
Hintergrund ist: Falls das zu installierende Paket seinerseits Pakete
benötigt, die noch nicht installiert sind (gut möglich), dann werden
diese sog. ``dependencies'' gleich mitinstalliert (wenn Sie
\texttt{dependencies\ =\ TRUE} setzen).
\EndKnitrBlock{rmdcaution}

Nicht vergessen: Installieren muss man eine Software \emph{nur einmal};
\emph{starten} (laden) muss man die R-Pakete jedes Mal, wenn man sie
vorher geschlossen hat und wieder nutzen möchte.

\begin{quote}
Wenn Sie R bzw. RStudio schließen, werden alle Pakete ebenfalls
geschlossen. Sie müssen die benötigten Pakete beim erneuten Öffnen von
RStudio wieder starten.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr) }
\end{Highlighting}
\end{Shaded}

Der Befehl bedeutet sinngemäß: ``Hey R, geh in die Bücherei (library)
und hole das Buch (package) dplyr!''.

\BeginKnitrBlock{rmdcaution}
Wann benutzt man bei R Anführungszeichen? Das ist etwas verwirrend im
Detail, aber die Grundegel lautet: wenn man Text anspricht. Im Beispiel
oben ``library(dplyr)'' ist ``dplyr'' hier erst mal für R nichts
Bekanntes, weil noch nicht geladen. Demnach müssten \emph{eigentlich}
Anführungsstriche stehen. Allerdings meinte ein Programmierer, dass es
doch so bequemer ist. Hat er Recht. Aber bedenken Sie, dass es sich um
die Ausnahme einer Regel handelt. Sie können also auch schreiben:
library(``dplyr'') oder library(`dplyr'); beides geht.
\EndKnitrBlock{rmdcaution}

\subsection{Vertiefung: Zuordnung von Paketen zu
Befehlen}\label{funs-pckgs}

\emph{Woher weiß man, welche Befehle (oder auch Daten) in einem Paket
enthalten sind?}

Eine einfache Möglichkeit ist es, beim Reiter `Pakete' auf den Namen
eines der installierten Pakete zu klicken. Daraufhin öffnet sich die
Dokumentation des Pakets und man sieht dort alle Befehle und Daten
aufgeführt (s. Abbildung \ref{fig:pakete-hilfe}). Übrigens sehen Sie
dort auch die Version eines Pakets (vielleicht sagt jemand mal zu Ihnen,
``Sie sind ja outdated'', dann schauen Sie mal auf die die
Paket-Versionen).

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/Rahmen/hilfe_pakete} 

}

\caption{Hier werden Sie geholfen: Die Dokumentation der R-Pakete}\label{fig:pakete-hilfe}
\end{figure}

Für geladenen Pakete kann man auch den Befehl \texttt{help} nutzen, z.B.
\texttt{help(ggplot2)}.

\emph{Und umgekehrt, woher weiß ich, in welchem Paket ein Befehl
`wohnt'?}

Probieren Sie den Befehl \texttt{help.search("qplot")}, wenn Sie wissen
möchten, in welchem Paket \texttt{qplot} zuhause ist.
\texttt{help.search} sucht alle Hilfeseiten von \emph{installierten}
Paketen, in der der Suchbegriff irgendwie vorkommt. Um das Paket eines
\emph{geladenen} Befehl zu finden, hilft der Befehl \texttt{find}:
\texttt{find("qplot")}.

Sie können auch den Befehl \texttt{find\_fun}s aus dem Paket
\texttt{prada} nutzen:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prada}\OperatorTok{::}\KeywordTok{find_funs}\NormalTok{(}\StringTok{"select"}\NormalTok{)}
\CommentTok{#> # A tibble: 5 x 3}
\CommentTok{#>   package_name builtin_pckage loaded}
\CommentTok{#>          <chr>          <lgl>  <lgl>}
\CommentTok{#> 1        dplyr          FALSE  FALSE}
\CommentTok{#> 2         MASS           TRUE  FALSE}
\CommentTok{#> 3       plotly          FALSE  FALSE}
\CommentTok{#> 4       raster          FALSE  FALSE}
\CommentTok{#> 5         VGAM          FALSE  FALSE}
\end{Highlighting}
\end{Shaded}

In diesem Skript sind am Ende jedes Kapitels die jeweils besprochenen
(neuen) Befehle aufgeführt - inklusive ihres Paketes. Falls bei einem
Befehl kein Paket angegeben ist, heißt das, dass der Befehl im
`Standard-R' wohnt - Sie müssen kein weiteres Paket laden\footnote{Eine
  Liste der Pakete, die beim Standard-R enthalten sind (also bereits
  installiert sind) finden Sie
  \href{https://stat.ethz.ch/R-manual/R-devel/doc/html/packages.html}{hier}}.
Also zum Beispiel \texttt{ggplot2::qplot}: Der \emph{Befehl}
\texttt{qplot} ist im \emph{Paket} \texttt{ggplot2} enthalten. Das
Zeichen \texttt{::} trennt also Paket von Befehl.

\BeginKnitrBlock{rmdcaution}
Manche Befehle haben Allerweltsnamen (z.B. `filter'). Manchmal gibt es
Befehle mit gleichem Namen in verschiedenen Paketen; besonders Befehle
mit Allerweltsnamen (wie `filter') sind betroffen (`mosaic::filter' vs.
`dplyr::filter'). Falls Sie von wirre Ausgaben bekommen oder diffuse
Fehlermeldung kann es sein, kann es sein, dass R einen Befehl mit dem
richtigen Namen aber aus dem `falschen' Paket zieht. Geben Sie im
Zweifel lieber den Namen des Pakets vor dem Paketnamen an, z.B. so
\texttt{dplyr::filter}. Der `doppelte Doppelpunnkt' trennt den
Paketnamen vom Namen der Funtion.
\EndKnitrBlock{rmdcaution}

Außerdem sind zu Beginn jedes Kapitels die in diesem Kapitel benötigten
Pakete angegeben. Wenn sie diese Pakete laden, werden alle Befehle
dieses Kapitels funktionieren\footnote{es sei denn, sie tun es nicht}.

\emph{Wie weiß ich, ob ein Paket geladen ist?}

Wenn der Haken im Reiter `Packages' gesetzt ist (s. Abbildung
\ref{fig:pakete-hilfe}), dann ist das Paket geladen. Sonst nicht.

\subsection{Datensätze}\label{daten}

Die folgenden Datensätze sind entweder im Paket \texttt{prada} enthalten
oder können aus anderen Paketen geladen werden. Um Daten aus einem Paket
zu laden, gibt es den Befehl \texttt{data}:
\texttt{data("name\_datenobjekt",\ package\ =\ "Paketname")}. Also zum
Beispiel:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(}\StringTok{"stats_test"}\NormalTok{, }\DataTypeTok{package =} \StringTok{"prada"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Wenn ein bestimmtes Paket geladen ist, können Sie auch auf den Parameter
\texttt{package\ =\ ...} verzichten, wenn ihr Datensatz in jedem Paket
wohnt: Geladene Pakete werden vom Befehl \texttt{data} automatisch
durchsucht.

Alternativ können Sie die Daten auch im Ordner \texttt{data} im
Github-Repositorium herunterladen. Gehen Sie auf die Github-Seite dieses
Kurses\footnote{\url{https://github.com/sebastiansauer/Praxis_der_Datenanalyse}},
klicken Sie auf den großen grünen Button ``Clone or download'', wählen
Sie dann ``Download ZIP'', um alle Dateien herunterzuladen. Nach dem
Entzippen können Sie dann auf alle Dateien, inklusive Daten, zugreifen.

\begin{quote}
Die Daten dieses Kurses liegen im Ordner `data'.
\end{quote}

\begin{itemize}
\tightlist
\item
  Datensatz \texttt{profiles} aus dem R-Paket \{okcupiddata\} (Kim und
  Escobedo-Land \protect\hyperlink{ref-kim2015okcupid}{2015}); es
  handelt sich um Daten von einer Online-Singlebörse
\item
  Datensatz \texttt{stats\_test} aus dem R-Paket \{prada\} (Sauer
  \protect\hyperlink{ref-Sauer_2017}{2017}\protect\hyperlink{ref-Sauer_2017}{a});
  es handelt sich um Ergebnisse einer Statistikklausur (einer
  Probeklausur)
\item
  Datensatz \texttt{flights} aus dem R-Paket \{nycflights13\} (RITA
  \protect\hyperlink{ref-nycflights13}{2013}); es handelt sich um
  Abflüge von den New Yorker Flughäfen
\item
  Datensatz \texttt{wo\_men}, URL: \url{https://osf.io/ja9dw} (Sauer
  \protect\hyperlink{ref-Sauer_2017a}{2017}\protect\hyperlink{ref-Sauer_2017a}{b});
  es handelt sich um Körper- und Schuhgröße von Studierenden
\item
  Datensatz \texttt{extra} aus dem R-Paket \{prada\} (Sauer
  \protect\hyperlink{ref-Sauer_2016}{2016}); es handelt sich die
  Ergebnisse einer Umfrage zu Extraversion
\item
  Datensatz \texttt{titanic\_train} aus dem Paket \{titanic\} von
  \href{https://www.kaggle.com/c/titanic/data}{kaggle}; es handelt sich
  um Überlebensraten vom Titanic-Unglück.
\item
  Datensatz \texttt{Affairs} aus dem Paket \{AER\} (Fair
  \protect\hyperlink{ref-fair1978theory}{1978}); es handelt sich um eine
  Umfrage zu außerehehlichen Affären.
\end{itemize}

Wie man Daten in R `einlädt' (Studierende sagen gerne `ins R
hochladen'), besprechen wir im Kapitel \ref{daten-einlesen}.

\section{ERRRstkontakt}\label{errrstkontakt}

\subsection{R-Skript-Dateien}\label{r-skript-dateien}

Ein neues \emph{R-Skript}\index{R-Skript} im RStudio können Sie z.B.
öffnen mit \texttt{File-New\ File-R\ Script}. Schreiben Sie dort Ihre
R-Befehle; Sie können die Skriptdatei speichern, öffnen, ausdrucken,
übers Bett hängen\ldots{} R-Skripte können Sie speichern (unter
\texttt{File-Save}) und öffnen. R-Skripte sind einfache Textdateien, die
jeder Texteditor verarbeiten kann. Nur statt der Endung \texttt{.txt},
sind R-Skripte stolzer Träger der Endung \texttt{.R}. Es bleibt aber
eine schnöde Textdatei. Geben Sie Ihren R-Skript-Dateien die Endung
``.R'', damit erkennt RStudio, dass es sich um ein R-Skript handelt und
bietet ein paar praktische Funktionen wie den ``Run-Button''.

\subsection{Datentypen in R}\label{datentypen-in-r}

Die (für diesen Kurs) wichtigsten Datentypen von R sind in Tabelle
\ref{tab:datentypen} aufgeführt (vgl. (\emph{Programmieren mit R}
\protect\hyperlink{ref-ligges}{2009})).

\begin{table}

\caption{\label{tab:datentypen}Wichtige Datentypen in R}
\centering
\begin{tabular}[t]{l|l|l}
\hline
Name & Beschreibung & Beispiel\\
\hline
Name & Beschreibung & Beispiel\\
\hline
NULL & die leere Menge & NULL\\
\hline
logical & Logische Ausdrücke & TRUE\\
\hline
integer & Ganze Zahl & 3\\
\hline
factor & nominale Variablen & "weiblich"\\
\hline
numeric & Reelle Zahl & 2.71\\
\hline
character & Text & "Schorsch"\\
\hline
\end{tabular}
\end{table}

All diese Datentypen (mit Ausnahme der leeren Menge) sind als
\emph{Vektoren}\index{Vektoren} angelegt, also mehreren Elementen (z.B.
Zahlen), die zu einem Ganzen (wie in einer Liste) verknüpft sind.
\emph{Faktoren} sind ganz interessant, weil die einzelnen Ausprägungen
(\emph{Faktorstufen}\index{Faktorstufen} genannt) für R als Zahlen
gespeichert sind (z.B. ``Frau Müller und Herr Schorsch'' = 1). Wenn ein
Vektor aus 100 Mal diesem Text (``Frau Müller\ldots{}'') besteht, muss R
nur 100 mal 1 speichern und einmal die Zuordnung, was die 1 bedeutet.
Spart Speicher. Außerdem kann man definieren, was alles eine Faktorstufe
ist (z.B. nur ``Mann'' und ``Frau''). Andere Eingaben sind dann nicht
möglich; das kann praktisch sein, wenn man von vornerein nur bestimmte
Ausprägungen zulassen möchte. Textvariablen sind da entspannter:
Jeglicher Art von Text ist erlaubt. Text ist in R immer in
Anführungszeichen (einfach oder doppelt) zu setzen.

Für die praktische Datenanalyse ist der \texttt{dataframe}
(\emph{Dataframe}\index{Dataframe}; auch Datentabelle\index{Dataframe}
oder Datensatz\index{Dataframe}) am wichtigsten. Grob gesagt handelt es
sich dabei um eine Tabelle, wie man sie aus Excel kennt. Etwas genauer
ist eine Kombination von Vektoren mit gleicher Länge, so dass eine
`rechteckige' Datenstruktur entsteht. Alle Spalten (d.h. Vektoren) haben
einen Namen, so dass es `Spaltenköpfe' gibt. Eine neuere Variante von
Dataframes sind \emph{tibbles} (Tibbles)\index{Tibbles}, die \emph{auch}
Dataframes sind, aber ein paar praktische Zusatzeigenschaften aufweisen
(normale Dataframes können sich manchmal in einfache Vektoren auflösen;
Tibbles tun dies nie).

\subsection{Hinweise}\label{hinweise-1}

Unser erster Kontakt mit R! Ein paar Anmerkungen vorweg:

\begin{itemize}
\tightlist
\item
  R unterscheidet zwischen Groß- und Kleinbuchstaben, d.h. \texttt{Oma}
  und \texttt{oma} sind zwei verschiedene Dinge für R!
\item
  R verwendet den Punkt \texttt{.} als Dezimaltrennzeichen.
\item
  Fehlende Werte werden in R durch \texttt{NA} kodiert.
\item
  Kommentare werden mit dem Rautezeichen \texttt{\#} eingeleitet; der
  Rest der Zeile von von R dann ignoriert.
\item
  \emph{Variablennamen}\index{Variablen} in R sollten mit Buchstaben
  beginnen; ansonsten dürfen nur Zahlen und Unterstriche (\texttt{\_})
  enthalten sein. Leerzeichen sollte man meiden. Das gilt auch für
  Spaltennamen.
\item
  Um den Inhalt einer Variablen auszulesen, geben wir einfach den Namen
  des Objekts ein (und schicken den Befehl ab).
\item
  Bleiben Sie konsistent, in der Art und Weise, wie Sie Ihre Syntax
  schreiben. Ein Vorschlag zum `Syntax-Stil' finden Sie
  \href{http://adv-r.had.co.nz/Style.html}{hier}.
\item
  Variablen einen treffenden Namen zu geben, ist nicht immer leicht,
  aber wichtig. Namen sollten knapp, aber aussagekräftig sein.
\end{itemize}

\begin{verbatim}
# so nicht:
var
x
dummy
objekt
dieser_name_ist_etwas_lang_vielleicht

# gut:
tips_mw
lm1
\end{verbatim}

\subsection{Text und Variablen
zuweisen}\label{text-und-variablen-zuweisen}

Man kann einer Variablen auch Text zuweisen (im Gegensatz zu Zahlen):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y <-}\StringTok{ "Hallo R!"}
\end{Highlighting}
\end{Shaded}

Man kann auch einer Variablen eine andere zuweisen:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y <-}\StringTok{ }\NormalTok{x}
\end{Highlighting}
\end{Shaded}

Wird jetzt y mit dem Inhalt von x überschrieben oder umgekehrt? Der
Zuweisungspfeil \texttt{\textless{}-} macht die Richtung der Zuweisung
ganz klar. Zwar ist in R das Gleichheitszeichen synonym zum
Zuweisungspfeil erlaubt, aber der Zuweisungspfeil macht die Sache
glasklar und sollte daher bevorzugt werden.

Man kann auch einer Variablen \emph{mehr als} einen Wert zuweisen:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Dieser Befehl erzeugt eine ``Spalte'' (einen Vektor). Will man einer
Variablen \emph{mehr als} einen Wert zuweisen, muss man die Werte erst
in einen Vektor ``zusammen binden''; das geht mit dem Befehl \texttt{c}
(vom engl. ``\emph{\textbf{c}ombine}'').

\subsection{Funktionen aufrufen}\label{funktionen-aufrufen}

Um einen \emph{Befehl}\index{Befehl, Funktion} (präziser aber synonym
hier: eine Funktion) aufzurufen, geben wir ihren Namen an und definieren
sog. \emph{Parameter}\index{Parameter eines R-Befehls} in einer runden
Klammer, z.B. so:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wo_men <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"data/wo_men.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Allgemein gesprochen:

\begin{verbatim}
funktionsname(parametername1 = wert1, parametername2 = wert2, ...)
\end{verbatim}

Die drei Punkte \texttt{...} sollen andeuten, dass evtl. weitere
Parameter zu übergeben wären. Die Reihenfolge der Parameter ist
\emph{egal} - wenn man die Parameternamen anführt. Ansonsten muss man
sich an die Standard-Reihenfolge, die eine Funktion vorgibt halten:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#ok:}
\NormalTok{wo_men <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\DataTypeTok{file =} \StringTok{"data/wo_men.csv"}\NormalTok{, }\DataTypeTok{header =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{sep =} \StringTok{","}\NormalTok{)}
\NormalTok{wo_men <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"data/wo_men.csv"}\NormalTok{, }\OtherTok{TRUE}\NormalTok{, }\StringTok{","}\NormalTok{)}
\NormalTok{wo_men <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\DataTypeTok{header =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{sep =} \StringTok{","}\NormalTok{, }\DataTypeTok{file =} \StringTok{"data/wo_men.csv"}\NormalTok{)}


\CommentTok{# ohno:}
\NormalTok{wo_men <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\StringTok{"data/wo_men.csv"}\NormalTok{, }\StringTok{","}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In der Hilfe zu einem Befehl findet man die Standard-Syntax inklusive
der möglichen Parameter, ihrer Reihenfolge und Standardwerten (default
values) von Parametern. Zum Beispiel ist beim Befehl \texttt{read.csv}
der Standardwert für \texttt{sep} mit \texttt{;} voreingestellt (schauen
Sie mal in der Hilfe nach). Gibt man einen Parameter nicht an, für den
ein Standardwert eingestellt ist, `befüllt' R den Parameter mit diesem
Standardwert.

\subsection{Das Arbeitsverzeichnis}\label{wd}

Das aktuelle Verzeichnis (Arbeitsverzeichnis; ``working directory'')
kann man mit \texttt{getwd()} erfragen und mit \texttt{setwd()}
einstellen. Komfortabler ist es aber, das aktuelle Verzeichnis per Menü
zu ändern (vgl. Abb. \ref{fig:Arbeitsverzeichnis}. In RStudio:
\texttt{Session\ \textgreater{}\ Set\ Working\ Directory\ \textgreater{}\ Choose\ Directory\ ...}
(oder per Shortcut, der dort angezeigt wird).

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/tidy/Arbeitsverzeichnis} 

}

\caption{Das Arbeitsverzeichnis mit RStudio auswählen}\label{fig:Arbeitsverzeichnis}
\end{figure}

Es ist praktisch, das Arbeitsverzeichnis festzulegen, denn dann kann man
z.B. eine Datendatei einlesen, ohne den Pfad eingeben zu müssen:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# nicht ausführen:}
\NormalTok{daten_deutsch <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"daten_deutsch.csv"}\NormalTok{, }\DataTypeTok{sep =} \StringTok{";"}\NormalTok{, }\DataTypeTok{dec =} \StringTok{"."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

R geht dann davon aus, dass sich die Datei \texttt{daten\_deutsch.csv}
im Arbeitsverzeichnis befindet.

Für diesen Kurs ist es sinnvoll, das Arbeitsverzeichnis in einen
``Hauptordner'' zu legen (z.B. ``Praxis\_der\_Datenanalyse''), in dem
Daten und sonstiges Material als Unterordner abgelegt sind.

\BeginKnitrBlock{rmdcaution}
Übrigens: Wenn Sie keinen Pfad angeben, so geht R davon aus, dass die
Daten im aktuellen Verzeichnis (dem \emph{working directory}) liegen.
\EndKnitrBlock{rmdcaution}

\section{Hier werden Sie geholfen}\label{hier-werden-sie-geholfen}

\subsection{Wo finde ich Hilfe?}\label{wo-finde-ich-hilfe}

Es ist keine Schande, nicht alle Befehle der ca. 10,000 R-Pakete
auswendig zu wissen. Schlauer ist, zu wissen, wo man Antworten findet.
Hier eine Auswahl:

\begin{itemize}
\item
  Zu diesen Paketen gibt es gute ``Spickzettel'' (cheatsheets): ggplot2,
  RMarkdown, dplyr, tidyr. Klicken Sie dazu in RStudio auf \emph{Help
  \textgreater{} Cheatsheets \textgreater{} \ldots{}} oder gehen Sie auf
  \url{https://www.rstudio.com/resources/cheatsheets/}.
\item
  In RStudio gibt es eine Reihe (viele) von Tastaturkürzeln (Shortcuts),
  die Sie hier finden: \emph{Tools \textgreater{} Keyboard Shortcuts
  Help}.
\item
  Für jeden Befehl aus einem \emph{geladenen} Paket können Sie mit
  \texttt{help()} die Hilfe-Dokumentation anschauen, also z.B.
  \texttt{help("qplot")}.
\item
  Im Internet finden sich zuhauf Tutorials.
\item
  Der Reiter ``Help'' bei RStudio verweist auf die Hilfe-Seite des
  jeweiligen Pakets bzw. Befehls.
\item
  Die bekannteste Seite um Fragen rund um R zu diskutieren ist
  \url{http://stackoverflow.com}.
\end{itemize}

\subsection{Einfache reproduzierbare Beispiele
(ERBies)}\label{einfache-reproduzierbare-beispiele-erbies}

Sagen wir, Sie haben ein Problem. Mit R. Bevor Sie jemanden bitten, Ihr
Problem zu lösen, haben Sie schon \sout{drei} \sout{dreizehn}
\sout{dreißig} Minuten recherchiert, ohne Erfolg. Sie entschließen sich,
bei \href{www.stackoverflow.com}{Stackoverflow} Ihr Problem zu posten.
Außerdem kann sicher eine Mail zu einem Bekannten, einem Dozenten oder
sonstwem, der sich auskennen sollte, nicht schaden. Sie formulieren also
Ihr Problem: ``Hallo, mein R startet nicht, und wenn es startet, dann
macht es nicht, was ich soll, außerdem funktioniert der Befehl `mean'
bei mir nicht. Bitte lös mein Problem!''. Seltsamerweise reagieren die
Empfänger Ihrer Nachricht nicht alle begeistert. Stattdessen verlangt
jemand (dreist) nach einer genauen Beschreibung Ihres Problems, mit dem
Hinweis, dass ``Ferndiagnosen'' schwierig sein. Genauer gesagt möchte
ihr potenzieller Helfer ein `minimal reproducible example' (MRE) oder,
Deutsch, ein \emph{einfaches reproduzierbares
Beispiel}\index{einfaches reproduzierbares Beispiel} (ERBie).

\begin{quote}
Wenn Sie jemanden um R-Hilfe bitten, dann sollten Sie Ihr Problem
prägnant beschreiben.
\end{quote}

Was sollte alles in einem ERBie enthalten sein?

\begin{quote}
Ein ERBie besteht aus vier Teilen: Syntax, Daten, Paketen und Infos zum
laufenden System (R Version etc.)
\end{quote}

Wie sollte so ein ERBie aussehen? Ich empfehle, folgende Eckpunkte zu
beachten\footnote{Hier finden Sie weitere Hinweise zu ERBies:
  \url{https://stackoverflow.com/help/mcve} oder
  \url{https://gist.github.com/hadley/270442}}:

\begin{itemize}
\tightlist
\item
  Syntax: Stellen Sie die R-Syntax bereit, die ein Problem bereit (d.h.
  die einen Fehler liefert).
\item
  Einfach: Geben Sie sowenig Syntax wie möglich an. Es bereitet Ihrem
  Helfer nur wenig Spaß, sich durch 2000 Zeilen Code zu wühlen, wenn es
  10 Zeilen auch getan hätten.
\item
  Reproduzierbar Geben Sie soviel Syntax wie nötig, um den Fehler zu
  erzeugen (aber nicht mehr).
\item
  Schreiben Sie Ihre Syntax übersichtlich, veständlich und kommentiert;
  z.B. sollten die Variablennamen informativ sein.
\item
  Beschreiben Sie den Fehler genau (``läuft nicht'' reicht nicht); z.B.
  ist es hilfreich, den Wortlaut einer Fehlermeldung bereitzustellen.
\item
  Zu Beginn der Syntax sollten die benötigten Pakete geladen werden.
\item
  Zu Ende des ERBie sollte der Output von \texttt{sessionInfo()}
  einkopiert werden; damit werden Informationen zum laufenden System
  (wie Version von R, Betriebssystem etc.) bereitgestellt.
\item
  Beziehen Sie sich möglichst auf Daten, die in R schon ``eingebaut
  sind'' wie die Datensätze \texttt{iris} oder \texttt{mtcars}.
\end{itemize}

Natürlich sollte man immer erst selbst nach einer Lösung recherchieren,
bevor man jemanden um Hilfe bittet. Viele Fragen wurden schon einmal
diskutiert und oft auch gelöst.

\section{Was ist Statistik? Wozu ist sie
gut?}\label{was-ist-statistik-wozu-ist-sie-gut}

Zwei Fragen bieten sich sich am Anfang der Beschäftigung mit jedem Thema
an: Was ist die Essenz des Themas? Warum ist das Thema (oder die
Beschäftigung damit) wichtig?

Was ist Statistik? \emph{Eine} Antwort dazu ist, dass Statistik die
Wissenschaft von Sammlung, Analyse, Interpretation und Kommunikation von
Daten ist mithilfe mathematischer Verfahren ist und zur
Entscheidungshilfe beitragen solle (\emph{The Oxford Dictionary of
Statistical Terms} \protect\hyperlink{ref-oxford}{2006}; Romeijn
\protect\hyperlink{ref-sep-statistics}{2016}). Damit hätten wir auch den
Unterschied zur schnöden Datenanalyse (ein Teil der Statistik)
herausgemeißelt. Statistik wird häufig in die zwei Gebiete
\emph{deskriptive} und \emph{inferierende} Statistik eingeteilt (vgl.
Abb. \ref{fig:desk-vs-inf}). Erstere fasst viele Zahlen zusammen, so
dass wir den Wald statt vieler Bäume sehen. Letztere verallgemeinert von
den vorliegenden (sog. ``Stichproben-'')Daten auf eine zugrunde liegende
Grundmenge (Population). Dabei spielt die Wahrscheinlichkeitsrechnung
(Stochastik) eine große Rolle.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/Rahmen/desk_vs_inf-crop} 

}

\caption{Sinnbild für die Deskriptiv- und die Inferenzstatistik}\label{fig:desk-vs-inf}
\end{figure}

Aufgabe der deskriptiven Statistik ist es primär, Daten prägnant
zusammenzufassen. Aufgabe der Inferenzstatistik ist es, zu prüfen, ob
Daten einer Stichprobe auf eine Grundgesamtheit verallgemeinert werden
können.

Dabei lässt sich der Begriff ``Statistik'' als Überbegriff von
``Datenanalyse'' verstehen, wenn diese Sicht auch nicht von allen
geteilt wird (Grolemund und Wickham
\protect\hyperlink{ref-grolemund2014cognitive}{2014}). In diesem Buch
steht die Aufbereitung, Analyse, Interpretation und Kommunikation von
Daten im Vordergrund. Liegt der Schwerpunkt dieser Aktivitäten bei
computerintensiven Methoden, so wird auch von \emph{Data Science}
gesprochen, wobei der Begriff nicht einheitlich verwendet wird (Wickham
und Grolemund \protect\hyperlink{ref-r4ds}{2016}; Hardin u.~a.
\protect\hyperlink{ref-hardin2015data}{2015})

\emph{Daten} kann man definieren als \emph{Informationen, die in einem
Kontext stehen} (Moore
\protect\hyperlink{ref-moore1990uncertainty}{1990}), wobei eine
numerische Konnotation mitschwingt.

\emph{Modellieren} kann man als \emph{zentrale Aufgabe von Statistik}
begreifen (Cobb \protect\hyperlink{ref-cobb2007introductory}{2007};
Grolemund und Wickham
\protect\hyperlink{ref-grolemund2014cognitive}{2014}). Einfach
gesprochen, bedeutet Modellieren in diesem Sinne, ein mathematisches
Narrativ (``Geschichte'') zu finden, welches als Erklärung für gewisse
Muster in den Daten fungiert; vgl. Kap. \ref{mod1}.

Statistisches Modellieren läuft gewöhnlich nach folgendem Muster ab
(Grolemund und Wickham
\protect\hyperlink{ref-grolemund2014cognitive}{2014}):

\begin{verbatim}
Prämisse 1: Wenn Modell M wahr ist, dann sollten die Daten das Muster D aufweisen.
Prämisse 2: Die Daten weisen das Muster D auf.
---
Konklusion: Daher muss das Modell M wahr sein.
\end{verbatim}

Die Konklusion ist \emph{nicht} zwangsläufig richtig. Es ist falsch zu
sagen, dass dieses Argumentationsmuster - Abduktion (Peirce
\protect\hyperlink{ref-peirce1955abduction}{1955}) genannt - wahre,
sichere Schlüsse (Konklusionen) liefert. Die Konklusion \emph{kann, muss
aber nicht}, zutreffen.

Ein Beispiel: Auf dem Nachhauseweg eines langen Arbeitstags wartet, in
einer dunklen Ecke, ein Mann, der sich als Statistik-Professor vorstellt
und Sie zu einem Glücksspiel einlädt. Sofort sagen Sie zu. Der
Statistiker will 10 Mal eine Münze werfen, er setzt auf Zahl (versteht
sich). Wenn er gewinnt, bekommt er 10\euro{} von Ihnen; gewinnen Sie,
bekommen Sie 11\euro{} von ihm. Hört sich gut an, oder? Nun wirft er die
Münze zehn Mal. Was passiert? Er gewinnt 10 Mal, natürlich (so will es
die Geschichte). Sollten wir glauben, dass er ein Betrüger ist?

Ein Modell, welches wir hier verwenden könnten, lautet: Wenn die Münze
gezinkt ist (Modell M zutrifft), dann wäre diese Datenlage D (10 von 10
Treffern) wahrscheinlich - Prämisse 1. Datenlage D ist tatsächlich der
Fall; der Statistiker hat 10 von 10 Treffer erzielt - Prämisse 2. Die
Daten D ``passen'' also zum Modell M; man entscheidet sich, dass der
Professor ein Falschspieler ist.

Wichtig zu erkennen ist, dass Abduktion mit dem Wörtchen \emph{wenn}
beginnt. Also davon \emph{ausgeht}, dass ein Modell M der Fall ist (der
Professor also tatsächlich ein Betrüger ist). Das, worüber wir
entscheiden wollen, wird bereits vorausgesetzt. Falls M gilt, gehen wir
mal davon aus, wie gut passen dann die Daten dazu?

\begin{quote}
Wie gut passen die Daten D zum Modell M?
\end{quote}

Das ist die Frage, die hier tatsächlich gestellt bzw. beantwortet wird.

Natürlich ist es keineswegs sicher, \emph{dass} das Modell gilt. Darüber
macht die Abduktion auch keine Aussage. Es könnte also sein, dass ein
anderes Modell zutrifft: Der Professor könnte ein Heiliger sein, der uns
auf etwas merkwürdige Art versucht, Geld zuzuschanzen\ldots{} Oder er
hat einfach Glück gehabt.

\begin{quote}
Statistische Modelle beantworten i.d.R. nicht, wie wahrscheinlich es
ist, dass ein Modell gilt. Statistische Modelle beurteilen, wie gut
Daten zu einem Modell passen.
\end{quote}

Häufig trifft ein Modell eine Reihe von Annahmen, die nicht immer
explizit gemacht werden, aber die klar sein sollten. Z.B. sind die
Münzwürfe unabhängig voneinander? Oder kann es sein, dass sich die Münze
``einschießt'' auf eine Seite? Dann wären die Münzwürfe nicht unabhängig
voneinander. In diesem Fall klingt das reichlich unplausibel; in anderen
Fällen kann dies eher der Fall sein\footnote{Sind z.B. die
  Prüfungsergebnisse von Schülern unabhängig voneinander? Möglicherweise
  haben sie von einem ``Superschüler'' abgeschrieben. Wenn der
  Superschüler viel weiß, dann zeigen die Abschreiber auch gute
  Leistung.}. Auch wenn die Münzwürfe unabhängig voneinander sind, ist
die Wahrscheinlichkeit für Zahl jedes Mal gleich? Hier ist es wiederum
unwahrscheinlich, dass sich die Münze verändert, ihre Masse verlagert,
so dass eine Seite Unwucht bekommt. In anderen Situationen können sich
Untersuchungsobjekte verändern (Menschen lernen manchmal etwas, sagt
man), so dass die Wahrscheinlichkeiten für ein Ereignis unterschiedlich
sein können, man dies aber nicht berücksichtigt.

\section{Aufgaben}\label{aufgaben}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Öffnen Sie das Cheatsheet für RStudio und machen Sie sich mit dem
  Cheatsheet vertraut.
\item
  Sichten Sie kurz die übrigen Cheatsheets; später werden die Ihnen
  vielleicht von Nutzen sein.
\item
  Führen Sie diese Syntax aus:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{meine_coole_variable <-}\StringTok{ }\DecValTok{10}
\NormalTok{meine_coole_var1able }
\end{Highlighting}
\end{Shaded}

Woher rührt der Fehler?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Korrigieren Sie die Syntax:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(dplyer)}
\end{Highlighting}
\end{Shaded}

\texttt{y\ \textless{}-\ Hallo\ R!}

\texttt{Hallo\ R\ \textless{}-\ 1}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Hallo_R }\OperatorTok{<}\StringTok{ }\OperatorTok{-}\StringTok{ }\DecValTok{1}
\end{Highlighting}
\end{Shaded}

Richtig oder Falsch???\footnote{R, F: die Daten müssen sinnvoll
  zusammengefasst werden, F, F, F: Wenn er ehrlich sein sollte, dann ist
  das Ereignis `10 von 10' selten}

\BeginKnitrBlock{rmdexercises}
Richtig oder Falsch!?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Statistik wird gemeinhin in zwei Bereiche unterteilt:
  Deskriptivstatistik und Inferenzstatistik.
\item
  Unter Deskriptivstatistik versteht man, Daten zu beschreiben. Dazu ist
  jede Art von Beschreibung sinnvoll, vorausgesetzt es wird eine
  konsistente Regel eingesetzt.
\item
  Unter Abduktion versteht man den Schluss vom Allgemeinen auf das
  Konkrete.
\item
  Wirft jemand bei 10 von 10 Münzwürfen `Kopf', so muss er ein Betrüger
  sein.
\item
  Wirft jemand bei 10 von 10 Münzwürfen `Kopf', so ist die
  Wahrscheinlichkeit groß, dass er ein Betrüger ist.
\end{enumerate}
\EndKnitrBlock{rmdexercises}

\section{Befehlsübersicht}\label{befehlsubersicht}

Tabelle \ref{tab:befehle-rahmen} stellt die Befehle dieses Kapitels dar.

\begin{table}

\caption{\label{tab:befehle-rahmen}Befehle des Kapitels 'Rahmen'}
\centering
\begin{tabular}[t]{l|l}
\hline
Paket::Funktion & Beschreibung\\
\hline
install.packages("x") & Installiert Paket "x" (nicht: Paket "X")\\
\hline
library & lädt ein Paket\\
\hline
<- & Weist einer Variablen einen Wert zu\\
\hline
c & erstellt eine Spalte/ einen Vektor\\
\hline
\end{tabular}
\end{table}

Diese Befehle ``wohnen'' alle im Standard-R; es ist für diese Befehle
nicht nötig, zusätzliche Pakete zu installieren/ laden.

\section{Verweise}\label{verweise}

\begin{itemize}
\item
  Chester Ismay erläutert einige Grundlagen von R und RStudio, die für
  Datenanalyse hilfreich sind:
  \url{https://bookdown.org/chesterismay/rbasics/}.
\item
  Roger Peng und Kollegen bieten hier einen Einstieg in Data Science mit
  R: \url{https://bookdown.org/rdpeng/artofdatascience/}
\item
  Wickham und Grolemund (\protect\hyperlink{ref-r4ds}{2016}) geben einen
  hervorragenden Überblick in das Thema dieses Buches; ihr Buch ist sehr
  zu empfehlen.
\item
  Wer einen stärker an der Statistik orientierten Zugang sucht, aber
  ``mathematisch sanft'' behandelt werden möchte, wird bei James et al.
  (\protect\hyperlink{ref-introstatlearning}{2013}\protect\hyperlink{ref-introstatlearning}{b})
  glücklich oder zumindest fündig werden.
\item
  Uwe Ligges (\emph{Programmieren mit R}
  \protect\hyperlink{ref-ligges}{2009}) `Programmieren mit R' gibt einen
  tieferen Einstieg in die Grundlagen von R.
\item
  Wer ganz tief ein- und abtauchen möchte in R, dem sei - solide
  Grundkenntnisse vorausgesetzt - Hadley Wickhams Wickham
  (\protect\hyperlink{ref-wickham2014advanced}{2014}\protect\hyperlink{ref-wickham2014advanced}{a})
  `Advanced R' ans Herz gelegt.
\end{itemize}

\chapter{Daten einlesen}\label{daten-einlesen}

\begin{center}\includegraphics[width=0.3\linewidth]{images/FOM} \end{center}

\begin{center}\includegraphics[width=0.1\linewidth]{images/licence} \end{center}

\BeginKnitrBlock{rmdcaution}
Lernziele:

\begin{itemize}
\tightlist
\item
  Wissen, auf welchen Wegen man Daten in R hineinbekommt.
\item
  Wissen, was eine CSV-Datei ist.
\item
  Wissen, was UTF-8 bedeutet.
\item
  Erläutern können, was R unter dem ``working directory'' versteht.
\item
  Erkennen können, ob eine Tabelle in Normalform vorliegt.
\item
  Daten aus R hinauskriegen (exportieren).
\end{itemize}
\EndKnitrBlock{rmdcaution}

In diesem Kapitel werden folgende Pakete benötigt:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(readr)  }\CommentTok{# Daten einlesen}
\KeywordTok{library}\NormalTok{(tidyverse) }\CommentTok{# Datenjudo und Visualisierung}
\end{Highlighting}
\end{Shaded}

Dieses Kapitel beantwortet eine Frage: ``Wie kriege ich Daten in
vernünftiger Form in R hinein?''.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/tidy/Einlesen} 

}

\caption{Daten sauber einlesen}\label{fig:step-Einlesen}
\end{figure}

\section{Daten in R importieren}\label{daten-in-r-importieren}

In R kann man ohne Weiteres verschiedene, gebräuchliche (Excel oder CSV)
oder weniger gebräuchliche (Feather\footnote{\url{https://cran.r-project.org/web/packages/feather/index.html}})
Datenformate einlesen. In RStudio lässt sich dies z.B. durch einen
schnellen Klick auf \texttt{Import\ Dataset} im Reiter
\texttt{Environment} erledigen\footnote{Um CSV-Dateien zu laden wird
  durch den Klick im Hintergrund das Paket \texttt{readr} verwendet
  (Wickham, Hester, und Francois
  \protect\hyperlink{ref-readr}{2016}\protect\hyperlink{ref-readr}{a});
  die entsprechende Syntax wird in der Konsole ausgegeben, so dass man
  sie sich anschauen und weiterverwenden kann}.

\subsection{Excel-Dateien importieren}\label{excel-dateien-importieren}

Am einfachsten ist es, eine Excel-Datei (.xls oder .xlsx) über die
RStudio-Oberfläche zu importieren; das ist mit ein paar Klicks
geschehen\footnote{im Hintergrund wird das Paket \texttt{readxl}
  verwendet}:

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/tidy/import_RStudio} 

}

\caption{Daten einlesen (importieren) mit RStudio}\label{fig:data-import-RStudio}
\end{figure}

Es ist für bestimmte Zwecke sinnvoll, nicht zu klicken, sondern die
Syntax einzutippen. Zum Beispiel: Wenn Sie die komplette Analyse als
Syntax in einer Datei haben (eine sog. ``Skriptdatei''), dann brauchen
Sie (in RStudio) nur alles auszuwählen und auf \texttt{Run} zu klicken,
und die komplette Analyse läuft durch! Die Erfahrung zeigt, dass das ein
praktisches Vorgehen ist.

\BeginKnitrBlock{rmdcaution}
Daten (CSV, Excel,\ldots{}) können Sie \emph{nicht} öffnen über
\texttt{File\ \textgreater{}\ Open\ File\ ...}. Dieser Weg ist
Skript-Dateien und R-Daten-Objekten vorbehalten.
\EndKnitrBlock{rmdcaution}

\subsection{Daten aus R-Paketen
importieren}\label{daten-aus-r-paketen-importieren}

In R-Paketen wohnen nicht nur Funktionen, sondern auch Daten. Diese
Daten kann man mit dem Befehl \texttt{data} laden, dem man den Namen des
zu ladenen Datensatzes \texttt{dataset} und seines Heimatpakets
\texttt{paket} übergibt: \texttt{data(dataset,\ package\ =\ "paket")}.
Natürlich muss das Paket installiert sein. Zum Beispiel:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(movies, }\DataTypeTok{package =} \StringTok{"ggplot2movies"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Daten im R-Format laden}\label{daten-im-r-format-laden}

Das R-Datenformat erkennt man an der R-Endung \texttt{.rda} oder
\texttt{RData}. Dateien mit diesem Format kann man in RStudio über
\texttt{File\ \textgreater{}\ Open\ File...} öffnen. Oder mit dem Befehl
\texttt{load(file)}, wobei \texttt{file} der Dateiname ist, also z.B.
\texttt{extra.RData}. Mit dem Schwesterbefehl \texttt{save} können Sie
ein Objekt im R-Datenformat speichern, z.B.
\texttt{save(stats\_test,\ file\ =\ "stats\_test.RData")}.

\subsection{CSV-Dateien importieren}\label{csv-dateien-importieren}

Die gebräuchlichste Form von Daten für statistische Analysen ist
wahrscheinlich das CSV-Format. Das ist ein einfaches Format, basierend
auf einer Textdatei. Schauen Sie sich mal diesen Auszug aus einer
CSV-Datei an.

\begin{verbatim}
row_number,date_time,study_time,self_eval,interest,score
1,05.01.2017 13:57:01,5,8,5,29
2,05.01.2017 21:07:56,3,7,3,29
3,05.01.2017 23:33:47,5,10,6,40
4,06.01.2017 09:58:05,2,3,2,18
5,06.01.2017 14:13:08,4,8,6,34
6,06.01.2017 14:21:18,NA,NA,NA,39
\end{verbatim}

Erkennen Sie das Muster? Die erste Zeile gibt die ``Spaltenköpfe''
wieder, also die Namen der Variablen. Hier sind es 6 Spalten; die fünft
heißt ``score'' und gibt die Punkte eines Studierenden in einer
Statistikklausur wieder. Die Spalten sind offenbar durch Komma
\texttt{,} voneinander getrennt. Dezimalstellen sind in amerikanischer
Manier mit einem Punkt \texttt{.} dargestellt. Die Daten sind
``rechteckig''; alle Spalten haben gleich viele Zeilen und umgekehrt
alle Spalten gleich viele Zeilen. Man kann sich diese Tabelle gut als
Excel-Tabelle mit Zellen vorstellen, in denen z.B. ``row\_number''
(Zelle oben links) oder ``39'' (Zelle unten rechts) steht.

An einigen Stelle steht \texttt{NA}. Das ist Errisch für ``fehlender
Wert''. Häufig wird die Zelle auch leer gelassen, um auszudrücken, dass
ein Wert hier fehlt (hört sich nicht ganz doof an). Aber man findet alle
möglichen Ideen, um fehlende Werte darzustellen. Ich rate von allen
anderen ab; führt nur zu Verwirrung.

Lesen wir diese Daten jetzt ein:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"data/stats_test.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Übrigens, Sie können die Daten (als CSV) für diesen Kurs auch über diese
URL importieren. Z.B. den Datensatz \texttt{stats\_test}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prada_stats_test_url <-}
\StringTok{  }\KeywordTok{paste0}\NormalTok{(}\StringTok{"https://raw.github.com/"}\NormalTok{,  }\CommentTok{# Webseite}
         \StringTok{"sebastiansauer/"}\NormalTok{,  }\CommentTok{# Nutzer}
         \StringTok{"Praxis_der_Datenanalyse/"}\NormalTok{,  }\CommentTok{# Projekt/Repositorium}
         \StringTok{"master/"}\NormalTok{,  }\CommentTok{# Variante}
         \StringTok{"data/stats_test.csv"}\NormalTok{)  }\CommentTok{# Ordner und Dateinamen}

\NormalTok{stats_test <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(prada_stats_test_url)}
\end{Highlighting}
\end{Shaded}

Analog gehen Sie für die anderen PraDa-Datensätze vor (vgl. Kapitel
\ref{daten}).

\subsubsection{Vorsicht bei nicht-amerikanisch kodierten
Textdateien}\label{vorsicht-bei-nicht-amerikanisch-kodierten-textdateien}

Der Befehl \texttt{read.csv} liest also eine CSV-Datei, was uns jetzt
nicht übermäßig überrascht. Aber Achtung: Wenn Sie aus einem Excel mit
\emph{deutscher} Einstellung eine CSV-Datei exportieren, wird diese
CSV-Datei als Spaltentrennung \texttt{;} (Strichpunkt) und als
Dezimaltrennzeichen \texttt{,} verwenden. Da der Befehl
\texttt{read.csv} laut amerikanischen Standard mit Komma als
Spaltentrennung und Punkt als Dezimaltrennzeichen arbeitet, müssen wir
die deutschen Sonderlocken explizit angeben, z.B. so:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# nicht ausführen:}
\NormalTok{daten_deutsch <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"daten_deutsch.csv"}\NormalTok{, }\DataTypeTok{sep =} \StringTok{";"}\NormalTok{, }\DataTypeTok{dec =} \StringTok{"."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Dabei steht \texttt{sep} (separator) für das Trennzeichen zwischen den
Spalten und \texttt{dec} für das Dezimaltrennzeichen. R bietet eine
Kurzfassung für \texttt{read.csv} mit diesen Parametern:
\texttt{read.csv2("daten\_deutsch.csv")}.

Man kommt hier auch mit ``Klicken statt Tippen'' zum Ziel; in der Maske
von ``Import Dataset'' (für CSV-Dateien) gibt es den Auswahlpunkt
``Delimiter'' (Trennzeichen). Dort kann man das Komma durch einen
Strichpunkt (oder was auch immer) ersetzen. Es hilft, im Zweifel, die
Textdatei vorab mit einem Texteditor zu öffnen.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/tidy/delimiter} 

}

\caption{Trennzeichen einer CSV-Datei in RStudio einstellen}\label{fig:rstudio-delimiter}
\end{figure}

\section{Normalform einer Tabelle}\label{normalform-einer-tabelle}

Tabellen in R werden als \texttt{data\ frames} (``Dataframe'' auf
Denglisch; moderner: als \texttt{tibble}, Tibble kurz für ``Table-df'')
bezeichnet. Tabellen sollten in ``Normalform'' vorliegen (``tidy''),
bevor wir weitere Analysen starten. Unter Normalform verstehen sich
folgende Punkte:

\begin{itemize}
\tightlist
\item
  Es handelt sich um einen Dataframe, also um eine Tabelle mit Spalten
  mit Namen und gleicher Länge; eine Datentabelle in rechteckiger Form
  und die Spalten haben einen Namen.
\item
  In jeder Zeile steht eine Beobachtung, in jeder Spalte eine Variable.
\item
  Fehlende Werte sollten sich in \emph{leeren} Zellen niederschlagen.
\item
  Daten sollten nicht mit Farbmarkierungen o.ä. kodiert werden.
\item
  Es gibt keine Leerzeilen und keine Leerspalten.
\item
  In jeder Zelle steht ein Wert.
\item
  Am besten verwendet man keine Sonderzeichen verwenden und keine
  Leerzeichen in Variablennamen und -werten, sondern nur Ziffern und
  Buchstaben und Unterstriche.
\item
  Variablennamen dürfen nicht mit einer Zahl beginnen.
\end{itemize}

Abbildung \ref{fig:tidy1} visualisiert die Bestimmungsstücke eines
Dataframes (Wickham und Grolemund \protect\hyperlink{ref-r4ds}{2016}):

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/tidy/tidy-1} 

}

\caption{Schematische Darstellung eines Dataframes in Normalform}\label{fig:tidy1}
\end{figure}

Der Punkt \emph{Jede Zeile eine Beobachtung, jede Spalte eine Variable,
jede Zelle ein Wert} verdient besondere Beachtung. Betrachten Sie das
Beispiel in Abbildung \ref{fig:lang-breit}.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/tidy/breit_lang} 

}

\caption{Dieselben Daten - einmal breit, einmal lang}\label{fig:lang-breit}
\end{figure}

In der rechten Tabelle sind die Variablen \texttt{Quartal} und
\texttt{Umsatz} klar getrennt; jede hat ihre eigene Spalte. In der
linken Tabelle hingegen sind die beiden Variablen vermischt. Sie haben
nicht mehr ihre eigene Spalte, sondern sind über vier Spalten verteilt.
Die rechte Tabelle ist ein Beispiel für eine Tabelle in Normalform, die
linke nicht.

\section{Tabelle in Normalform bringen}\label{normalform}

Eine der ersten Aktionen einer Datenanalyse sollte also die
``Normalisierung'' Ihrer Tabelle sein. In R bietet sich dazu das Paket
\texttt{tidyr} an, mit dem die Tabelle von \emph{Breit- auf Langformat}
(und wieder zurück) geschoben werden kann.

Abb. \ref{fig:gather-spread} zeigt ein Beispiel dazu.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/tidy/gather_spread-crop} 

}

\caption{Mit 'gather' und 'spread' wechselt man von der breiten Form zur langen Form}\label{fig:gather-spread}
\end{figure}

Warum ist es wichtig, von der ``breiten'' (links in Abb.
\ref{fig:gather-spread}) zur ``langen'' oder ``Normalform'' (rechts in
Abb. \ref{fig:gather-spread}) zu wechseln. Ganz einfach: viele Befehle
(allgemeiner: Tätigkeiten) verlangen die Normalform; hin und wieder sind
aber die Tabellen von ihrem Schöpfer in breiter Form geschaffen worden.
Zum Beispiel erwartet \texttt{ggplot2} - und viele andere
Diagrammbefehle - dass man \emph{einer} Achse \emph{eine} Spalte
(Variable) zuweist, z.B. die Variable ``Umsatz'' auf die Y-Achse. Der
X-Achse könnten wir dann z.B. die Variable ``Quartal'' packen (s. Abb.
\ref{fig:bsp-abb}).

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/tidy/bsp_diagramm-crop} 

}

\caption{Ein Beispiel für eine Abbildung zu einer Normalform-Tabelle}\label{fig:bsp-abb}
\end{figure}

Um von der breiten Form zur langen Form zu kommen, kann man den Befehl
\texttt{tidyr::gather} nehmen. Von der langen Form zur breiten Form gibt
es \texttt{tidyr::spread}. Also etwa:

\begin{verbatim}
library(tidyr)
df_lang <- gather(df_breit, key = "Quartal", value = "Umsatz")

df_breit <- spread(df_lang, Quartal, Umsatz)
\end{verbatim}

Dabei baut \texttt{gather} den Dataframe so um, dass nur zwei Spalten
übrig bleiben (s. Abb. \ref{fig:gather-spread}). Eine Spalte nur
\emph{Spaltennamen} (``Q1'', ``Q2'', \ldots{}) enthält; diese Spalte
nennt \texttt{gather} im Standard \texttt{key}. Die zweite Spalte
enthält die Werte (z.B. Umsätze), die vormals über mehrere Spalten
verstreut waren. Diese Spalte heißt per Default \texttt{value}. Im
Beispiel oben macht die Spalte \texttt{ID} bei dem Spiel ``Aus vielen
Spalten werden zwei'' nicht mit. Möchte man eine Spalte aussparen, so
schreibt man das bei \texttt{gather} so:

\begin{verbatim}
df_lang <- gather(df_breit, key = "Quartal", value = "Umsatz", -ID)
\end{verbatim}

In Kapitel \ref{case-movies} werden wir dazu ein Fallstudie einüben.

\section{Textkodierung}\label{textkodierung}

Öffnet man eine Textdatei mit einem Texteditor seiner Wahl, so sieht
man\ldots{} Text und sonst nichts, also keine Formatierung etc. Eine
Textdatei besteht aus Text und sonst nichts (daher der Name\ldots{}).
Auch eine R-Skript-Datei (\texttt{Coole\_Syntax.R}) ist eine Textdatei.
Technisch gesprochen werden nur die Textzeichen gespeichert, sonst
nichts; im Gegensatz dazu speichert eine Word-Datei noch mehr, z.B.
Formatierung. Jetzt steht in der Textdatei der Code ``42'' für den
nächsten Buchstaben. Ja, ist das jetzt ein ``A'', oder ein ``Ä'' oder
vielleicht ein griechischer Buchstabe? Woher weiß der Computer das
eigentlich? Die Antwort ist: Er braucht eine Art Übersetzungstabelle
oder Kodierungstafel. Mehrere solcher Kodierungstafeln existieren. Die
gebräuchlichste im Internet heißt \emph{UTF-8}\footnote{\url{https://de.wikipedia.org/wiki/UTF-8}}.
Leider benutzen unterschiedliche Betriebssysteme unterschiedliche
Kodierungstafeln, was zu Verwirrung führt. Ich empfehle, Ihre
Textdateien als UTF-8 zu kodieren. RStudio fragt sie, wie eine Textdatei
kodiert werden soll. Sie können auch unter
\texttt{File\ \textgreater{}\ Save\ with\ Encoding...} die Kodierung
einer Textdatei festlegen.

\begin{quote}
Speichern Sie R-Textdateien wie Skripte stets mit UTF-8-Kodierung ab.
\end{quote}

Wie bekommt man seine Daten wieder aus R raus (``ich will zu Excel
zurück!'')?

Eine Möglichkeit bietet die Funktion \texttt{write.csv}; sie schreibt
eine CSV-Datei:

\begin{verbatim}
write.csv(name_der_tabelle, "Dateiname.csv")
\end{verbatim}

Mit \texttt{help(write.csv)} bekommt man mehr Hinweise dazu. Beachten
Sie, dass immer in das aktuelle Arbeitsverzeichnis geschrieben wird.

\section{Befehlsübersicht}\label{befehlsubersicht-1}

Tabelle \ref{tab:befehle-tidy} stellt die Befehle dieses Kapitels dar.

\begin{table}

\caption{\label{tab:befehle-tidy}Befehle des Kapitels 'Daten einlesen'}
\centering
\begin{tabular}[t]{l|l}
\hline
Paket::Funktion & Beschreibung\\
\hline
read.csv & Liest eine CSV-Datei ein.\\
\hline
write.csv & Schreibt einen Dateframe in eine CSV-Datei.\\
\hline
tidyr::gather & Macht aus einem "breiten" Dataframe einen "langen".\\
\hline
tidyr::separate & "Zieht" Spalten auseinander.\\
\hline
\end{tabular}
\end{table}

\section[Aufgaben]{\texorpdfstring{Aufgaben\footnote{F, R, F, R, R, R,
  F, F}}{Aufgaben}}\label{aufgaben-1}

\BeginKnitrBlock{rmdexercises}
Richtig oder Falsch!?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In CSV-Dateien dürfen Spalten \emph{nie} durch Komma getrennt sein.
\item
  RStudio bietet die Möglichkeit, CSV-Dateien per Klick zu importieren.
\item
  RStudio bietet \emph{nicht} die Möglichkeit, CSV-Dateien per Klick zu
  importieren.
\item
  ``Deutsche'' CSV-Dateien verwenden als Spalten-Trennzeichen einen
  Strichpunkt.
\item
  In einer Tabelle in Normalform stehen in jeder Zeile eine Beobachtung.
\item
  In einer Tabelle in Normalform stehen in jeder Spalte eine Variable.
\item
  R stellt fehlende Werte mit einem Fragezeichen \texttt{?} dar.
\item
  Um Excel-Dateien zu importieren, kann man den Befehl \texttt{read.csv}
  verwenden.
\end{enumerate}
\EndKnitrBlock{rmdexercises}

\section{Verweise}\label{verweise-1}

\begin{itemize}
\tightlist
\item
  \emph{R for Data Science} bietet umfangreiche Unterstützung zu diesem
  Thema (Wickham und Grolemund \protect\hyperlink{ref-r4ds}{2016}).
\end{itemize}

\chapter{Datenjudo}\label{Datenjudo}

\begin{center}\includegraphics[width=0.3\linewidth]{images/FOM} \end{center}

\begin{center}\includegraphics[width=0.1\linewidth]{images/licence} \end{center}

\BeginKnitrBlock{rmdcaution}
Lernziele:

\begin{itemize}
\tightlist
\item
  Die zentralen Ideen der Datenanalye mit dplyr verstehen.
\item
  Typische Probleme der Datenanalyse schildern können.
\item
  Zentrale \texttt{dplyr}-Befehle anwenden können.
\item
  \texttt{dplyr}-Befehle kombinieren können.
\item
  Die Pfeife anwenden können.
\item
  Werte umkodieren und ``binnen'' können.
\end{itemize}
\EndKnitrBlock{rmdcaution}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/Datenjudo/Aufbereiten} 

}

\caption{Daten aufbereiten}\label{fig:fig-datenjudo}
\end{figure}

In diesem Kapitel werden folgende Pakete benötigt:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)  }\CommentTok{# Datenjudo}
\KeywordTok{library}\NormalTok{(stringr)   }\CommentTok{# Texte bearbeiten}
\KeywordTok{library}\NormalTok{(car)  }\CommentTok{# für 'recode'}
\KeywordTok{library}\NormalTok{(desctable)  }\CommentTok{# Statistiken auf einen Streich}
\KeywordTok{library}\NormalTok{(lsr)  }\CommentTok{# für Befehl `aad`}
\end{Highlighting}
\end{Shaded}

Das Paket \texttt{tidyverse} lädt \texttt{dplyr}, \texttt{ggplot2} und
weitere Pakete\footnote{für eine Liste s.
  \texttt{tidyverse\_packages(include\_self\ =\ TRUE)}}. Daher ist es
komfortabler, \texttt{tidyverse} zu laden, damit spart man sich
Tipparbeit. Die eigentliche Funktionalität, die wir in diesem Kapitel
nutzen, kommt aus dem Paket \texttt{dplyr}.

Mit \emph{Datenjudo}\index{Datenjudo} ist gemeint, die Daten für die
eigentliche Analyse ``aufzubereiten''. Unter
\emph{Aufbereiten}\index{Datenjudo} ist hier das Umformen, Prüfen,
Bereinigen, Gruppieren und Zusammenfassen von Daten gemeint. Die
deskriptive Statistik fällt unter die Rubrik Aufbereiten. Kurz gesagt:
Alles, was tut, nachdem die Daten ``da'' sind und bevor man mit
anspruchsvoller(er) Modellierung beginnt.

Ist das Aufbereiten von Daten auch nicht statistisch anspruchsvoll, so
ist es trotzdem von großer Bedeutung und häufig recht zeitintensiv. Eine
Anekdote zur Relevanz der Datenaufbereitung, die (so will es die
Geschichte) mir an einer Bar nach einer einschlägigen Konferenz erzählt
wurde (daher keine Quellenangabe, Sie verstehen\ldots{}). Eine
Computerwissenschaftlerin aus den USA (deutschen Ursprungs) hatte einen
beeindruckenden ``Track Record'' an Siegen in Wettkämpfen der
Datenanalyse. Tatsächlich hatte sie keine besonderen, raffinierten
Modellierungstechniken eingesetzt; klassische Regression war ihre
Methode der Wahl. Bei einem Wettkampf, bei dem es darum ging, Krebsfälle
aus Krankendaten vorherzusagen (z.B. von Röntgenbildern) fand sie nach
langem Datenjudo heraus, dass in die ``ID-Variablen'' Information
gesickert war, die dort nicht hingehörte und die sie nutzen konnte für
überraschend (aus Sicht der Mitstreiter) gute Vorhersagen zu
Krebsfällen. Wie war das möglich? Die Daten stammten aus mehreren
Kliniken, jede Klinik verwendete ein anderes System, um IDs für
Patienten zu erstellen. Überall waren die IDs stark genug, um die
Anonymität der Patienten sicherzustellen, aber gleich wohl konnte man
(nach einigem Judo) unterscheiden, welche ID von welcher Klinik stammte.
Was das bringt? Einige Kliniken waren reine Screening-Zentren, die die
Normalbevölkerung versorgte. Dort sind wenig Krebsfälle zu erwarten.
Andere Kliniken jedoch waren Onkologie-Zentren für bereits bekannte
Patienten oder für Patienten mit besonderer Risikolage. Wenig
überraschen, dass man dann höhere Krebsraten vorhersagen kann.
Eigentlich ganz einfach; besondere Mathe steht hier (zumindest in dieser
Geschichte) nicht dahinter. Und, wenn man den Trick kennt, ganz einfach.
Aber wie so oft ist es nicht leicht, den Trick zu finden. Sorgfältiges
Datenjudo hat hier den Schlüssel zum Erfolg gebracht.

\section{Typische Probleme der
Datenaufbereitung}\label{typische-probleme-der-datenaufbereitung}

Bevor man seine Statistik-Trickkiste so richtig schön aufmachen kann,
muss man die Daten häufig erst noch in Form bringen. Das ist nicht
schwierig in dem Sinne, dass es um komplizierte Mathe ginge. Allerdings
braucht es mitunter recht viel Zeit und ein paar (oder viele)
handwerkliche Tricks sind hilfreich. Hier soll das folgende Kapitel
helfen.

Typische Probleme, die immer wieder auftreten, sind:

\begin{itemize}
\tightlist
\item
  \emph{Fehlende Werte}: Irgend jemand hat auf eine meiner schönen
  Fragen in der Umfrage nicht geantwortet!
\item
  \emph{Unerwartete Daten}: Auf die Frage, wie viele Facebook-Freunde er
  oder sie habe, schrieb die Person ``I like you a lot''. Was tun???
\item
  \emph{Daten müssen umgeformt werden}: Für jede der beiden Gruppen
  seiner Studie hat Joachim einen Google-Forms-Fragebogen aufgesetzt.
  Jetzt hat er zwei Tabellen, die er ``verheiraten'' möchte. Geht das?
\item
  \emph{Neue Variablen (Spalten) berechnen}: Ein Student fragt nach der
  Anzahl der richtigen Aufgaben in der Statistik-Probeklausur. Wir
  wollen helfen und im entsprechenden Datensatz eine Spalte erzeugen, in
  der pro Person die Anzahl der richtig beantworteten Fragen steht.
\end{itemize}

\section{\texorpdfstring{Daten aufbereiten mit
\texttt{dplyr}}{Daten aufbereiten mit dplyr}}\label{daten-aufbereiten-mit-dplyr}

Willkommen in der Welt von \texttt{dyplr}! \texttt{dplyr} hat seinen
Namen, weil es sich ausschließlich um \emph{D}ataframes bemüht; es
erwartet einen Dataframe als Eingabe und gibt einen Dataframe zurück
(zumindest bei den meisten Befehlen).

\subsection{\texorpdfstring{Die zwei Prinzipien von
\texttt{dplyr}}{Die zwei Prinzipien von dplyr}}\label{die-zwei-prinzipien-von-dplyr}

Es gibt viele Möglichkeiten, Daten mit R aufzubereiten;
\texttt{dplyr}\footnote{\url{https://cran.r-project.org/web/packages/dplyr/index.html}}
ist ein populäres Paket dafür. \texttt{dplyr} basiert auf zwei Ideen:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Lego-Prinzip} Komplexe Datenanalysen in Bausteine zerlegen (vgl.
  Abb. \ref{fig:bausteine}).
\item
  \emph{Durchpfeifen}: Alle Operationen werden nur auf Dataframes
  angewendet; jede Operation erwartet einen Dataframe als Eingabe und
  gibt wieder einen Dataframe aus (vgl. Abb.
  \ref{fig:durchpfeifen-allgemein}).
\end{enumerate}

Das \emph{erste Prinzip} von \texttt{dplyr} ist, dass es nur ein paar
\emph{wenige Grundbausteine} geben sollte, die sich gut kombinieren
lassen. Sprich: Wenige grundlegende Funktionen mit eng umgrenzter
Funktionalität. Der Autor, Hadley Wickham, sprach einmal in einem Forum
(citation needed\ldots{}), dass diese Befehle wenig können, das Wenige
aber gut. Ein Nachteil dieser Konzeption kann sein, dass man recht viele
dieser Bausteine kombinieren muss, um zum gewünschten Ergebnis zu
kommen. Außerdem muss man die Logik des Baukastens gut verstanden habe -
die Lernkurve ist also erstmal steiler. Dafür ist man dann nicht darauf
angewiesen, dass es irgendwo ``Mrs Right'' gibt, die genau das kann, was
ich will. Außerdem braucht man sich auch nicht viele Funktionen merken.
Es reicht einen kleinen Satz an Funktionen zu kennen (die
praktischerweise konsistent in Syntax und Methodik sind). Diese
Bausteine sind typische Tätigkeiten im Umgang mit Daten; nichts
Überraschendes. Wir schauen wir uns diese Bausteine gleich näher an.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/Datenjudo/Bausteine_dplyr_crop} 

}

\caption{Lego-Prinzip: Zerlege eine komplexe Struktur in einfache Bausteine}\label{fig:bausteine}
\end{figure}

Das \emph{zweite Prinzip} von \texttt{dplyr} ist es, einen Dataframe von
Operation zu Operation \emph{durchzureichen.} \texttt{dplyr} arbeitet
also \emph{nur} mit Dataframes. Jeder Arbeitsschritt bei \texttt{dplyr}
erwartet einen Dataframe als Eingabe und gibt im Gegenzug wieder einen
Dataframe aus.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/Datenjudo/durchpfeifen_allgemein_crop} 

}

\caption{Durchpfeifen: Ein Dataframe wird von Operation zu Operation weitergereicht}\label{fig:durchpfeifen-allgemein}
\end{figure}

Werfen wir einen Blick auf ein paar typische Bausteine von
\texttt{dplyr}.

\section{\texorpdfstring{Zentrale Bausteine von
\texttt{dplyr}}{Zentrale Bausteine von dplyr}}\label{zentrale-bausteine-von-dplyr}

\subsection{\texorpdfstring{Zeilen filtern mit
\texttt{filter}}{Zeilen filtern mit filter}}\label{zeilen-filtern-mit-filter}

Häufig will man bestimmte Zeilen aus einer Tabelle filtern;
\texttt{filter}\index{dplyr::filter}. Zum Beispiel man arbeitet für die
Zigarettenindustrie und ist nur an den Rauchern interessiert (die im
Übrigen unser Gesundheitssystem retten (Krämer
\protect\hyperlink{ref-kraemer2011wir}{2011})), nicht an Nicht-Rauchern;
es sollen die nur Umsatzzahlen des letzten Quartals untersucht werden,
nicht die vorherigen Quartale; es sollen nur die Daten aus Labor X
(nicht Labor Y) ausgewertet werden etc.

Abb. \ref{fig:fig-filter} zeigt ein Sinnbild für \texttt{filter}.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/Datenjudo/filter} 

}

\caption{Zeilen filtern}\label{fig:fig-filter}
\end{figure}

Merke:

\begin{quote}
Die Funktion \texttt{filter} filtert Zeilen aus einem Dataframe.
\end{quote}

Schauen wir uns einige Beispiel an; zuerst die Daten laden nicht
vergessen. Achtung: ``Wohnen'' die Daten in einem Paket, muss dieses
Paket installiert sein, damit man auf die Daten zugreifen kann.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(profiles, }\DataTypeTok{package =} \StringTok{"okcupiddata"}\NormalTok{)  }\CommentTok{# Das Paket muss installiert sein}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df_frauen <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(profiles, sex }\OperatorTok{==}\StringTok{ "f"}\NormalTok{)  }\CommentTok{# nur die Frauen}
\NormalTok{df_alt <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(profiles, age }\OperatorTok{>}\StringTok{ }\DecValTok{70}\NormalTok{)  }\CommentTok{# nur die alten Menschen}
\NormalTok{df_alte_frauen <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(profiles, age }\OperatorTok{>}\StringTok{ }\DecValTok{70}\NormalTok{, sex }\OperatorTok{==}\StringTok{ "f"}\NormalTok{) }
\CommentTok{# nur die alten Frauen, d.h. UND-Verknüpfung}
\NormalTok{df_mittelalt <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(profiles, }\KeywordTok{between}\NormalTok{(age, }\DecValTok{35}\NormalTok{, }\DecValTok{60}\NormalTok{))}
\CommentTok{# zwischen 35 und 60}

\NormalTok{df_nosmoke_nodrinks <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(profiles, smokes }\OperatorTok{==}\StringTok{ "no"} \OperatorTok{|}\StringTok{ }\NormalTok{drinks }\OperatorTok{==}\StringTok{ "not at all"}\NormalTok{) }
\CommentTok{# liefert alle Personen, die Nicht-Raucher *oder* Nicht-Trinker sind}
\end{Highlighting}
\end{Shaded}

Gar nicht so schwer, oder? Allgemeiner gesprochen werden diejenigen
Zeilen gefiltert (also behalten bzw. zurückgeliefert), für die das
Filterkriterium \texttt{TRUE} ist.

\texttt{filter} ist deutlich einfacher (und klarer) als Standard-R.
Vergleichen Sie mal:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{filter}\NormalTok{(profiles, age }\OperatorTok{>}\StringTok{ }\DecValTok{70}\NormalTok{, sex }\OperatorTok{==}\StringTok{ "f"}\NormalTok{, drugs }\OperatorTok{==}\StringTok{ "sometimes"}\NormalTok{)}

\CommentTok{# base-R:}

\NormalTok{profiles[df}\OperatorTok{$}\NormalTok{age }\OperatorTok{>}\StringTok{ }\DecValTok{70} \OperatorTok{&}\StringTok{ }\NormalTok{df}\OperatorTok{$}\NormalTok{sex }\OperatorTok{==}\StringTok{ "f"} \OperatorTok{&}\StringTok{ }\NormalTok{df}\OperatorTok{$}\NormalTok{drugs }\OperatorTok{==}\StringTok{ "sometimes"}\NormalTok{, ]}
\end{Highlighting}
\end{Shaded}

\BeginKnitrBlock{rmdcaution}
Manche Befehle wie \texttt{filter} haben einen Allerweltsnamen; gut
möglich, dass ein Befehl mit gleichem Namen in einem anderen (geladenen)
Paket existiert. Das kann dann zu Verwirrungen führen - und kryptischen
Fehlern. Im Zweifel den Namen des richtigen Pakets ergänzen, und zwar
zum Beispiel so: \texttt{dplyr::filter(...)}.
\EndKnitrBlock{rmdcaution}

\subsubsection[Aufgaben]{\texorpdfstring{Aufgaben\footnote{F, R, F, F,
  R, R}}{Aufgaben}}\label{aufgaben-2}

\BeginKnitrBlock{rmdexercises}
Richtig oder Falsch!?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{filter} filtert Spalten.
\item
  \texttt{filter} ist eine Funktion aus dem Paket \texttt{dplyr}.
\item
  \texttt{filter} erwartet als ersten Parameter das Filterkriterium.
\item
  \texttt{filter} lässt nur ein Filterkriterium zu.
\item
  Möchte man aus dem Datensatz \texttt{profiles} (\texttt{okcupiddata})
  die Frauen filtern, so ist folgende Syntax korrekt:
  \texttt{filter(profiles,\ sex\ ==\ "f")}.
\item
  \texttt{filter(profiles,\ age\ \textgreater{}\ 35\ \textbar{}\ age\ \textgreater{}\ 60)}
  filtert die mittelalten Frauen (zwischen 35 und 60)
\end{enumerate}
\EndKnitrBlock{rmdexercises}

\subsection{\texorpdfstring{Spalten wählen mit
\texttt{select}}{Spalten wählen mit select}}\label{spalten-wahlen-mit-select}

Das Gegenstück zu \texttt{filter} ist
\texttt{select}\index{dplyr::select}; dieser Befehl liefert die
gewählten Spalten zurück. Das ist häufig praktisch, wenn der Datensatz
sehr ``breit'' ist, also viele Spalten enthält. Dann kann es
übersichtlicher sein, sich nur die relevanten auszuwählen. Abb.
\ref{fig:fig-select} zeigt Sinnbild für diesen Befehl:

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/Datenjudo/select} 

}

\caption{Spalten auswählen}\label{fig:fig-select}
\end{figure}

Merke:

\begin{quote}
Die Funktion select wählt Spalten aus einem Dataframe aus.
\end{quote}

Laden wir als ersten einen Datensatz.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"data/stats_test.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Dieser Datensatz beinhaltet Daten zu einer Statistikklausur.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{select}\NormalTok{(stats_test, score)  }\CommentTok{# Spalte `score` auswählen}
\KeywordTok{select}\NormalTok{(stats_test, score, study_time)  }
\CommentTok{# Spalten `score` und `study_time` auswählen}

\KeywordTok{select}\NormalTok{(stats_test, score}\OperatorTok{:}\NormalTok{study_time) }\CommentTok{# dito}
\KeywordTok{select}\NormalTok{(stats_test, }\DecValTok{5}\OperatorTok{:}\DecValTok{6}\NormalTok{)  }\CommentTok{# Spalten 5 bis 6 auswählen}
\end{Highlighting}
\end{Shaded}

Tatsächlich ist der Befehl \texttt{select} sehr flexibel; es gibt viele
Möglichkeiten, Spalten auszuwählen. Im
\texttt{dplyr}-Cheatsheet\footnote{\url{https://www.rstudio.com/resources/cheatsheets/}}
findet sich ein guter Überblick dazu.

\subsubsection[Aufgaben]{\texorpdfstring{Aufgaben\footnote{F, F, R, R, F}}{Aufgaben}}\label{aufgaben-3}

\BeginKnitrBlock{rmdexercises}
Richtig oder Falsch!?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{select} wählt \emph{Zeilen} aus.
\item
  \texttt{select} ist eine Funktion aus dem Paket \texttt{knitr}.
\item
  Möchte man zwei Spalten auswählen, so ist folgende Syntax prinzipiell
  korrekt: \texttt{select(df,\ spalte1,\ spalte2)}.
\item
  Möchte man Spalten 1 bis 10 auswählen, so ist folgende Syntax
  prinzipiell korrekt: `select(df, spalte1:spalte10)
\item
  Mit \texttt{select} können Spalten nur bei ihrem Namen, aber nicht bei
  ihrer Nummer aufgerufen werden.
\end{enumerate}
\EndKnitrBlock{rmdexercises}

\subsection{\texorpdfstring{Zeilen sortieren mit
\texttt{arrange}}{Zeilen sortieren mit arrange}}\label{zeilen-sortieren-mit-arrange}

Man kann zwei Arten des Umgangs mit R unterscheiden: Zum einen der
``interaktive Gebrauch'' und zum anderen ``richtiges Programmieren''. Im
interaktiven Gebrauch geht es uns darum, die Fragen zum aktuell
vorliegenden Datensatz (schnell) zu beantworten. Es geht nicht darum,
eine allgemeine Lösung zu entwickeln, die wir in die Welt verschicken
können und die dort ein bestimmtes Problem löst, ohne dass der
Entwickler (wir) dabei Hilfestellung geben muss. ``Richtige'' Software,
wie ein R-Paket oder Microsoft PowerPoint, muss diese Erwartung
erfüllen; ``richtiges Programmieren'' ist dazu vonnöten. Natürlich sind
in diesem Fall die Ansprüche an die Syntax (der ``Code'', hört sich
cooler an) viel höher. In dem Fall muss man alle Eventualitäten
voraussehen und sicherstellen, dass das Programm auch beim
merkwürdigsten Nutzer brav seinen Dienst tut. Wir haben hier, beim
interaktiven Gebrauch, niedrigere Ansprüche bzw. andere Ziele.

Beim interaktiven Gebrauch von R (oder beliebigen Analyseprogrammen) ist
das Sortieren von Zeilen eine recht häufige Tätigkeit. Typisches
Beispiel wäre der Lehrer, der eine Tabelle mit Noten hat und wissen
will, welche Schüler die schlechtesten oder die besten sind in einem
bestimmten Fach. Oder bei der Prüfung der Umsätze nach Filialen möchten
wir die umsatzstärksten sowie -schwächsten Niederlassungen kennen.

Ein R-Befehl hierzu ist \texttt{arrange}\index{dplyr::arrange}; einige
Beispiele zeigen die Funktionsweise am besten:

\begin{Shaded}
\begin{Highlighting}[]

\KeywordTok{arrange}\NormalTok{(stats_test, score) }\CommentTok{# liefert die *schlechtesten* Noten zuerst zurück}
\KeywordTok{arrange}\NormalTok{(stats_test, }\OperatorTok{-}\NormalTok{score) }\CommentTok{# liefert die *besten* Noten zuerst zurück}
\KeywordTok{arrange}\NormalTok{(stats_test, interest, score)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>   row_number           date_time bestanden study_time self_eval interest
#> 1        234 23.01.2017 18:13:15      nein          3         1        1
#> 2          4 06.01.2017 09:58:05      nein          2         3        2
#>   score
#> 1    17
#> 2    18
#>   row_number           date_time bestanden study_time self_eval interest
#> 1          3 05.01.2017 23:33:47        ja          5        10        6
#> 2          7 06.01.2017 14:25:49        ja         NA        NA       NA
#>   score
#> 1    40
#> 2    40
#>   row_number           date_time bestanden study_time self_eval interest
#> 1        234 23.01.2017 18:13:15      nein          3         1        1
#> 2        142 19.01.2017 19:02:12      nein          3         4        1
#>   score
#> 1    17
#> 2    18
\end{verbatim}

Einige Anmerkungen. Die generelle Syntax lautet
\texttt{arrange(df,\ Spalte1,\ ...)}, wobei \texttt{df} den Dataframe
bezeichnet und \texttt{Spalte1} die erste zu sortierende Spalte; die
Punkte \texttt{...} geben an, dass man weitere Parameter übergeben kann.
Man kann sowohl numerische Spalten als auch Textspalten sortieren. Am
wichtigsten ist hier, dass man weitere Spalten übergeben kann. Dazu
gleich mehr.

Standardmäßig sortiert \texttt{arrange} \emph{aufsteigend} (weil kleine
Zahlen im Zahlenstrahl vor den großen Zahlen kommen). Möchte man diese
Reihenfolge umdrehen (große Werte zuerst, d.h. \emph{absteigend}), so
kann man ein Minuszeichen vor den Namen der Spalte setzen.

Gibt man \emph{zwei oder mehr} Spalten an, so werden pro Wert von
\texttt{Spalte1} die Werte von \texttt{Spalte2} sortiert etc; man
betrachte den Output des Beispiels oben dazu. Abbildung
\ref{fig:fig-arrange}) erläutert die Arbeitsweise von \texttt{arrange}.

Merke:

\begin{quote}
Die Funktion \texttt{arrange} sortiert die Zeilen eines Dataframes.
\end{quote}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/Datenjudo/arrange-crop} 

}

\caption{Spalten sortieren}\label{fig:fig-arrange}
\end{figure}

Ein ähnliches Ergebnis erhält mit man \texttt{top\_n()}, welches die
\texttt{n} \emph{größten Ränge} wiedergibt:

\begin{Shaded}
\begin{Highlighting}[]


\KeywordTok{top_n}\NormalTok{(stats_test, }\DecValTok{3}\NormalTok{, interest)}
\CommentTok{#>   row_number           date_time bestanden study_time self_eval interest}
\CommentTok{#> 1          3 05.01.2017 23:33:47        ja          5        10        6}
\CommentTok{#> 2          5 06.01.2017 14:13:08        ja          4         8        6}
\CommentTok{#> 3         43 13.01.2017 14:14:16        ja          4         8        6}
\CommentTok{#> 4         65 15.01.2017 12:41:27      nein          3         6        6}
\CommentTok{#> 5        110 18.01.2017 18:53:02        ja          5         8        6}
\CommentTok{#> 6        136 19.01.2017 18:22:57        ja          3         1        6}
\CommentTok{#> 7        172 20.01.2017 20:42:46        ja          5        10        6}
\CommentTok{#> 8        214 22.01.2017 21:57:36        ja          2         6        6}
\CommentTok{#> 9        301 27.01.2017 08:17:59        ja          4         8        6}
\CommentTok{#>   score}
\CommentTok{#> 1    40}
\CommentTok{#> 2    34}
\CommentTok{#> 3    36}
\CommentTok{#> 4    22}
\CommentTok{#> 5    37}
\CommentTok{#> 6    39}
\CommentTok{#> 7    34}
\CommentTok{#> 8    31}
\CommentTok{#> 9    33}
\end{Highlighting}
\end{Shaded}

Gibt man \emph{keine} Spalte an (also nur \texttt{top\_n(stats\_test)}),
so bezieht sich \texttt{top\_n} auf die letzte Spalte im Datensatz.

Wenn sich aber, wie hier, mehrere Objekte, den größten Rang (Wert 6)
teilen, bekommen wir \emph{nicht} 3 Zeilen zurückgeliefert, sondern
entsprechend mehr. dplyr ``denkt'' sich: ``Ok, er will die drei besten
Ränge; aber 9 Studenten teilen sich den ersten Rang (Interesse 6), wen
sollte ich da ausschließen? Am besten ich liefere alle 9 zurück, sonst
wäre es ja ungerecht, weil alle 9 sind ja gleich vom Interesse her''.

\subsubsection[Aufgaben]{\texorpdfstring{Aufgaben\footnote{F, F, F, F, F}}{Aufgaben}}\label{aufgaben-4}

\BeginKnitrBlock{rmdexercises}
Richtig oder Falsch!?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{arrange} arrangiert Spalten.
\item
  \texttt{arrange} sortiert im Standard absteigend.
\item
  \texttt{arrange} lässt nur ein Sortierkriterium zu.
\item
  \texttt{arrange} kann numerische Werte, aber nicht Zeichenketten
  sortieren.
\item
  \texttt{top\_n(5)} liefert immer fünf Werte zurück.
\end{enumerate}
\EndKnitrBlock{rmdexercises}

\subsection{\texorpdfstring{Datensatz gruppieren mit
\texttt{group\_by}}{Datensatz gruppieren mit group\_by}}\label{datensatz-gruppieren-mit-group_by}

Einen Datensatz zu gruppieren ist eine häufige Angelegenheit: Was ist
der mittlere Umsatz in Region X im Vergleich zu Region Y? Ist die
Reaktionszeit in der Experimentalgruppe kleiner als in der
Kontrollgruppe? Können Männer schneller ausparken als Frauen? Man sieht,
dass das Gruppieren v.a. in Verbindung mit Mittelwerten oder anderen
Zusammenfassungen sinnvoll ist; dazu im nächsten Abschnitt mehr.

\begin{quote}
Gruppieren meint, einen Datensatz anhand einer diskreten Variablen (z.B.
Geschlecht) so aufzuteilen, dass Teil-Datensätze entstehen - pro Gruppe
ein Teil-Datensatz (z.B. ein Datensatz, in dem nur Männer enthalten sind
und einer, in dem nur Frauen enthalten sind).
\end{quote}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/Datenjudo/group_by} 

}

\caption{Datensätze nach Subgruppen aufteilen}\label{fig:fig-groupby}
\end{figure}

In Abbildung \ref{fig:fig-groupby} wurde der Datensatz anhand der Spalte
(d.h. Variable) \texttt{Fach} in mehrere Gruppen geteilt (Fach A, Fach
B\ldots{}). Wir könnten uns als nächstes z.B. Mittelwerte pro Fach -
d.h. pro Gruppe (pro Ausprägung von \texttt{Fach}) - ausgeben lassen; in
diesem Fall vier Gruppen (Fach A bis D).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test_gruppiert <-}\StringTok{ }\KeywordTok{group_by}\NormalTok{(stats_test, interest)}
\NormalTok{test_gruppiert}
\CommentTok{#> # A tibble: 306 x 7}
\CommentTok{#> # Groups:   interest [7]}
\CommentTok{#>    row_number           date_time bestanden study_time self_eval interest}
\CommentTok{#>         <int>              <fctr>    <fctr>      <int>     <int>    <int>}
\CommentTok{#>  1          1 05.01.2017 13:57:01        ja          5         8        5}
\CommentTok{#>  2          2 05.01.2017 21:07:56        ja          3         7        3}
\CommentTok{#>  3          3 05.01.2017 23:33:47        ja          5        10        6}
\CommentTok{#>  4          4 06.01.2017 09:58:05      nein          2         3        2}
\CommentTok{#>  5          5 06.01.2017 14:13:08        ja          4         8        6}
\CommentTok{#>  6          6 06.01.2017 14:21:18        ja         NA        NA       NA}
\CommentTok{#>  7          7 06.01.2017 14:25:49        ja         NA        NA       NA}
\CommentTok{#>  8          8 06.01.2017 17:24:53      nein          2         5        3}
\CommentTok{#>  9          9 07.01.2017 10:11:17        ja          2         3        5}
\CommentTok{#> 10         10 07.01.2017 18:10:05        ja          4         5        5}
\CommentTok{#> # ... with 296 more rows, and 1 more variables: score <int>}
\end{Highlighting}
\end{Shaded}

Schaut man sich nun den Datensatz an, sieht man erstmal wenig Effekt der
Gruppierung. R teilt uns lediglich mit
\texttt{Groups:\ interest\ {[}7{]}}, dass es 7 Gruppen gibt, aber es
gibt keine extra Spalte oder sonstige Anzeichen der Gruppierung. Aber
keine Sorge, wenn wir gleich einen Mittelwert ausrechnen, bekommen wir
den Mittelwert pro Gruppe!

Ein paar Hinweise: \texttt{Source:\ local\ data\ frame\ {[}306\ x\ 6{]}}
will sagen, dass die Ausgabe sich auf einen \texttt{tibble}
bezieht\footnote{\url{http://stackoverflow.com/questions/29084380/what-is-the-meaning-of-the-local-data-frame-message-from-dplyrprint-tbl-df}},
also eine bestimmte Art von Dataframe.
\texttt{Groups:\ interest\ {[}7{]}} zeigt, dass der Tibble in 7 Gruppen
- entsprechend der Werte von \texttt{interest} aufgeteilt ist.

\texttt{group\_by} an sich ist nicht wirklich nützlich. Nützlich wird es
erst, wenn man weitere Funktionen auf den gruppierten Datensatz anwendet
- z.B. Mittelwerte ausrechnet (z.B mit \texttt{summarise}, s. unten).
Die nachfolgenden Funktionen (wenn sie aus \texttt{dplyr} kommen),
berücksichtigen nämlich die Gruppierung. So kann man einfach Mittelwerte
pro Gruppe ausrechnen. \texttt{dplyr} kombiniert dann die
Zusammenfassungen (z.B. Mittelwerte) der einzelnen Gruppen in einen
Dataframe und gibt diesen dann aus.

Die Idee des ``Gruppieren - Zusammenfassen - Kombinieren'' ist flexibel;
man kann sie häufig brauchen. Es lohnt sich, diese Idee zu lernen (vgl.
Abb. \ref{fig:sac}).

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/Datenjudo/sac_crop} 

}

\caption{Schematische Darstellung des 'Gruppieren - Zusammenfassen - Kombinieren'}\label{fig:sac}
\end{figure}

\subsubsection[Aufgaben]{\texorpdfstring{Aufgaben\footnote{R, F, R, R}}{Aufgaben}}\label{aufgaben-5}

\BeginKnitrBlock{rmdexercises}
Richtig oder Falsch!?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Mit \texttt{group\_by} gruppiert man einen Datensatz.
\item
  \texttt{group\_by} lässt nur ein Gruppierungskriterium zu.
\item
  Die Gruppierung durch \texttt{group\_by} wird nur von Funktionen aus
  \texttt{dplyr} erkannt.
\item
  \texttt{group\_by} ist sinnvoll mit \texttt{summarise} zu kombinieren.
\end{enumerate}
\EndKnitrBlock{rmdexercises}

Merke:

\begin{quote}
Mit group\_by teilt man einen Datensatz in Gruppen ein, entsprechend der
Werte einer mehrerer Spalten.
\end{quote}

\subsection{\texorpdfstring{Eine Spalte zusammenfassen mit
\texttt{summarise}}{Eine Spalte zusammenfassen mit summarise}}\label{eine-spalte-zusammenfassen-mit-summarise}

Vielleicht die wichtigste oder häufigste Tätigkeit in der Analyse von
Daten ist es, eine Spalte zu \emph{einem} Wert zusammenzufassen;
\texttt{summarise}\index{dplyr::summarise} leistet dies. Anders gesagt:
Einen Mittelwert berechnen, den größten (kleinsten) Wert heraussuchen,
die Korrelation berechnen oder eine beliebige andere Statistik ausgeben
lassen. Die Gemeinsamkeit dieser Operationen ist, dass sie eine Spalte
zu einem Wert zusammenfassen, ``aus Spalte mach Zahl'', sozusagen. Daher
ist der Name des Befehls \texttt{summarise} ganz passend. Genauer gesagt
fasst dieser Befehl eine Spalte zu einer Zahl zusammen \emph{anhand}
einer Funktion wie \texttt{mean} oder \texttt{max} (vgl. Abb.
\ref{fig:fig-summarise}. Hierbei ist jede Funktion erlaubt, die eine
Spalte als Input verlangt und eine Zahl zurückgibt; andere Funktionen
sind bei \texttt{summarise} nicht erlaubt.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/Datenjudo/summarise} 

}

\caption{Spalten zu einer Zahl zusammenfassen}\label{fig:fig-summarise}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summarise}\NormalTok{(stats_test, }\KeywordTok{mean}\NormalTok{(score))}
\CommentTok{#>   mean(score)}
\CommentTok{#> 1        31.1}
\end{Highlighting}
\end{Shaded}

Man könnte diesen Befehl so ins Deutsche übersetzen:
\texttt{Fasse\ aus\ Tabelle\ stats\_test\ die\ Spalte\ score\ anhand\ des\ Mittelwerts\ zusammen}.
Nicht vergessen, wenn die Spalte \texttt{score} fehlende Werte hat, wird
der Befehl \texttt{mean} standardmäßig dies mit \texttt{NA} quittieren.
Ergänzt man den Parameter \texttt{nr.rm\ =\ TRUE}, so ignoriert R
fehlende Werte und der Befehl \texttt{mean} liefert ein Ergebnis zurück.

Jetzt können wir auch die Gruppierung nutzen:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test_gruppiert <-}\StringTok{ }\KeywordTok{group_by}\NormalTok{(stats_test, interest)}
\KeywordTok{summarise}\NormalTok{(test_gruppiert, }\KeywordTok{mean}\NormalTok{(score, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{))}
\CommentTok{#> # A tibble: 7 x 2}
\CommentTok{#>   interest `mean(score, na.rm = TRUE)`}
\CommentTok{#>      <int>                       <dbl>}
\CommentTok{#> 1        1                        28.3}
\CommentTok{#> 2        2                        29.7}
\CommentTok{#> 3        3                        30.8}
\CommentTok{#> 4        4                        29.9}
\CommentTok{#> 5        5                        32.5}
\CommentTok{#> 6        6                        34.0}
\CommentTok{#> 7       NA                        33.1}
\end{Highlighting}
\end{Shaded}

Der Befehl \texttt{summarise} erkennt also, wenn eine (mit
\texttt{group\_by}) gruppierte Tabelle vorliegt. Jegliche
Zusammenfassung, die wir anfordern, wird anhand der
Gruppierungsinformation aufgeteilt werden. In dem Beispiel bekommen wir
einen Mittelwert für jeden Wert von \texttt{interest}.
Interessanterweise sehen wir, dass der Mittelwert tendenziell größer
wird, je größer \texttt{interest} wird.

Alle diese \texttt{dplyr}-Befehle geben einen Dataframe zurück, was
praktisch ist für weitere Verarbeitung. In diesem Fall heißen die
Spalten \texttt{interst} und \texttt{mean(score)}. Zweiter Name ist
nicht so schön, daher ändern wir den wie folgt:

Jetzt können wir auch die Gruppierung nutzen:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test_gruppiert <-}\StringTok{ }\KeywordTok{group_by}\NormalTok{(stats_test, interest)}
\KeywordTok{summarise}\NormalTok{(test_gruppiert, }\DataTypeTok{mw_pro_gruppe =} \KeywordTok{mean}\NormalTok{(score, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{))}
\CommentTok{#> # A tibble: 7 x 2}
\CommentTok{#>   interest mw_pro_gruppe}
\CommentTok{#>      <int>         <dbl>}
\CommentTok{#> 1        1          28.3}
\CommentTok{#> 2        2          29.7}
\CommentTok{#> 3        3          30.8}
\CommentTok{#> 4        4          29.9}
\CommentTok{#> 5        5          32.5}
\CommentTok{#> 6        6          34.0}
\CommentTok{#> 7       NA          33.1}
\end{Highlighting}
\end{Shaded}

Nun heißt die zweite Spalte \texttt{mw\_pro\_Gruppe}.
\texttt{na.rm\ =\ TRUE} veranlasst, bei fehlenden Werten trotzdem einen
Mittelwert zurückzuliefern (die Zeilen mit fehlenden Werten werden in
dem Fall ignoriert).

Grundsätzlich ist die Philosophie der \texttt{dplyr}-Befehle: ``Mach nur
eine Sache, aber die dafür gut''. Entsprechend kann \texttt{summarise}
nur \emph{Spalten} zusammenfassen, aber keine \emph{Zeilen}.

Merke:

\begin{quote}
Mit summarise kann man eine Spalte eines Dataframes zu einem Wert
zusammenfassen.
\end{quote}

\subsubsection[Aufgaben]{\texorpdfstring{Aufgaben\footnote{R, R, R, R, R}}{Aufgaben}}\label{aufgaben-6}

\BeginKnitrBlock{rmdexercises}
Richtig oder Falsch!?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Möchte man aus der Tabelle \texttt{stats\_test} den Mittelwert für die
  Spalte \texttt{score} berechnen, so ist folgende Syntax korrekt:
  \texttt{summarise(stats\_test,\ mean(score))}.
\item
  \texttt{summarise} liefert eine Tabelle, genauer: einen Tibble,
  zurück.
\item
  Die Tabelle, die diese Funktion zurückliefert:
  \texttt{summarise(stats\_test,\ mean(score))}, hat eine Spalte mit dem
  Namen \texttt{mean(score)}.
\item
  \texttt{summarise} lässt zu, dass die zu berechnende Spalte einen
  Namen vom Nutzer zugewiesen bekommt.
\item
  \texttt{summarise} darf nur verwendet werden, wenn eine Spalte zu
  einem Wert zusammengefasst werden soll.
\end{enumerate}
\EndKnitrBlock{rmdexercises}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  (Fortgeschritten) Bauen Sie einen eigenen Weg, um den mittleren
  Absolutabstand auszurechnen! Gehen Sie der Einfachheit halber (zuerst)
  von einem Vektor mit den Werten (1,2,3) aus!
\end{enumerate}

Lösung:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{x_mw <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(x)}
\NormalTok{x_delta <-}\StringTok{ }\NormalTok{x }\OperatorTok{-}\StringTok{ }\NormalTok{x_mw}
\NormalTok{x_delta <-}\StringTok{ }\KeywordTok{abs}\NormalTok{(x_delta)}
\NormalTok{mad <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(x_delta)}
\NormalTok{mad}
\CommentTok{#> [1] 0.667}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{Zeilen zählen mit \texttt{n} und
\texttt{count}}{Zeilen zählen mit n und count}}\label{zeilen-zahlen-mit-n-und-count}

Ebenfalls nützlich ist es, Zeilen zu zählen, also Häufigkeiten zu
bestimmen. Im Gegensatz zum Standardbefehl\footnote{Standardbefehl
  meint, dass die Funktion zum Standardrepertoire von R gehört, also
  nicht über ein Paket extra geladen werden muss} \texttt{nrow} versteht
der \texttt{dyplr}-Befehl \texttt{n}\index{dplyr::n} auch Gruppierungen.
\texttt{n} darf im Pfeifen-Workflow nur im Rahmen von \texttt{summarise}
oder ähnlichen \texttt{dplyr}-Befehlen verwendet werden.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summarise}\NormalTok{(stats_test, }\KeywordTok{n}\NormalTok{())}
\CommentTok{#>   n()}
\CommentTok{#> 1 306}
\KeywordTok{summarise}\NormalTok{(test_gruppiert, }\KeywordTok{n}\NormalTok{())}
\CommentTok{#> # A tibble: 7 x 2}
\CommentTok{#>   interest `n()`}
\CommentTok{#>      <int> <int>}
\CommentTok{#> 1        1    30}
\CommentTok{#> 2        2    47}
\CommentTok{#> 3        3    66}
\CommentTok{#> 4        4    41}
\CommentTok{#> 5        5    45}
\CommentTok{#> 6        6     9}
\CommentTok{#> 7       NA    68}
\KeywordTok{nrow}\NormalTok{(stats_test)}
\CommentTok{#> [1] 306}
\end{Highlighting}
\end{Shaded}

Außerhalb von gruppierten Datensätzen ist \texttt{nrow} meist
praktischer.

Praktischer ist der Befehl \texttt{count}\index{dplyr::count}, der
nichts anderes ist als die Hintereinanderschaltung von
\texttt{group\_by} und \texttt{n}. Mit \texttt{count} zählen wir die
Häufigkeiten nach Gruppen; Gruppen sind hier zumeist die Werte einer
auszuzählenden Variablen (oder mehrerer auszuzählender Variablen). Das
macht \texttt{count} zu einem wichtigen Helfer bei der Analyse von
Häufigkeitsdaten.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{count}\NormalTok{(stats_test, interest)}
\CommentTok{#> # A tibble: 7 x 2}
\CommentTok{#>   interest     n}
\CommentTok{#>      <int> <int>}
\CommentTok{#> 1        1    30}
\CommentTok{#> 2        2    47}
\CommentTok{#> 3        3    66}
\CommentTok{#> 4        4    41}
\CommentTok{#> 5        5    45}
\CommentTok{#> 6        6     9}
\CommentTok{#> 7       NA    68}
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{count}\NormalTok{(stats_test, study_time)}
\CommentTok{#> # A tibble: 6 x 2}
\CommentTok{#>   study_time     n}
\CommentTok{#>        <int> <int>}
\CommentTok{#> 1          1    31}
\CommentTok{#> 2          2    49}
\CommentTok{#> 3          3    85}
\CommentTok{#> 4          4    56}
\CommentTok{#> 5          5    17}
\CommentTok{#> 6         NA    68}
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{count}\NormalTok{(stats_test, interest, study_time)}
\CommentTok{#> # A tibble: 29 x 3}
\CommentTok{#>    interest study_time     n}
\CommentTok{#>       <int>      <int> <int>}
\CommentTok{#>  1        1          1    12}
\CommentTok{#>  2        1          2     7}
\CommentTok{#>  3        1          3     8}
\CommentTok{#>  4        1          4     2}
\CommentTok{#>  5        1          5     1}
\CommentTok{#>  6        2          1     9}
\CommentTok{#>  7        2          2    15}
\CommentTok{#>  8        2          3    16}
\CommentTok{#>  9        2          4     6}
\CommentTok{#> 10        2          5     1}
\CommentTok{#> # ... with 19 more rows}
\end{Highlighting}
\end{Shaded}

Allgemeiner formuliert lautet die Syntax:
\texttt{count(df,\ Spalte1,\ ...)}, wobei \texttt{df} der Dataframe ist
und \texttt{Spalte1} die erste (es können mehrere sein) auszuzählende
Spalte. Gibt man z.B. zwei Spalten an, so wird pro Wert der 1. Spalte
die Häufigkeiten der 2. Spalte ausgegeben (vgl. Abb.
\ref{fig:fig-count}).

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/Datenjudo/count-crop} 

}

\caption{Sinnbild für 'count'}\label{fig:fig-count}
\end{figure}

Merke:

\begin{quote}
n und count zählen die Anzahl der Zeilen, d.h. die Anzahl der Fälle.
\end{quote}

\subsubsection{Vertiefung zum Zählen von Zeilen: Relative
Häufigkeiten}\label{vertiefung-zum-zahlen-von-zeilen-relative-haufigkeiten}

Manchmal ist es praktisch, nicht zur die (absolute) Häufigkeiten von
Zeilen zu zählen, sondern ihren Anteil nach (relative Häufigkeit).
Klassisches Beispiel: Wie viel Prozent der Fälle sind Frauen, wie viele
sind Männer?

In \texttt{dplyr} kann man das so umsetzen:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{(interest) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{prop_interest =}\NormalTok{ n }\OperatorTok{/}\StringTok{ }\KeywordTok{sum}\NormalTok{(n))}
\CommentTok{#> # A tibble: 7 x 3}
\CommentTok{#>   interest     n prop_interest}
\CommentTok{#>      <int> <int>         <dbl>}
\CommentTok{#> 1        1    30        0.0980}
\CommentTok{#> 2        2    47        0.1536}
\CommentTok{#> 3        3    66        0.2157}
\CommentTok{#> 4        4    41        0.1340}
\CommentTok{#> 5        5    45        0.1471}
\CommentTok{#> 6        6     9        0.0294}
\CommentTok{#> 7       NA    68        0.2222}
\end{Highlighting}
\end{Shaded}

\texttt{prop} steht hier für ``Proportion'', also Anteil.
\texttt{sum(n)} liefert die Summe der Fälle zurück, also 306 in diesem
Fall.

Etwas komplexer ist es, wenn man zwei Gruppierungsvariablen hat und dann
Anteile auszählen möchte:

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{stats_test}\OperatorTok{$}\NormalTok{bestanden <-}\StringTok{ }\NormalTok{stats_test}\OperatorTok{$}\NormalTok{score }\OperatorTok{>}\StringTok{ }\DecValTok{25}

\NormalTok{stats_test }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(interest, bestanden) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{n =} \KeywordTok{n}\NormalTok{()) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{prop_interest =}\NormalTok{ n }\OperatorTok{/}\StringTok{ }\KeywordTok{sum}\NormalTok{(n)) }
\CommentTok{#> # A tibble: 14 x 4}
\CommentTok{#> # Groups:   interest [7]}
\CommentTok{#>    interest bestanden     n prop_interest}
\CommentTok{#>       <int>     <lgl> <int>         <dbl>}
\CommentTok{#>  1        1     FALSE    10         0.333}
\CommentTok{#>  2        1      TRUE    20         0.667}
\CommentTok{#>  3        2     FALSE     9         0.191}
\CommentTok{#>  4        2      TRUE    38         0.809}
\CommentTok{#>  5        3     FALSE    14         0.212}
\CommentTok{#>  6        3      TRUE    52         0.788}
\CommentTok{#>  7        4     FALSE     9         0.220}
\CommentTok{#>  8        4      TRUE    32         0.780}
\CommentTok{#>  9        5     FALSE     6         0.133}
\CommentTok{#> 10        5      TRUE    39         0.867}
\CommentTok{#> 11        6     FALSE     1         0.111}
\CommentTok{#> 12        6      TRUE     8         0.889}
\CommentTok{#> 13       NA     FALSE     7         0.103}
\CommentTok{#> 14       NA      TRUE    61         0.897}
\end{Highlighting}
\end{Shaded}

Synonym zur letzten Syntax könnte man auch schreiben:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{(interest, bestanden) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{prop_interest =}\NormalTok{ n }\OperatorTok{/}\StringTok{ }\KeywordTok{sum}\NormalTok{(n)) }
\end{Highlighting}
\end{Shaded}

\subsubsection[Aufgaben]{\texorpdfstring{Aufgaben\footnote{R, R, F, F}}{Aufgaben}}\label{aufgaben-7}

\BeginKnitrBlock{rmdexercises}
Richtig oder Falsch!?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Mit \texttt{count} kann man Zeilen zählen.
\item
  \texttt{count} ist ähnlich (oder identisch) zu einer Kombination von
  \texttt{group\_by} und \texttt{n()}.
\item
  Mit \texttt{count} kann man nur nur eine Gruppe beim Zählen
  berücksichtigen.
\item
  \texttt{count} darf nicht bei nominalskalierten Variablen verwendet
  werden.
\end{enumerate}
\EndKnitrBlock{rmdexercises}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Bauen Sie sich einen Weg, um den Modus mithilfe von \texttt{count} und
  \texttt{arrange} zu bekommen!
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_count <-}\StringTok{ }\KeywordTok{count}\NormalTok{(stats_test, score)}
\NormalTok{stats_count_sortiert <-}\StringTok{ }\KeywordTok{arrange}\NormalTok{(stats_count, }\OperatorTok{-}\NormalTok{n)}
\KeywordTok{head}\NormalTok{(stats_count_sortiert,}\DecValTok{1}\NormalTok{)}
\CommentTok{#> # A tibble: 1 x 2}
\CommentTok{#>   score     n}
\CommentTok{#>   <int> <int>}
\CommentTok{#> 1    34    22}
\end{Highlighting}
\end{Shaded}

Ah! Der Score \texttt{34} ist der häufigste!

\section{Die Pfeife}\label{die-pfeife}

Die zweite Idee zentrale Idee von \texttt{dplyr} kann man salopp als
``Durchpfeifen''\index{Pfeife} oder die ``Idee der Pfeife''
(Durchpfeifen)\index{Durchpfeifen} bezeichnen; ikonographisch mit einem
Pfeifen ähnlichen Symbol dargestellt \texttt{\%\textgreater{}\%}. Der
Begriff ``Durchpfeifen'' ist frei vom Englischen ``to pipe'' übernommen.
Das berühmte Bild von René Magritte stand dabei Pate (s. Abb.
\ref{fig:cecie-une-pipe}; (M7
\protect\hyperlink{ref-m7_savinellis_2004}{2004})).

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/Datenjudo/800px-Pipa_savinelli} 

}

\caption{Das ist keine Pfeife}\label{fig:cecie-une-pipe}
\end{figure}

Hierbei ist gemeint, einen Datensatz sozusagen auf ein Fließband zu
legen und an jedem Arbeitsplatz einen Arbeitsschritt auszuführen. Der
springende Punkt ist, dass ein Dataframe als ``Rohstoff'' eingegeben
wird und jeder Arbeitsschritt seinerseits wieder einen Dataframe
ausgibt. Damit kann man sehr schön, einen ``Flow'' an Verarbeitung
erreichen, außerdem spart man sich Tipparbeit und die Syntax wird
lesbarer. Damit das Durchpfeifen funktioniert, benötigt man Befehle, die
als Eingabe einen Dataframe erwarten und wieder einen Dataframe
zurückliefern. Das Schaubild verdeutlicht beispielhaft eine Abfolge des
Durchpfeifens (s. Abb. \ref{fig:fig-durchpfeifen}).

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/Datenjudo/durchpfeifen} 

}

\caption{Das 'Durchpeifen'}\label{fig:fig-durchpfeifen}
\end{figure}

Die sog. ``Pfeife'' (pipe\index{Pfeife}: \texttt{\%\textgreater{}\%}) in
Anspielung an das berühmte Bild von René Magritte, verkettet Befehle
hintereinander. Das ist praktisch, da es die Syntax vereinfacht.

\BeginKnitrBlock{rmdcaution}
Tipp: In RStudio gibt es einen Shortcut für die Pfeife: Strg-Shift-M
(auf allen Betriebssystemen).
\EndKnitrBlock{rmdcaution}

Vergleichen Sie mal diese Syntax

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{filter}\NormalTok{(}\KeywordTok{summarise}\NormalTok{(}\KeywordTok{group_by}\NormalTok{(}\KeywordTok{filter}\NormalTok{(stats_test, }
       \OperatorTok{!}\KeywordTok{is.na}\NormalTok{(score)), interest), }\DataTypeTok{mw =} \KeywordTok{mean}\NormalTok{(score)), mw }\OperatorTok{>}\StringTok{ }\DecValTok{30}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

mit dieser

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(score)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(interest) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{mw =} \KeywordTok{mean}\NormalTok{(score)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(mw }\OperatorTok{>}\StringTok{ }\DecValTok{30}\NormalTok{)}
\CommentTok{#> # A tibble: 4 x 2}
\CommentTok{#>   interest    mw}
\CommentTok{#>      <int> <dbl>}
\CommentTok{#> 1        3  30.8}
\CommentTok{#> 2        5  32.5}
\CommentTok{#> 3        6  34.0}
\CommentTok{#> 4       NA  33.1}
\end{Highlighting}
\end{Shaded}

Die zweite ist viel einfacher! Lassen Sie uns die ``Pfeifen-Syntax'' in
deutschen Pseudo-Code zu übersetzen.

\BeginKnitrBlock{rmdpseudocode}
Nimm die Tabelle ``stats\_test'' UND DANN\\
filtere alle nicht-fehlenden Werte UND DANN\\
gruppiere die verbleibenden Werte nach ``interest'' UND DANN\\
bilde den Mittelwert (pro Gruppe) für ``score'' UND DANN\\
liefere nur die Werte größer als 30 zurück.
\EndKnitrBlock{rmdpseudocode}

Die zweite Syntax, in ``Pfeifenform'' ist viel einfacher zu verstehen
als die erste! Die erste Syntax ist verschachtelt, man muss sie von
innen nach außen lesen. Das ist kompliziert. Die Pfeife in der 2. Syntax
macht es viel einfacher, die Syntax zu verstehen, da die Befehle
``hintereinander'' gestellt (sequenziell organisiert) sind.

Die Pfeife zerlegt die ``russische Puppe'', also ineinander
verschachtelten Code, in sequenzielle Schritte und zwar in der richtigen
Reihenfolge (entsprechend der Abarbeitung). Wir müssen den Code nicht
mehr von innen nach außen lesen (wie das bei einer mathematischen Formel
der Fall ist), sondern können wie bei einem Kochrezept ``erstens
\ldots{}, zweitens .., drittens \ldots{}'' lesen. Die Pfeife macht die
Syntax einfacher. Natürlich hätten wir die verschachtelte Syntax in
viele einzelne Befehle zerlegen können und jeweils eine Zwischenergebnis
speichern mit dem Zuweisungspfeil \texttt{\textless{}-} und das
Zwischenergebnis dann explizit an den nächsten Befehl weitergeben.
Eigentlich macht die Pfeife genau das - nur mit weniger Tipparbeit. Und
auch einfacher zu lesen. Flow!

\BeginKnitrBlock{rmdcaution}
Wenn Sie Befehle verketten mit der Pfeife, sind nur Befehle erlaubt, die
einen Datensatz als Eingabe verlangen und einen Datensatz ausgeben. Das
ist bei den hier vorgestellten Funktionen der Fall. Viele andere
Funktionen erfüllen dieses Kriterium aber nicht; in dem Fall liefert
\texttt{dplyr} eine Fehlermeldung.
\EndKnitrBlock{rmdcaution}

\subsection{\texorpdfstring{Spalten berechnen mit
\texttt{mutate}}{Spalten berechnen mit mutate}}\label{spalten-berechnen-mit-mutate}

Wenn man die Pfeife benutzt, ist der Befehl
\texttt{mutate}\index{dplyr::mutate} ganz praktisch: Er berechnet eine
Spalte. Normalerweise kann man einfach eine Spalte berechnen mit dem
Zuweisungsoperator:

Zum Beispiel so:

\begin{verbatim}
df$neue_spalte <- df$spalte1 + df$spalte2
\end{verbatim}

Innerhalb einer Pfeifen-Syntax geht das aber nicht (so gut). Da ist man
mit der Funktion \texttt{mutate} besser beraten; \texttt{mutate}
leistest just dasselbe wie die Pseudo-Syntax oben:

\begin{verbatim}
df %>% 
  mutate(neue_spalte = spalte1 + spalte2)
\end{verbatim}

In Worten:

\BeginKnitrBlock{rmdpseudocode}
Nimm die Tabelle ``df'' UND DANN\\
bilde eine neue Spalte mit dem Namen \texttt{neue\_spalte}, die sich
berechnet als Summe von \texttt{spalte1} und \texttt{spalte2}.
\EndKnitrBlock{rmdpseudocode}

Allerdings berücksichtigt \texttt{mutate} auch Gruppierungen, das ist
praktisch. Der Hauptvorteil ist die bessere Lesbarkeit durch Auflösen
der Verschachtelungen.

Ein konkretes Beispiel:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(bestanden, interest, score) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Streber =}\NormalTok{ score }\OperatorTok{>}\StringTok{ }\DecValTok{38}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{head}\NormalTok{()}
\CommentTok{#>   bestanden interest score Streber}
\CommentTok{#> 1      TRUE        5    29   FALSE}
\CommentTok{#> 2      TRUE        3    29   FALSE}
\CommentTok{#> 3      TRUE        6    40    TRUE}
\CommentTok{#> 4     FALSE        2    18   FALSE}
\CommentTok{#> 5      TRUE        6    34   FALSE}
\CommentTok{#> 6      TRUE       NA    39    TRUE}
\end{Highlighting}
\end{Shaded}

Diese Syntax erzeugt eine neue Spalte innerhalb von
\texttt{stats\_test}; diese Spalte prüft pro Person, ob \texttt{score}
\textgreater{} 38 ist. Falls ja (TRUE), dann ist \texttt{Streber} TRUE,
ansonsten ist \texttt{Streber} FALSE (tja). \texttt{head} zeigt die
ersten 6 Zeilen des resultierenden Dataframes an.

Abb. \ref{fig:fig-mutate} zeigt Sinnbild für \texttt{mutate}:

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/Datenjudo/mutate} 

}

\caption{Sinnbild für mutate}\label{fig:fig-mutate}
\end{figure}

\BeginKnitrBlock{rmdcaution}
\texttt{mutate} erwartet als Input \emph{keinen} Dateframe, sondern eine
Spalte. Betrachten Sie das Sinnbild von \texttt{mutate}. Die Idee ist,
eine Spalte umzuwandeln nach dem Motto: ``Nimm eine Spalte, mach was
damit und liefere die neue Spalte zurück''. Die Spalte (und damit jeder
einzelne Wert in der Spalte) wird \emph{verändert} (`mutiert', daher
`mutate'). Man kann auch sagen, die Spalte wird \emph{transformiert}.
\EndKnitrBlock{rmdcaution}

\subsection{Aufgaben}\label{aufgaben-8}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Entschlüsseln Sie dieses Ungetüm! Übersetzen Sie diese Syntax auf
  Deutsch:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{bestanden_gruppen <-}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}
    \KeywordTok{summarise}\NormalTok{(}
      \KeywordTok{group_by}\NormalTok{(}\KeywordTok{filter}\NormalTok{(}\KeywordTok{select}\NormalTok{(stats_test, }\OperatorTok{-}\KeywordTok{c}\NormalTok{(row_number, date_time)) , bestanden }\OperatorTok{==}\StringTok{ "ja"}\NormalTok{), interest), }
      \DataTypeTok{Punkte =} \KeywordTok{mean}\NormalTok{(score), }\DataTypeTok{n =} \KeywordTok{n}\NormalTok{()))}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Entschlüsseln Sie jetzt diese Syntax bzw. übersetzen Sie sie ins
  Deutsche:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{stats_test }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{row_number, }\OperatorTok{-}\NormalTok{date_time) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(bestanden }\OperatorTok{==}\StringTok{ "ja"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(interest) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{Punkte =} \KeywordTok{mean}\NormalTok{(score),}
            \DataTypeTok{n =} \KeywordTok{n}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Die Pfeife bei im Klausur-Datensatz
\end{enumerate}

\begin{itemize}
\tightlist
\item
  (Übersetzen Sie die folgende Pseudo-Syntax ins ERRRische!
\end{itemize}

\BeginKnitrBlock{rmdpseudocode}
Nimm den Datensatz \texttt{stats\_test} UND DANN\ldots{}\\
Wähle daraus die Spalte \texttt{score} UND DANN\ldots{}\\
Berechne den Mittelwert der Spalte UND DANN\ldots{}\\
ziehe vom Mittelwert die Spalte ab UND DANN\ldots{} quadriere die
einzelnen Differenzen UND DANN\ldots{} bilde davon den Mittelwert.
\EndKnitrBlock{rmdpseudocode}

Lösung:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(score) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{score_delta =}\NormalTok{ score }\OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(.}\OperatorTok{$}\NormalTok{score)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{score_delta_squared =}\NormalTok{ score_delta}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{score_var =} \KeywordTok{mean}\NormalTok{(score_delta_squared)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\KeywordTok{sqrt}\NormalTok{(score_var))}
\end{Highlighting}
\end{Shaded}

Was sagt uns der Punkt \texttt{.} in der Syntax oben? Der Punkt steht
für die Tabelle, wie sie gerade aufbereitet ist (also laut letzter Zeile
in der Syntax). Warum müssen wir dem Befehl \texttt{mean} sagen, welche
Spalte/Variable \texttt{score} wir meinen? Ist doch logo, wir meinen
natürlich die Spalte score im aktuellen, durchgepfiffenen Datensatz!
Leider weiß das der Befehl \texttt{mean} nicht. \texttt{mean} hat
keinerlei Idee von Pfeifen, unseren Wünschen und Sorgen. \texttt{mean}
denkt sich: ``Not my job! Sag mir gefälligst \emph{wie immer}, in
welchem Dataframe ich die Spalte finde!''. Also sagen wir \texttt{mean},
wo er die Spalte findet\ldots{}

\begin{itemize}
\item
  Berechnen Sie die sd von \texttt{score} in \texttt{stats\_test}!
  Vergleichen Sie sie mit dem Ergebnis der vorherigen Aufgabe!\footnote{\texttt{sd(stats\_test\$score)}}
\item
  Was hat die Pfeifen-Syntax oben berechnet?\footnote{die sd von
    \texttt{score}}
\end{itemize}

\section{Deskriptive Statistik}\label{deskriptive-statistik}

\texttt{dplyr} kann man gut gebrauchen, um deskriptive Statistik zu
berechnen. \texttt{summarise} charakterisiert eine Hauptidee der
Deskriptivstatistik: Einen Vektor zu einer Zahl zusammenzufassen.
\texttt{group\_by} steht für die Idee, `Zahlensäcke' (Verteilungen) in
Subgruppen aufzuteilen. \texttt{mutate} transformiert Daten. \texttt{n}
zählt Häufigkeiten.

Ein weiterer zentraler Gedanken der Deskriptivstatistik ist es, dass es
beim Zusammenfassen von Daten nicht reicht, sich auf den Mittelwert oder
eine (hoffentlich) `repräsentative' Zahl zu verlassen. Man braucht auch
einen Hinweis, wie unterschiedlich die Daten sind. Entsprechend spricht
man von zwei Hauptbereichen der deskriptiven Statistik.

\begin{quote}
Die deskriptive Statistik hat zwei Hauptbereiche: Lagemaße und
Streuungsmaße.
\end{quote}

\emph{Lagemaße} geben den ``typischen'', ``mittleren'' oder
``repräsentativen'' Vertreter der Verteilung an. Bei den
Lagemaßen\index{Lagemaße} denkt man sofort an das \emph{arithmetische
Mittel} (synonym: Mittelwert, arithmetisches Mittel; häufig als
\(\bar{X}\) abgekürzt; \texttt{mean}). Ein Nachteil von Mittelwerten
ist, dass sie \emph{nicht robust} gegenüber Extremwerte sind: Schon ein
vergleichsweise großer Einzelwert kann den Mittelwert stark verändern
und damit die Repräsentativität des Mittelwerts für die Gesamtmenge der
Daten in Frage stellen. Eine robuste Variante ist der \emph{Median} (Md;
\texttt{median}). Ist die Anzahl der (unterschiedlichen) Ausprägungen
nicht zu groß im Verhältnis zur Fallzahl, so ist der \emph{Modus} eine
sinnvolle Statistik; er gibt die häufigste Ausprägung an\footnote{Der
  \emph{Modus} ist im Standard-R nicht mit einem eigenen Befehl
  vertreten. Man kann ihn aber leicht von Hand bestimmen; s.u. Es gibt
  auch einige Pakete, die diese Funktion anbieten: z.B.
  \url{https://cran.r-project.org/web/packages/modes/index.html}}.

\emph{Streuungsmaße}\index{Streuungsmaße} geben die Unterschiedlichkeit
in den Daten wieder; mit anderen Worten: sind die Daten sich ähnlich
oder unterscheiden sich die Werte deutlich? Zentrale Statistiken sind
der \emph{mittlere Absolutabstand} (MAA; engl. mean absolute deviation,
MAD),\footnote{Der \emph{MAD} ist im Standard-R nicht mit einem eigenen
  Befehl vertreten. Es gibt einige Pakete, die diese Funktion anbieten:
  z.B. \texttt{lsr::aad} (absolute average deviation from the mean)
  \url{https://artax.karlin.mff.cuni.cz/r-help/library/lsr/html/aad.html}}
die \emph{Standardabweichung} (sd; \texttt{sd}), die \emph{Varianz}
(Var; \texttt{var}) und der \emph{Interquartilsabstand} (IQR;
\texttt{IQR}). Da nur der IQR \emph{nicht} auf dem Mittelwert basiert,
ist er robuster als Statistiken, die sich aus dem Mittelwert ergeben.
Beliebige Quantile bekommt man mit dem R-Befehl \texttt{quantile}.
Möchte man z.B. Q1, Median und Q3, so kann man das so sagen:
\texttt{quantile(x,\ probs\ =\ c(.25,\ .50,\ .75)),\ wobei}x` eine
Spalte (ein Vektor) ist.

Der Befehl \texttt{summarise} eignet sich, um deskriptive Statistiken
auszurechnen.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summarise}\NormalTok{(stats_test, }\KeywordTok{mean}\NormalTok{(score))}
\CommentTok{#>   mean(score)}
\CommentTok{#> 1        31.1}
\KeywordTok{summarise}\NormalTok{(stats_test, }\KeywordTok{sd}\NormalTok{(score))}
\CommentTok{#>   sd(score)}
\CommentTok{#> 1      5.74}
\KeywordTok{summarise}\NormalTok{(stats_test, }\KeywordTok{aad}\NormalTok{(score))  }\CommentTok{# aus Paket 'lsr'}
\CommentTok{#>   aad(score)}
\CommentTok{#> 1       4.84}
\end{Highlighting}
\end{Shaded}

Natürlich könnte man auch einfacher schreiben:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(stats_test}\OperatorTok{$}\NormalTok{score)}
\CommentTok{#> [1] 31.1}
\KeywordTok{median}\NormalTok{(stats_test}\OperatorTok{$}\NormalTok{score)}
\CommentTok{#> [1] 31}
\KeywordTok{aad}\NormalTok{(stats_test}\OperatorTok{$}\NormalTok{score)}
\CommentTok{#> [1] 4.84}
\end{Highlighting}
\end{Shaded}

\BeginKnitrBlock{rmdcaution}
Viele R-Befehle der deskriptiven Statistik sind im Standard so
eingestellt, dass sie \texttt{NA} zurückliefern, falls es in den Daten
fehlende Werte gibt. Das ist einerseits informativ, aber oft unnötig.
Mit dem Parameter \texttt{na.rm\ =\ TRUE} kann man dieses Verhalten
abstellen.

Tipp: Mit dem Befehl \texttt{df\ \textless{}-\ na.omit(df)} entfernen
Sie alle fehlenden Werte aus \texttt{df}.
\EndKnitrBlock{rmdcaution}

\texttt{summarise} liefert aber im Unterschied zu \texttt{mean} etc.
immer einen Dataframe zurück. Da der Dataframe die typische
Datenstruktur ist, ist es häufig praktisch, wenn man einen Dataframe
zurückbekommt, mit dem man weiterarbeiten kann. Außerdem lassen
\texttt{mean} etc. keine Gruppierungsoperationen zu; über
\texttt{group\_by} kann man dies aber bei \texttt{dplyr} erreichen.

\section{Befehlsübersicht}\label{befehlsubersicht-2}

Tabelle \ref{tab:befehle-datenjudo} fasst die R-Funktionen dieses
Kapitels zusammen.

\begin{table}

\caption{\label{tab:befehle-datenjudo}Befehle des Kapitels 'Datenjudo'}
\centering
\begin{tabular}[t]{l|l}
\hline
Paket::Funktion & Beschreibung\\
\hline
dplyr::arrange & Sortiert Spalten\\
\hline
dplyr::filter & Filtert Zeilen\\
\hline
dplyr::select & Wählt Spalten\\
\hline
dplyr::group\_by & gruppiert einen Dataframe\\
\hline
dplyr::n & zählt Zeilen\\
\hline
dplyr::count & zählt Zeilen nach Untergruppen\\
\hline
\%>\% (dplyr) & verkettet Befehle\\
\hline
dplyr::mutate & erzeugt/berechnet Spalten\\
\hline
\end{tabular}
\end{table}

\section{Verweise}\label{verweise-2}

\begin{itemize}
\item
  Die offizielle Dokumentation von \texttt{dplyr} findet sich hier:
  \url{https://cran.r-project.org/web/packages/dplyr/dplyr.pdf}.
\item
  Eine schöne Demonstration wie mächtig \texttt{dplyr} ist findet sich
  hier: \url{http://bit.ly/2kX9lvC}.
\item
  Die GUI ``exploratory'' ist ein ``klickbare'' Umsetzung von
  \texttt{dplyr} and friends; mächtig, modern und sieht cool aus:
  \url{https://exploratory.io}.
\item
  \emph{R for Data Science} bietet umfangreiche Unterstützung zu diesem
  Thema (Wickham und Grolemund \protect\hyperlink{ref-r4ds}{2016}).
\end{itemize}

\chapter{Praxisprobleme der
Datenaufbereitung}\label{praxisprobleme-der-datenaufbereitung}

\BeginKnitrBlock{rmdcaution}
Lernziele:

\begin{itemize}
\tightlist
\item
  Typische Probleme der Datenaufbereitung kennen.
\item
  Typische Probleme der Datenaufbereitung bearbeiten können.
\end{itemize}
\EndKnitrBlock{rmdcaution}

Laden wir zuerst die benötigten Pakete; v.a. ist das \texttt{dplyr} and
friends. Das geht mit dem Paket \texttt{tidyverse}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(corrr)}
\KeywordTok{library}\NormalTok{(gridExtra)}
\KeywordTok{library}\NormalTok{(car)}
\end{Highlighting}
\end{Shaded}

Stellen wir einige typische Probleme des Datenjudo (genauer: der
Datenaufbereitung) zusammen. Probleme heißt hier nicht, dass es etwas
Schlimmes passiert ist, sondern es ist gemeint, wir schauen uns ein paar
typische Aufgabenstellungen an, die im Rahmen der Datenaufbereitung
häufig anfallen.

\section{Datenaufbereitung}\label{datenaufbereitung}

\subsection{Auf fehlende Werte prüfen}\label{auf-fehlende-werte-prufen}

Das geht recht einfach mit \texttt{summary(mein\_dataframe)}. Der Befehl
liefert für jede Spalte des Dataframe \texttt{mein\_dataframe} die
Anzahl der fehlenden Werte zurück.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"data/stats_test.csv"}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(stats_test)}
\CommentTok{#>    row_number                  date_time   bestanden    study_time }
\CommentTok{#>  Min.   :  1.0   05.01.2017 13:57:01:  1   ja  :261   Min.   :1.0  }
\CommentTok{#>  1st Qu.: 77.2   05.01.2017 21:07:56:  1   nein: 45   1st Qu.:2.0  }
\CommentTok{#>  Median :153.5   05.01.2017 23:33:47:  1              Median :3.0  }
\CommentTok{#>  Mean   :153.5   06.01.2017 09:58:05:  1              Mean   :2.9  }
\CommentTok{#>  3rd Qu.:229.8   06.01.2017 14:13:08:  1              3rd Qu.:4.0  }
\CommentTok{#>  Max.   :306.0   06.01.2017 14:21:18:  1              Max.   :5.0  }
\CommentTok{#>                  (Other)            :300              NA's   :68   }
\CommentTok{#>    self_eval       interest       score     }
\CommentTok{#>  Min.   : 1.0   Min.   :1.0   Min.   :17.0  }
\CommentTok{#>  1st Qu.: 4.0   1st Qu.:2.0   1st Qu.:27.0  }
\CommentTok{#>  Median : 5.0   Median :3.0   Median :31.0  }
\CommentTok{#>  Mean   : 5.4   Mean   :3.2   Mean   :31.1  }
\CommentTok{#>  3rd Qu.: 7.0   3rd Qu.:4.0   3rd Qu.:36.0  }
\CommentTok{#>  Max.   :10.0   Max.   :6.0   Max.   :40.0  }
\CommentTok{#>  NA's   :68     NA's   :68}
\end{Highlighting}
\end{Shaded}

\subsection{Fälle mit fehlenden Werte
löschen}\label{falle-mit-fehlenden-werte-loschen}

Weist eine Variable (Spalte) ``wenig'' fehlende Werte auf, so kann es
schlau sein, nichts zu tun. Eine andere Möglichkeit besteht darin, alle
entsprechenden Zeilen zu löschen. Man sollte aber schauen, wie viele
Zeilen dadurch verloren gehen.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Unsprünglich Anzahl an Fällen (Zeilen)}
\KeywordTok{nrow}\NormalTok{(stats_test)}
\CommentTok{#> [1] 306}

\CommentTok{# Nach Umwandlung in neuen Dataframe}
\NormalTok{stats_test }\OperatorTok{%>%}
\StringTok{   }\NormalTok{na.omit ->}\StringTok{ }\NormalTok{stats_test_na_omit}
\KeywordTok{nrow}\NormalTok{(stats_test_na_omit)}
\CommentTok{#> [1] 238}

\CommentTok{# Nur die Anzahl der bereinigten Daten}
\NormalTok{stats_test }\OperatorTok{%>%}
\StringTok{   }\NormalTok{na.omit }\OperatorTok{%>%}
\StringTok{   }\NormalTok{nrow}
\CommentTok{#> [1] 238}
\end{Highlighting}
\end{Shaded}

\BeginKnitrBlock{rmdcaution}
Bei mit der Pfeife verketteten Befehlen darf man für Funktionen die
runden Klammern weglassen, wenn man keinen Parameter schreibt. Also ist
\texttt{nrow} (ohne Klammern) erlaubt bei \texttt{dplyr}, wo es
eigentlich \texttt{nrow()} heißen müsste. Sie dürfen die Klammern
natürlich schreiben, aber sie müssen nicht.
\EndKnitrBlock{rmdcaution}

Hier verlieren wir 68 Zeilen, das verschmerzen wir.

Welche Zeilen verlieren wir eigentlich? Lassen wir uns nur die
\emph{nicht-}kompletten Fälle anzeigen (und davon nur die ersten paar):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test }\OperatorTok{%>%}\StringTok{ }
\StringTok{   }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{complete.cases}\NormalTok{(.)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{head }
\CommentTok{#>   row_number           date_time bestanden study_time self_eval interest}
\CommentTok{#> 1          6 06.01.2017 14:21:18        ja         NA        NA       NA}
\CommentTok{#> 2          7 06.01.2017 14:25:49        ja         NA        NA       NA}
\CommentTok{#> 3         15 09.01.2017 15:23:15        ja         NA        NA       NA}
\CommentTok{#> 4         19 10.01.2017 17:16:48      nein         NA        NA       NA}
\CommentTok{#> 5         42 13.01.2017 14:08:08        ja         NA        NA       NA}
\CommentTok{#> 6         49 14.01.2017 07:02:39        ja         NA        NA       NA}
\CommentTok{#>   score}
\CommentTok{#> 1    39}
\CommentTok{#> 2    40}
\CommentTok{#> 3    30}
\CommentTok{#> 4    22}
\CommentTok{#> 5    38}
\CommentTok{#> 6    39}
\end{Highlighting}
\end{Shaded}

\BeginKnitrBlock{rmdcaution}
Man beachte, dass der Punkt \texttt{.} für den Datensatz steht, wie er
vom letzten Schritt weitergegeben wurde. Innerhalb einer
dplyr-Befehls-Kette können wir den Datensatz, wie er im letzten Schritt
beschaffen war, stets mit \texttt{.} ansprechen; ganz praktisch, weil
schnell zu tippen. Natürlich könnten wir diesen Datensatz jetzt als
neues Objekt speichern und damit weiter arbeiten. Das Ausrufezeichen
\texttt{!} steht für logisches ``Nicht''. Mit \texttt{head} bekommt man
nur die ersten paar Fälle (6 im Standard) angezeigt, was oft reicht für
einen Überblick.
\EndKnitrBlock{rmdcaution}

In Pseudo-Syntax liest es sich so:

\BeginKnitrBlock{rmdpseudocode}
Nehme den Datensatz \texttt{stats\_test} UND DANN\ldots{}\\
filtere die nicht-kompletten Fälle
\EndKnitrBlock{rmdpseudocode}

\subsection{Fehlende Werte zählen}\label{fehlende-werte-zahlen}

Wie viele fehlende Wert weist eine Spalte auf?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test}\OperatorTok{$}\NormalTok{self_eval }\OperatorTok{%>%}\StringTok{ }\NormalTok{is.na }\OperatorTok{%>%}\StringTok{ }\NormalTok{sum}
\CommentTok{#> [1] 68}
\end{Highlighting}
\end{Shaded}

\BeginKnitrBlock{rmdpseudocode}
Nimm die Spalte \texttt{self\_eval} aus der Tabelle ``stats\_test'' UND
DANN\\
finde die fehlenden Werte (NAs) in dieser Spalten UND DANN summiere alle
Treffer auf.
\EndKnitrBlock{rmdpseudocode}

\subsection{Fehlende Werte ggf.
ersetzen}\label{fehlende-werte-ggf.-ersetzen}

Ist die Anzahl der fehlenden Werte zu groß, als dass wir es verkraften
könnten, die Zeilen zu löschen, so können wir die fehlenden Werte
ersetzen. Allein, das ist ein weites Feld und übersteigt den Anspruch
dieses Kurses\footnote{Das sagen Autoren, wenn sie nicht genau wissen,
  wie etwas funktioniert.}. Eine einfache, aber nicht die beste
Möglichkeit, besteht darin, die fehlenden Werte durch einen
repräsentativen Wert, z.B. den Mittelwert der Spalte, zu ersetzen.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{interest =} \KeywordTok{replace}\NormalTok{(.}\OperatorTok{$}\NormalTok{interest,}
                            \KeywordTok{is.na}\NormalTok{(stats_test}\OperatorTok{$}\NormalTok{interest),}
                            \KeywordTok{mean}\NormalTok{(stats_test}\OperatorTok{$}\NormalTok{interest, }
                                 \DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{))) ->}\StringTok{ }\NormalTok{stats_test}

\KeywordTok{sum}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(stats_test}\OperatorTok{$}\NormalTok{interest))}
\CommentTok{#> [1] 0}
\end{Highlighting}
\end{Shaded}

\texttt{replace}\footnote{aus dem ``Standard-R'', d.h. Paket ``base''.}
ersetzt Werte aus dem Vektor \texttt{stats\_test\$interest} alle Werte,
für die \texttt{is.na(stats\_test\$interest)} wahr ist, bei Zeilen mit
fehlenden Werten in dieser Spalte also. Diese Werte werden durch den
Mittelwert der Spalte ersetzt\footnote{Hier findet sich eine
  ausführlichere Darstellung:
  \url{https://sebastiansauer.github.io/checklist_data_cleansing/index.html}}.
Der Punkt \texttt{.} ersetzt den Daten der Tabelle (wir hätten aber auch
den Namen der Tabelle ausschreiben können).

\subsection{Nach Fehlern suchen}\label{nach-fehlern-suchen}

Leicht schleichen sich Tippfehler oder andere Fehler ein. Man sollte
darauf prüfen; so könnte man sich ein Histogramm ausgeben lassen pro
Variable, um ``ungewöhnliche'' Werte gut zu erkennen. Meist geht das
besser als durch das reine Betrachten von Zahlen. Gibt es wenig
unterschiedliche Werte, so kann man sich auch die unterschiedlichen
Werte ausgeben lassen.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{(interest, }\DataTypeTok{sort =} \OtherTok{TRUE}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\NormalTok{head}
\CommentTok{#> # A tibble: 6 x 2}
\CommentTok{#>   interest     n}
\CommentTok{#>      <dbl> <int>}
\CommentTok{#> 1     3.21    68}
\CommentTok{#> 2     3.00    66}
\CommentTok{#> 3     2.00    47}
\CommentTok{#> 4     5.00    45}
\CommentTok{#> 5     4.00    41}
\CommentTok{#> 6     1.00    30}
\end{Highlighting}
\end{Shaded}

Da in der Umfrage nur ganze Zahlen von 1 bis 5 abgefragt wurden, ist die
\texttt{3.21...} auf den ersten Blick suspekt. In diesem Fall ist aber
alles ok, da wir diesen Wert selber erzeugt haben.

Findet man `merkwürdige' (unplausible) Werte, so kann es sinnvoll sein,
diese Werte herauszunehmen (im Detail eine schwierige Entscheidung).
Besser als die Zeilen zu löschen, ist es oft, diese Werte in \texttt{NA}
umzuwandeln. Sagen wir, wir möchten alle Fälle mit
\texttt{score\ \textless{}\ 21} entfernen bzw. in \texttt{NA} umwandeln.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{score_bereinigt =} \KeywordTok{replace}\NormalTok{(.}\OperatorTok{$}\NormalTok{score, }
\NormalTok{                                   .}\OperatorTok{$}\NormalTok{score }\OperatorTok{<}\StringTok{ }\DecValTok{21}\NormalTok{, }
                                   \OtherTok{NA}\NormalTok{)) ->}\StringTok{ }\NormalTok{stats_test}
\end{Highlighting}
\end{Shaded}

\subsection{Ausreißer identifizieren}\label{ausreier-identifizieren}

Ähnlich zu Fehlern, steht man Ausreißer häufig skeptisch gegenüber.
Allerdings kann man nicht pauschal sagen, das Extremwerte entfernt
werden sollen: Vielleicht war jemand in der Stichprobe wirklich nur
1.20m groß? Hier gilt es, begründet und nachvollziehbar im Einzelfall zu
entscheiden. Histogramme und Boxplots sind wieder ein geeignetes Mittel,
um Ausreißer zu finden (vgl. Abb. \ref{fig:fig-ausreisser}).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ score, }\DataTypeTok{data =}\NormalTok{ stats_test, }\DataTypeTok{binwidth =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{043_Typische_Probleme_Datenanalyse_files/figure-latex/fig-ausreisser-1} 

}

\caption{Ausreißer identifizieren}\label{fig:fig-ausreisser}
\end{figure}

Mit \texttt{binwidth\ =\ 1} sagen wir, dass jeder Balken (bin) eine
Breite (width) von 1 haben soll.

\subsection{Hochkorrelierte Variablen
finden}\label{hochkorrelierte-variablen-finden}

Haben zwei Leute die gleiche Meinung, so ist einer von beiden
überflüssig - wird behauptet. Ähnlich bei Variablen; sind zwei Variablen
sehr hoch korreliert (\textgreater{}.9, als grober (!) Richtwert), so
bringt die zweite kaum Informationszuwachs zur ersten. Und kann z.B.
ausgeschlossen werden.

Nehmen wir dazu den Datensatz \texttt{extra} her.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{extra <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"data/extra.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{extra }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(i01}\OperatorTok{:}\NormalTok{i10) }\OperatorTok{%>%}\StringTok{ }\CommentTok{# Wähle die Variablen von i1 bis i10 aus}
\StringTok{  }\KeywordTok{correlate}\NormalTok{() ->}\StringTok{ }\NormalTok{km   }\CommentTok{# Korrelationsmatrix berechnen}
\NormalTok{km  }
\CommentTok{#> # A tibble: 10 x 11}
\CommentTok{#>    rowname   i01   i02r    i03   i04    i05  i06r   i07   i08   i09    i10}
\CommentTok{#>      <chr> <dbl>  <dbl>  <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl>}
\CommentTok{#>  1     i01    NA 0.4676 0.1167 0.433 0.4437 0.450 0.298 0.349 0.363 0.1866}
\CommentTok{#>  2    i02r 0.468     NA 0.1152 0.349 0.3951 0.516 0.235 0.286 0.245 0.0607}
\CommentTok{#>  3     i03 0.117 0.1152     NA 0.057 0.0679 0.154 0.122 0.109 0.038 0.0922}
\CommentTok{#>  4     i04 0.433 0.3495 0.0570    NA 0.6604 0.286 0.440 0.193 0.233 0.3525}
\CommentTok{#>  5     i05 0.444 0.3951 0.0679 0.660     NA 0.321 0.402 0.270 0.298 0.2988}
\CommentTok{#>  6    i06r 0.450 0.5159 0.1542 0.286 0.3207    NA 0.150 0.267 0.264 0.1317}
\CommentTok{#>  7     i07 0.298 0.2345 0.1223 0.440 0.4016 0.150    NA 0.303 0.209 0.3444}
\CommentTok{#>  8     i08 0.349 0.2863 0.1094 0.193 0.2703 0.267 0.303    NA 0.360 0.1807}
\CommentTok{#>  9     i09 0.363 0.2447 0.0380 0.233 0.2982 0.264 0.209 0.360    NA 0.1423}
\CommentTok{#> 10     i10 0.187 0.0607 0.0922 0.352 0.2988 0.132 0.344 0.181 0.142     NA}
\end{Highlighting}
\end{Shaded}

In diesem Beispiel sind keine Variablen sehr hoch korreliert. Wir leiten
keine weiteren Schritte ein, abgesehen von einer Visualisierung.

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{km }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{shave}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\CommentTok{# Oberes Dreieck ist redundant, wird "abrasiert"}
\StringTok{  }\KeywordTok{rplot}\NormalTok{()  }\CommentTok{# Korrelationsplot}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{043_Typische_Probleme_Datenanalyse_files/figure-latex/fig-corrr-1} 

}

\caption{Ein Korrelationsplot}\label{fig:fig-corrr}
\end{figure}

Die Funktion \texttt{correlate} stammt aus dem Paket
\texttt{corrr}\footnote{\url{https://github.com/drsimonj/corrr}},
welches vorher installiert und geladen sein muss. Hier ist die
Korrelation nicht zu groß, so dass wir keine weiteren Schritte
unternehmen. Hätten wir eine sehr hohe Korrelation gefunden, so hätten
wir eine der beiden beteiligten Variablen aus dem Datensatz löschen
können.

\subsection{z-Standardisieren}\label{z-standardisieren}

Für eine Reihe von Analysen ist es wichtig, die Skalierung der Variablen
zur vereinheitlichen. Die z-Standardisierung ist ein übliches Vorgehen.
Dabei wird der Mittelwert auf 0 transformiert und die SD auf 1; man
spricht - im Falle von (hinreichend) normalverteilten Variablen - jetzt
von der \emph{Standardnormalverteilung}\index{Standardnormalverteilung}.
Unterscheiden sich zwei Objekte A und B in einer
standardnormalverteilten Variablen, so sagt dies nur etwas zur relativen
Position von A zu B innerhalb ihrer Verteilung aus - im Gegensatz zu den
Rohwerten.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{extra }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(i01, i02r) }\OperatorTok{%>%}\StringTok{  }
\StringTok{  }\KeywordTok{scale}\NormalTok{() }\OperatorTok{%>%}\StringTok{  }\CommentTok{# z-standardisieren}
\StringTok{  }\KeywordTok{head}\NormalTok{()  }\CommentTok{# nur die ersten paar Zeilen abdrucken}
\CommentTok{#>         i01   i02r}
\CommentTok{#> [1,] -0.519 -0.134}
\CommentTok{#> [2,] -2.002 -1.383}
\CommentTok{#> [3,] -0.519  1.115}
\CommentTok{#> [4,] -0.519 -0.134}
\CommentTok{#> [5,]  0.964 -0.134}
\CommentTok{#> [6,] -0.519 -1.383}
\end{Highlighting}
\end{Shaded}

Dieser Befehl liefert z-standardisierte Spalten zurück. Kommoder ist es
aber, alle Spalten des Datensatzes zurück zu bekommen, wobei zusätzlich
die z-Werte aller numerischen Variablen hinzugekommen sind:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{extra }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate_if}\NormalTok{(is.numeric, }\KeywordTok{funs}\NormalTok{(}\StringTok{"z"}\NormalTok{ =}\StringTok{ }\NormalTok{scale)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{head}
\end{Highlighting}
\end{Shaded}

Der Befehl \texttt{mutate} berechnet eine neue Spalte;
\texttt{mutate\_if} tut dies nur, wenn die Spalte numerisch ist. Die
neue Spalte wird berechnet als z-Transformierung der alten Spalte; zum
Spaltenname wird ein ``\_z" hinzugefügt. Natürlich hätten wir auch mit
\texttt{select} ``händisch'' die relevanten Spalten auswählen können.

\subsection{Quasi-Konstante finden}\label{quasi-konstante-finden}

Hier suchen wir nach Variablen (Spalten), die nur einen Wert oder
zumindest nur sehr wenige verschiedene Werte aufweisen. Oder, ähnlich:
Wenn 99.9\% der Fälle nur von einem Wert bestritten wird. In diesen
Fällen kann man die Variable als ``Quasi-Konstante'' bezeichnen.
Quasi-Konstanten sind für die Modellierung von keiner oder nur geringer
Bedeutung; sie können in der Regel für weitere Analysen ausgeschlossen
werden.

Haben wir z.B. nur Männer im Datensatz, so kann das Geschlecht nicht für
Unterschiede im Einkommen verantwortlich sein. Besser ist es, die
Variable Geschlecht zu entfernen. Auch hier sind Histogramme oder
Boxplots von Nutzen zur Identifikation von (Quasi-)Konstanten.
Alternativ kann man sich auch pro die Streuung (numerische Variablen)
oder die Anzahl unterschiedlicher Werte (qualitative Variablen) ausgeben
lassen:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{IQR}\NormalTok{(extra}\OperatorTok{$}\NormalTok{n_facebook_friends, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)  }\CommentTok{# keine Konstante}
\CommentTok{#> [1] 300}
\KeywordTok{n_distinct}\NormalTok{(extra}\OperatorTok{$}\NormalTok{sex)  }\CommentTok{# es scheint 3 Geschlechter zu geben...}
\CommentTok{#> [1] 3}
\end{Highlighting}
\end{Shaded}

\subsection{Auf Normalverteilung
prüfen}\label{auf-normalverteilung-prufen}

Einige statistische Verfahren gehen von normalverteilten Variablen aus,
daher macht es Sinn, Normalverteilung zu prüfen. \emph{Perfekte}
Normalverteilung ist genau so häufig wie \emph{perfekte} Kreise in der
Natur. Entsprechend werden Signifikanztests, die ja auf perfekte
Normalverteilung prüfen, \emph{immer signifikant} sein, sofern die
\emph{Stichprobe groß} genug ist. Daher ist meist zweckmäßiger, einen
graphischen ``Test'' durchzuführen: ein Histogramm, ein QQ-Plot oder ein
Dichte-Diagramm als ``glatt geschmirgelte'' Variante des Histogramms
bieten sich an (s. Abb. \ref{fig:fig-norm-check}).

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{043_Typische_Probleme_Datenanalyse_files/figure-latex/fig-norm-check-1} 

}

\caption{Visuelles Prüfen der Normalverteilung}\label{fig:fig-norm-check}
\end{figure}

Während die der mittlere Extraversionswert recht gut normalverteilt ist,
ist die Anzahl der Facebookfreunde ordentlich (rechts-)schief. Bei
schiefen Verteilung können Transformationen Abhilfe schaffen; ein Thema,
auf das wir hier nicht weiter eingehen.

\subsection{\texorpdfstring{Werte umkodieren und partionieren
(``binnen'')}{Werte umkodieren und partionieren (binnen)}}\label{werte-umkodieren-und-partionieren-binnen}

\emph{Umkodieren}\index{Umkodieren} meint, die Werte zu ändern. Man
sieht immer mal wieder, dass die Variable ``gender'' (Geschlecht) mit
\texttt{1} und \texttt{2} kodiert ist. Verwechslungen sind da
vorprogrammiert (``Ich bin mir echt ziemlich sicher, dass ich 1 für
Männer kodiert habe, wahrscheinlich\ldots{}''). Besser wäre es, die
Ausprägungen \texttt{male} und \texttt{female} (``Mann'', ``Frau'') o.ä.
zu verwenden (vgl. Abb. \ref{fig:umkodieren}).

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/typ_prob/umkodieren_crop} 

}

\caption{Sinnbild für Umkodieren}\label{fig:umkodieren}
\end{figure}

Partitionieren\index\{Partitionieren) oder
\emph{``Binnen''}\index{Binnen} meint, eine kontinuierliche Variablen in
einige Bereiche (mindestens 2) zu zerschneiden. Damit macht man aus
einer kontinuierlichen Variablen eine diskrete. Ein Bild erläutert das
am einfachsten (vgl. Abb. \ref{fig:cut-schere}).

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/typ_prob/cut_schere_crop} 

}

\caption{Sinnbild zum 'Binnen'}\label{fig:cut-schere}
\end{figure}

\subsubsection{\texorpdfstring{Umkodieren und partionieren mit
\texttt{car::recode}}{Umkodieren und partionieren mit car::recode}}\label{umkodieren-und-partionieren-mit-carrecode}

Manchmal möchte man z.B. negativ gepolte Items umdrehen oder bei
kategoriellen Variablen kryptische Bezeichnungen in sprechendere
umwandeln. Hier gibt es eine Reihe praktischer Befehle, z.B.
\texttt{recode} aus dem Paket \texttt{car}. Schauen wir uns ein paar
Beispiele zum Umkodieren an.

\begin{Shaded}
\begin{Highlighting}[]


\NormalTok{stats_test}\OperatorTok{$}\NormalTok{score_fac <-}\StringTok{ }\NormalTok{car}\OperatorTok{::}\KeywordTok{recode}\NormalTok{(stats_test}\OperatorTok{$}\NormalTok{study_time, }
                        \StringTok{"5 = 'sehr viel'; 2:4 = 'mittel'; 1 = 'wenig'"}\NormalTok{,}
                        \DataTypeTok{as.factor.result =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{stats_test}\OperatorTok{$}\NormalTok{score_fac <-}\StringTok{ }\NormalTok{car}\OperatorTok{::}\KeywordTok{recode}\NormalTok{(stats_test}\OperatorTok{$}\NormalTok{study_time, }
                        \StringTok{"5 = 'sehr viel'; 2:4 = 'mittel'; 1 = 'wenig'"}\NormalTok{,}
                        \DataTypeTok{as.factor.result =} \OtherTok{FALSE}\NormalTok{)}

\NormalTok{stats_test}\OperatorTok{$}\NormalTok{study_time_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\NormalTok{car}\OperatorTok{::}\KeywordTok{recode}\NormalTok{(stats_test}\OperatorTok{$}\NormalTok{study_time, }
                                       \StringTok{"5 = 'sehr viel'; 4 = 'wenig'; }
\StringTok{                                       else = 'Hilfe'"}\NormalTok{, }
                                       \DataTypeTok{as.factor.result =} \OtherTok{TRUE}\NormalTok{)}

\KeywordTok{head}\NormalTok{(stats_test}\OperatorTok{$}\NormalTok{study_time_}\DecValTok{2}\NormalTok{)}
\CommentTok{#> [1] sehr viel Hilfe     sehr viel Hilfe     wenig     Hilfe    }
\CommentTok{#> Levels: Hilfe sehr viel wenig}
\end{Highlighting}
\end{Shaded}

Der Befehle \texttt{recode} ist praktisch; mit \texttt{:} kann man ``von
bis'' ansprechen (das ginge mit \texttt{c()} übrigens auch);
\texttt{else} für ``ansonsten'' ist möglich und mit
\texttt{as.factor.result} kann man entweder einen Faktor oder eine
Text-Variable zurückgeliefert bekommen. Der ganze ``Wechselterm'' steht
in Anführungsstrichen (\texttt{"}). Einzelne Teile des Wechselterms sind
mit einem Strichpunkt (\texttt{;}) voneinander getrennt.

Das klassische Umkodieren von Items aus Fragebögen kann man so
anstellen; sagen wir \texttt{interest} soll umkodiert werden:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test}\OperatorTok{$}\NormalTok{no_interest <-}\StringTok{ }\NormalTok{car}\OperatorTok{::}\KeywordTok{recode}\NormalTok{(stats_test}\OperatorTok{$}\NormalTok{interest, }
                                      \StringTok{"1 = 6; 2 = 5; 3 = 4; 4 = 3; }
\StringTok{                                      5 = 2; 6 = 1; else = NA"}\NormalTok{)}
\KeywordTok{glimpse}\NormalTok{(stats_test}\OperatorTok{$}\NormalTok{no_interest)}
\CommentTok{#>  num [1:306] 2 4 1 5 1 NA NA 4 2 2 ...}
\end{Highlighting}
\end{Shaded}

Bei dem Wechselterm muss man aufpassen, nichts zu verwechseln; die
Zahlen sehen alle ähnlich aus\ldots{}

Testen kann man den Erfolg des Umpolens mit

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{count}\NormalTok{(stats_test, interest)}
\CommentTok{#> # A tibble: 7 x 2}
\CommentTok{#>   interest     n}
\CommentTok{#>      <dbl> <int>}
\CommentTok{#> 1     1.00    30}
\CommentTok{#> 2     2.00    47}
\CommentTok{#> 3     3.00    66}
\CommentTok{#> 4     3.21    68}
\CommentTok{#> 5     4.00    41}
\CommentTok{#> 6     5.00    45}
\CommentTok{#> 7     6.00     9}
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{count}\NormalTok{(stats_test, no_interest)}
\CommentTok{#> # A tibble: 7 x 2}
\CommentTok{#>   no_interest     n}
\CommentTok{#>         <dbl> <int>}
\CommentTok{#> 1           1     9}
\CommentTok{#> 2           2    45}
\CommentTok{#> 3           3    41}
\CommentTok{#> 4           4    66}
\CommentTok{#> 5           5    47}
\CommentTok{#> 6           6    30}
\CommentTok{#> 7          NA    68}
\end{Highlighting}
\end{Shaded}

Scheint zu passen. Noch praktischer ist, dass man so auch numerische
Variablen in Bereiche aufteilen kann (``binnen''):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test}\OperatorTok{$}\NormalTok{Ergebnis <-}\StringTok{ }\NormalTok{car}\OperatorTok{::}\KeywordTok{recode}\NormalTok{(stats_test}\OperatorTok{$}\NormalTok{score, }
                                   \StringTok{"1:38 = 'durchgefallen'; }
\StringTok{                                   else = 'bestanden'"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Natürlich gibt es auch eine Pfeifen kompatible Version, um Variablen
umzukodieren bzw. zu binnen: \texttt{dplyr::recode}\footnote{\url{https://blog.rstudio.org/2016/06/27/dplyr-0-5-0/}}.
Die Syntax ist allerdings etwas weniger komfortabel (da strenger), so
dass wir an dieser Stelle bei \texttt{car::recode} bleiben.

\subsubsection{Einfaches Umkodieren mit einer
Logik-Prüfung}\label{einfaches-umkodieren-mit-einer-logik-prufung}

Nehmen wir an, wir möchten die Anzahl der Punkte in einer
Statistikklausur (\texttt{score}) umkodieren in eine Variable
``bestanden'' mit den zwei Ausprägungen ``ja'' und ``nein''; der
griesgrämige Professor beschließt, dass die Klausur ab 25 Punkten (von
40) bestanden sei. Die Umkodierung ist also von der Art ``viele
Ausprägungen in zwei Ausprägungen umkodieren''. Das kann man z.B. so
erledigen:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test}\OperatorTok{$}\NormalTok{bestanden <-}\StringTok{ }\NormalTok{stats_test}\OperatorTok{$}\NormalTok{score }\OperatorTok{>}\StringTok{ }\DecValTok{24}

\KeywordTok{head}\NormalTok{(stats_test}\OperatorTok{$}\NormalTok{bestanden)}
\CommentTok{#> [1]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE}
\end{Highlighting}
\end{Shaded}

Genauso könnte man sich die ``Grenzfälle'' - die Bemitleidenswerten mit
24 Punkten - anschauen (knapp daneben ist auch vorbei, so der
griesgrämige Professor weiter):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test}\OperatorTok{$}\NormalTok{Grenzfall <-}\StringTok{ }\NormalTok{stats_test}\OperatorTok{$}\NormalTok{score }\OperatorTok{==}\StringTok{ }\DecValTok{24}

\KeywordTok{count}\NormalTok{(stats_test, Grenzfall)}
\CommentTok{#> # A tibble: 2 x 2}
\CommentTok{#>   Grenzfall     n}
\CommentTok{#>       <lgl> <int>}
\CommentTok{#> 1     FALSE   294}
\CommentTok{#> 2      TRUE    12}
\end{Highlighting}
\end{Shaded}

Natürlich könnte man auch hier ``Durchpfeifen'':

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test <-}\StringTok{ }
\NormalTok{stats_test }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Grenzfall =}\NormalTok{ score }\OperatorTok{==}\StringTok{ }\DecValTok{24}\NormalTok{)}

\KeywordTok{count}\NormalTok{(stats_test, Grenzfall)}
\CommentTok{#> # A tibble: 2 x 2}
\CommentTok{#>   Grenzfall     n}
\CommentTok{#>       <lgl> <int>}
\CommentTok{#> 1     FALSE   294}
\CommentTok{#> 2      TRUE    12}
\end{Highlighting}
\end{Shaded}

\subsubsection{\texorpdfstring{Binnen mit
\texttt{cut}}{Binnen mit cut}}\label{binnen-mit-cut}

Numerische Werte in Klassen zu gruppieren (``to bin'', denglisch:
``binnen'') kann mit dem Befehl \texttt{cut} (and friends) besorgt
werden.

Es lassen sich drei typische Anwendungsformen unterscheiden:

Eine numerische Variable \ldots{}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  in \emph{k} gleich große Klassen gruppieren (gleichgroße Intervalle)
\item
  so in Klassen gruppieren, dass in jeder Klasse \emph{n} Beobachtungen
  sind (gleiche Gruppengrößen)
\item
  in beliebige Klassen gruppieren
\end{enumerate}

\paragraph{Gleichgroße Intervalle}\label{gleichgroe-intervalle}

Nehmen wir an, wir möchten die numerische Variable ``Körpergröße'' in
drei Gruppen einteilen: ``klein'', ``mittel'' und ``groß''. Der Range
von Körpergröße soll gleichmäßig auf die drei Gruppen aufgeteilt werden,
d.h. der Range (Intervall) der drei Gruppen soll gleich groß sein. Dazu
kann man \texttt{cut\_interval} aus \texttt{ggplot2} nehmen\footnote{d.h.
  \texttt{ggplot2} muss geladen sein; wenn man \texttt{tidyverse} lädt,
  wird \texttt{ggplot2} automatisch auch geladen}.

\begin{Shaded}
\begin{Highlighting}[]


\NormalTok{temp <-}\StringTok{ }\KeywordTok{cut_interval}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ stats_test}\OperatorTok{$}\NormalTok{score, }\DataTypeTok{n =} \DecValTok{3}\NormalTok{)}

\KeywordTok{levels}\NormalTok{(temp)}
\CommentTok{#> [1] "[17,24.7]"   "(24.7,32.3]" "(32.3,40]"}
\end{Highlighting}
\end{Shaded}

\texttt{cut\_interval} liefert eine Variable vom Typ \texttt{factor}
zurück. Hier haben wir das Punktespektrum in drei gleich große Bereiche
unterteilt (d.h. mit jeweils gleichem Punkte-Range).

\paragraph{Gleiche Gruppengrößen}\label{gleiche-gruppengroen}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{temp <-}\StringTok{ }\KeywordTok{cut_number}\NormalTok{(stats_test}\OperatorTok{$}\NormalTok{score, }\DataTypeTok{n =} \DecValTok{2}\NormalTok{)}
\KeywordTok{str}\NormalTok{(temp)}
\CommentTok{#>  Factor w/ 2 levels "[17,31]","(31,40]": 1 1 2 1 2 2 2 1 1 2 ...}
\KeywordTok{median}\NormalTok{(stats_test}\OperatorTok{$}\NormalTok{score)}
\CommentTok{#> [1] 31}
\end{Highlighting}
\end{Shaded}

Mit \texttt{cut\_number} (aus ggplot2) kann man einen Vektor in
\texttt{n} Gruppen mit (etwa) gleich viel Observationen einteilen. Hier
haben wir \texttt{score} am Median geteilt.

\begin{quote}
Teilt man einen Vektor in zwei gleich große Gruppen, so entspricht das
einer Aufteilung am Median (Median-Split).
\end{quote}

\paragraph{In beliebige Klassen
gruppieren}\label{in-beliebige-klassen-gruppieren}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test}\OperatorTok{$}\NormalTok{punkte_gruppe <-}\StringTok{ }\KeywordTok{cut}\NormalTok{(stats_test}\OperatorTok{$}\NormalTok{score, }
                             \DataTypeTok{breaks =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\OtherTok{Inf}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{29}\NormalTok{, }\DecValTok{33}\NormalTok{, }\DecValTok{37}\NormalTok{, }\DecValTok{40}\NormalTok{),}
                             \DataTypeTok{labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"5"}\NormalTok{, }\StringTok{"4"}\NormalTok{, }\StringTok{"3"}\NormalTok{, }\StringTok{"2"}\NormalTok{, }\StringTok{"1"}\NormalTok{))}

\KeywordTok{count}\NormalTok{(stats_test, punkte_gruppe)}
\CommentTok{#> # A tibble: 5 x 2}
\CommentTok{#>   punkte_gruppe     n}
\CommentTok{#>          <fctr> <int>}
\CommentTok{#> 1             5    56}
\CommentTok{#> 2             4    68}
\CommentTok{#> 3             3    63}
\CommentTok{#> 4             2    64}
\CommentTok{#> 5             1    55}
\end{Highlighting}
\end{Shaded}

\texttt{cut} ist im Standard-R (Paket ``base'') enthalten. Mit
\texttt{breaks} gibt man die Intervallgrenzen an. Zu beachten ist, dass
man eine Unter- bzw. Obergrenze angeben muss. D.h. der kleinste Wert in
der Stichprobe wird nicht automatisch als unterste Intervallgrenze
herangezogen. Anschaulich gesprochen ist \texttt{cut} ein Messer, das
ein Seil (die kontinuierliche Variable) mit einem oder mehreren
Schnitten zerschneidet (vgl. Abb. \ref{fig:cut-schere}). Wenn wir 6
Schnitte (\texttt{breaks}) tun, haben wir 5 Teile, wie Abb.
\ref{fig:cut-schere} zeigt. Darum müssen wir auch nur 5 (6-1)
\texttt{labels} für die Teile vergeben.

\section{Deskriptive Statistiken
berechnen}\label{deskriptive-statistiken-berechnen}

\subsection{Mittelwerte pro Zeile
berechnen}\label{mittelwerte-pro-zeile-berechnen}

\subsubsection{\texorpdfstring{\texttt{rowMeans}}{rowMeans}}\label{rowmeans}

Um Umfragedaten auszuwerten, will man häufig einen Mittelwert \emph{pro
Zeile} berechnen. Normalerweise fasst man eine \emph{Spalte} zu einer
Zahl zusammen; aber jetzt, fassen wir eine \emph{Zeile} zu einer Zahl
zusammen. Der häufigste Fall ist, wie gesagt, einen Mittelwert zu bilden
für jede Person. Nehmen wir an, wir haben eine Befragung zur
Extraversion durchgeführt und möchten jetzt den mittleren
Extraversionswert pro Person (d.h. pro Zeile) berechnen.

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{extra_items <-}\StringTok{ }\NormalTok{extra }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(i01}\OperatorTok{:}\NormalTok{i10)  }\CommentTok{# `select` ist aus `dplyr`}

\CommentTok{# oder:}
\CommentTok{# select(extra_items, i01:i10)}

\NormalTok{extra}\OperatorTok{$}\NormalTok{extra_mw <-}\StringTok{ }\KeywordTok{rowMeans}\NormalTok{(extra_items)}
\end{Highlighting}
\end{Shaded}

Da der Datensatz über 28 Spalten verfügt, wir aber nur 10 Spalten
heranziehen möchten, um Zeilen auf eine Zahl zusammenzufassen, bilden
wir als Zwischenschritt einen ``schmäleren'' Datensatz,
\texttt{extra\_items}. Im Anschluss berechnen wir mit \texttt{rowMeans}
die Mittelwerte pro Zeile (engl. ``row'').

\subsection{Mittelwerte pro Spalte
berechnen}\label{mittelwerte-pro-spalte-berechnen}

Eine Möglichkeit ist der Befehl \texttt{summary} aus \texttt{dplyr}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{na.omit }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\KeywordTok{mean}\NormalTok{(score),}
            \KeywordTok{sd}\NormalTok{(score),}
            \KeywordTok{median}\NormalTok{(score),}
            \KeywordTok{IQR}\NormalTok{(score))}
\CommentTok{#>   mean(score) sd(score) median(score) IQR(score)}
\CommentTok{#> 1          31      5.37            31          8}
\end{Highlighting}
\end{Shaded}

Die Logik von \texttt{dplyr} lässt auch einfach Subgruppenanalysen zu.
Z.B. können wir eine Teilmenge des Datensatzes mit \texttt{filter}
erstellen und dann mit \texttt{group\_by} Gruppen vergleichen:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(study_time }\OperatorTok{>}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(interest) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\KeywordTok{median}\NormalTok{(score, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{))}
\CommentTok{#> # A tibble: 6 x 2}
\CommentTok{#>   interest `median(score, na.rm = TRUE)`}
\CommentTok{#>      <dbl>                         <dbl>}
\CommentTok{#> 1        1                            28}
\CommentTok{#> 2        2                            30}
\CommentTok{#> 3        3                            33}
\CommentTok{#> 4        4                            31}
\CommentTok{#> 5        5                            34}
\CommentTok{#> 6        6                            34}
\end{Highlighting}
\end{Shaded}

Wir können auch Gruppierungskriterien unterwegs erstellen:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{na.omit }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(study_time }\OperatorTok{>}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(}\DataTypeTok{intessiert =}\NormalTok{ interest }\OperatorTok{>}\StringTok{ }\DecValTok{3}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{md_gruppe =} \KeywordTok{median}\NormalTok{(score))}
\CommentTok{#> # A tibble: 2 x 2}
\CommentTok{#>   intessiert md_gruppe}
\CommentTok{#>        <lgl>     <int>}
\CommentTok{#> 1      FALSE        30}
\CommentTok{#> 2       TRUE        32}
\end{Highlighting}
\end{Shaded}

Die beiden Gruppen von \texttt{interessiert} sind ``ja, interessiert''
(\texttt{interest\ \textgreater{}\ 3} ist \texttt{TRUE}) und ``nein,
nicht interessiert'' (\texttt{interest\ \textgreater{}\ 3} ist
\texttt{FALSE}). Außerdem haben wir der Spalte, die die Mediane
zurückliefert einen ansprechenderen Namen gegeben (\texttt{md\_gruppe}).

Etwas expliziter wäre es, \texttt{mutate} zu verwenden, um die Variable
\texttt{interessiert} zu erstellen:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{na.omit }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(study_time }\OperatorTok{>}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{interessiert =}\NormalTok{ interest }\OperatorTok{>}\StringTok{ }\DecValTok{3}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(interessiert) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{md_gruppe =} \KeywordTok{median}\NormalTok{(score),}
            \DataTypeTok{mw_gruppe =} \KeywordTok{mean}\NormalTok{(score))}
\CommentTok{#> # A tibble: 2 x 3}
\CommentTok{#>   interessiert md_gruppe mw_gruppe}
\CommentTok{#>          <lgl>     <int>     <dbl>}
\CommentTok{#> 1        FALSE        30      31.2}
\CommentTok{#> 2         TRUE        32      31.8}
\end{Highlighting}
\end{Shaded}

Dieses Mal haben wir nicht nur eine Spalte mit den Medianwerten, sondern
zusätzlich noch mit Mittelwerten berechnet.

\BeginKnitrBlock{rmdcaution}
Statistiken, die auf dem Mittelwert (arithmetisches Mittel) beruhen,
sind nicht robust gegenüber Ausreißer: Schon wenige Extremwerte können
diese Statistiken so verzerren, dass sie erheblich an Aussagekraft
verlieren.

Daher: besser robuste Statistiken verwenden. Der Median, der Modus und
der IQR bieten sich an.
\EndKnitrBlock{rmdcaution}

\subsection{Korrelationstabellen
berechnen}\label{korrelationstabellen-berechnen}

Korrelationen bzw. Korrelationstabellen lassen sich mit dem
R-Standardbefehl \texttt{cor} berechnen:

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{stats_test }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(study_time,interest,score) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{cor}\NormalTok{()}
\CommentTok{#>            study_time interest score}
\CommentTok{#> study_time          1       NA    NA}
\CommentTok{#> interest           NA    1.000 0.196}
\CommentTok{#> score              NA    0.196 1.000}
\end{Highlighting}
\end{Shaded}

Oh! Lauter NAs! Besser wir löschen Zeilen mit fehlenden Werten bevor wir
die Korrelation ausrechnen:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(study_time}\OperatorTok{:}\NormalTok{score) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{na.omit }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{cor}\NormalTok{()}
\CommentTok{#>            study_time self_eval interest score}
\CommentTok{#> study_time      1.000     0.559    0.461 0.441}
\CommentTok{#> self_eval       0.559     1.000    0.360 0.628}
\CommentTok{#> interest        0.461     0.360    1.000 0.223}
\CommentTok{#> score           0.441     0.628    0.223 1.000}
\end{Highlighting}
\end{Shaded}

Alternativ zu \texttt{cor} kann man auch \texttt{corrr:correlate}
verwenden:

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{stats_test }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(study_time}\OperatorTok{:}\NormalTok{score) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{correlate}
\CommentTok{#> # A tibble: 4 x 5}
\CommentTok{#>      rowname study_time self_eval interest score}
\CommentTok{#>        <chr>      <dbl>     <dbl>    <dbl> <dbl>}
\CommentTok{#> 1 study_time         NA     0.559    0.461 0.441}
\CommentTok{#> 2  self_eval      0.559        NA    0.360 0.628}
\CommentTok{#> 3   interest      0.461     0.360       NA 0.196}
\CommentTok{#> 4      score      0.441     0.628    0.196    NA}
\end{Highlighting}
\end{Shaded}

\texttt{correlate} hat den Vorteil, dass es bei fehlenden Werten einen
Wert ausgibt; die Korrelation wird paarweise mit den verfügbaren
(nicht-fehlenden) Werten berechnet. Außerdem wird eine Dataframe
(genauer: tibble) zurückgeliefert, was häufig praktischer ist zur
Weiterverarbeitung. Wir könnten jetzt die resultierende
Korrelationstabelle plotten, vorher ``rasieren'' wir noch das
redundanten obere Dreieck ab (da Korrelationstabellen ja symmetrisch
sind):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(study_time}\OperatorTok{:}\NormalTok{score) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{correlate }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{shave }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{rplot}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{043_Typische_Probleme_Datenanalyse_files/figure-latex/rplot-demo-1} \end{center}

\section{Befehlsübersicht}\label{befehlsubersicht-3}

Tabelle \ref{tab:befehle-praxisprobleme} stellt die Befehle dieses
Kapitels dar.

\begin{table}

\caption{\label{tab:befehle-praxisprobleme}Befehle des Kapitels 'Praxisprobleme'}
\centering
\begin{tabular}[t]{l|l}
\hline
Paket::Funktion & Beschreibung\\
\hline
na.omit & Löscht Zeilen, die fehlende Werte enthalten\\
\hline
nrow & Liefert die Anzahl der Zeilen des Dataframes zurück\\
\hline
complete.cases & Zeigt die Zeilen ohne fehlenden Werte\\
\hline
car::recode & Kodiert Werte um\\
\hline
cut & Schneidet eine kontinuierliche Variable in Wertebereiche\\
\hline
rowMeans & Berechnet Zeilen-Mittelwerte\\
\hline
dplyr::rowwise & Gruppiert nach Zeilen\\
\hline
ggplot2::cut\_number & Schneidet in n gleich große Bereiche\\
\hline
ggplot2::cut\_interval & Schneidet ein Intervalle der Größe k\\
\hline
head & Zeigt nur die ersten Werte eines Objekts an.\\
\hline
scale & z-skaliert eine Variable\\
\hline
dplyr::select\_if & Wählt eine Spalte aus, wenn ein Kriterium erfüllt ist\\
\hline
dplyr::glimpse & Gibt einen Überblick über einen Dataframe\\
\hline
dplyr::mutate\_if & definiert eine Spalte, wenn eine Kriterium erfüllt ist\\
\hline
: & Definiert einen Bereich von … bis …\\
\hline
corrr::correlate & Berechnet Korrelationtabelle, liefert einen Dataframe zurück\\
\hline
cor & Berechnet Korrelationtabelle\\
\hline
corrr::rplot & Plottet Korrelationsmatrix von correlate\\
\hline
corrr::shave & “Rasiert” redundantes Dreieck in Korrelationsmatrix ab\\
\hline
\end{tabular}
\end{table}

\chapter{\texorpdfstring{Fallstudie
`movies'}{Fallstudie movies}}\label{case-movies}

\begin{center}\includegraphics[width=0.3\linewidth]{images/FOM} \end{center}

\begin{center}\includegraphics[width=0.1\linewidth]{images/licence} \end{center}

\BeginKnitrBlock{rmdcaution}
Lernziele:

\begin{itemize}
\tightlist
\item
  Grundlegende Funktionen von \texttt{dplyr} andwenden können.
\item
  Das Konzept der Pfeife in einem echten Datensatz anwenden können.
\item
  Auch mit relativ großen Daten sicher hantieren können.
\end{itemize}
\EndKnitrBlock{rmdcaution}

Der Datensatz \texttt{movies} enthält Bewertungen von Filmen, zusammen
mit einigen zusätzlichen Informationen wie Genre, Erscheinungsjahr und
Budgethöhe. Wir nutzen diesen Datensatz um uns einige Übung mit
Aufbereiten und Zusammenfassen von Daten zu verschaffen.

Für dieses Kapitel werden folgende Pakete benötigt:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)  }\CommentTok{# Datenjudo und Visualisierung}
\KeywordTok{library}\NormalTok{(corrr)  }\CommentTok{# Korrelation}
\KeywordTok{library}\NormalTok{(ggplot2movies)  }\CommentTok{# Daten}
\end{Highlighting}
\end{Shaded}

Zunächst laden wir die Daten und werfen einen Blick in den Datensatz:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(movies, }\DataTypeTok{package =} \StringTok{"ggplot2movies"}\NormalTok{)}
\KeywordTok{glimpse}\NormalTok{(movies)}
\end{Highlighting}
\end{Shaded}

Hier findet man einige Erklärungen zu diesem Datensatz:
\url{http://had.co.nz/data/movies/}.

\section{Wie viele Filme gibt es pro
Genre?}\label{wie-viele-filme-gibt-es-pro-genre}

Normalerweise würde man für diese Frage eine Spalte wie ``Genre'' nehmen
und die verschiedenen Werte dieser Spalte auszählen. Das geht sehr
bequem mit \texttt{dplyr::count}. Hier gibt es allerdings so eine Spalte
nicht. Wir müssen uns anders behelfen.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{movies }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(Action}\OperatorTok{:}\NormalTok{Short) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise_all}\NormalTok{(}\KeywordTok{funs}\NormalTok{(sum))}
\CommentTok{#> # A tibble: 1 x 7}
\CommentTok{#>   Action Animation Comedy Drama Documentary Romance Short}
\CommentTok{#>    <int>     <int>  <int> <int>       <int>   <int> <int>}
\CommentTok{#> 1   4688      3690  17271 21811        3472    4744  9458}
\end{Highlighting}
\end{Shaded}

Auf Deutsch heißt diese Syntax

\BeginKnitrBlock{rmdpseudocode}
Nimm die Tabelle ``movies'' UND DANN\\
nimm alle Spalten von ``Action'' bis ``Short'' UND DANN\\
fasse alle Spalten (die wir genommen haben) zusammen und zwar\ldots{}
mit der oder den Funktionen ``Summe'' (sum).
\EndKnitrBlock{rmdpseudocode}

Genau wie der Befehl \texttt{summarise} fasst auch
\texttt{summarise\_all} Spalten zu einer Zahl zusammen - nur eben nicht
\emph{eine}, sondern \emph{alle} Spalten eines Dataframe. Die
Funktion(en), die beim Zusammenfassen verwendet werden sollen, werden
mit \texttt{funs()} definiert.

\section{Welches Genre ist am
häufigsten?}\label{welches-genre-ist-am-haufigsten}

Bzw. in welchem Genre wurden am meisten Filme gedreht (in unserem
Datensatz)?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{movies }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(Action}\OperatorTok{:}\NormalTok{Short) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise_all}\NormalTok{(}\KeywordTok{funs}\NormalTok{(sum)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{gather}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(}\OperatorTok{-}\NormalTok{value)}
\CommentTok{#> # A tibble: 7 x 2}
\CommentTok{#>           key value}
\CommentTok{#>         <chr> <int>}
\CommentTok{#> 1       Drama 21811}
\CommentTok{#> 2      Comedy 17271}
\CommentTok{#> 3       Short  9458}
\CommentTok{#> 4     Romance  4744}
\CommentTok{#> 5      Action  4688}
\CommentTok{#> 6   Animation  3690}
\CommentTok{#> 7 Documentary  3472}
\end{Highlighting}
\end{Shaded}

Der Befehl \texttt{gather} baut einen Dataframe von ``breit'' nach
``lang'' um (vgl. Kapitel \ref{normalform}). Ah, \sout{Schmunzetten}
Dramen sind also am häufigsten (wie der Befehl \texttt{arrange} dann
zeigt). Welcome to Hollywood. :tada:

\section{Zusammenhang zwischen Budget und
Beurteilung}\label{zusammenhang-zwischen-budget-und-beurteilung}

Werden teurere Filme (also Filme mit mehr Budget) besser beurteilt im
Schnitt? Das würde man erwarten, denn zum Spaß werden die Investoren
wohl nicht ihr Geld raus. Schauen wir es uns an.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{movies }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(budget, rating, votes) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{correlate }
\CommentTok{#> # A tibble: 3 x 4}
\CommentTok{#>   rowname  budget  rating votes}
\CommentTok{#>     <chr>   <dbl>   <dbl> <dbl>}
\CommentTok{#> 1  budget      NA -0.0142 0.441}
\CommentTok{#> 2  rating -0.0142      NA 0.104}
\CommentTok{#> 3   votes  0.4413  0.1037    NA}
\end{Highlighting}
\end{Shaded}

Wir haben gerade die drei Spalten \texttt{budget}, \texttt{rating} und
\texttt{votes} ausgewählt, dann in der nächsten Zeile die fehlenden
Werte eliminiert und schließlich die Korrelation zwischen allen Paaren
gebildet. Interessanterweise gibt es keine Korrelation zwischen dem
Budget und dem Rating! Teuere Filme sind also mitnichten besser
bewertet. Allerdings haben Filme mit mehr Budget eine größere Anzahl an
Bewertungen, sind also offenbar bekannter. Vielleicht gehen dann auch
entsprechend mehr Leute im Kino - auch wenn diese Filme nicht besser
sind. Teurere Filme sind also bekannter, wenn auch nicht besser
(beurteilt); so könnte man die Daten lesen.

\section{\texorpdfstring{Wurden die Filme im Lauf der Jahre teurer
und/oder
``besser''?}{Wurden die Filme im Lauf der Jahre teurer und/oder besser?}}\label{wurden-die-filme-im-lauf-der-jahre-teurer-undoder-besser}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{movies }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(year, rating, budget) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{correlate}
\CommentTok{#> # A tibble: 3 x 4}
\CommentTok{#>   rowname    year  rating  budget}
\CommentTok{#>     <chr>   <dbl>   <dbl>   <dbl>}
\CommentTok{#> 1    year      NA -0.0699  0.2907}
\CommentTok{#> 2  rating -0.0699      NA -0.0142}
\CommentTok{#> 3  budget  0.2907 -0.0142      NA}
\end{Highlighting}
\end{Shaded}

Offenbar wurden die Filme im Lauf der Zeit nicht besser beurteilt: Die
Korrelation von \texttt{year} und \texttt{rating} ist praktisch Null.
Wohl wurden sie aber teurer: Die Korrelation von \texttt{year} und
\texttt{budget} ist substanziell.

\chapter{Daten visualisieren}\label{daten-visualisieren}

\begin{center}\includegraphics[width=0.3\linewidth]{images/FOM} \end{center}

\begin{center}\includegraphics[width=0.1\linewidth]{images/licence} \end{center}

\BeginKnitrBlock{rmdcaution}
Lernziele:

\begin{itemize}
\tightlist
\item
  An einem Beispiel erläutern können, warum/ wann ein Bild mehr sagt,
  als 1000 Worte.
\item
  Häufige Arten von Diagrammen erstellen können.
\item
  Diagramme bestimmten Zwecken zuordnen können.
\end{itemize}
\EndKnitrBlock{rmdcaution}

In diesem Kapitel werden folgende Pakete benötigt:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)  }\CommentTok{# Zum Plotten}
\KeywordTok{library}\NormalTok{(ggplot2movies)  }\CommentTok{# Daten 'movies'}
\CommentTok{# library(prada)  # optional: Daten 'wo_men', 'stats'test'}
\CommentTok{# library(AER)  # optional: Daten 'Affairs'}
\CommentTok{# library(okcupiddata)  # optional: Daten 'profiles'}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{images/visualisieren/Visualisieren} \end{center}

Dieses Kapitel erläutert das Daten visualisieren anhand des R-Pakets
\texttt{ggplot2}.

\section{Ein Bild sagt mehr als 1000
Worte}\label{ein-bild-sagt-mehr-als-1000-worte}

Ein Bild sagt bekanntlich mehr als 1000 Worte. Schauen wir uns zur
Verdeutlichung das berühmte Beispiel von Anscombe\footnote{\url{https://de.wikipedia.org/wiki/Anscombe-Quartett}}
an. Es geht hier um vier Datensätze mit zwei Variablen (Spalten; X und
Y). Offenbar sind die Datensätze praktisch identisch: Alle X haben den
gleichen Mittelwert und die gleiche Varianz; dasselbe gilt für die Y.
Die Korrelation zwischen X und Y ist in allen vier Datensätzen gleich.
Allerdings erzählt eine Visualisierung der vier Datensätze eine ganz
andere Geschichte.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/visualisieren/anscombe} 

}

\caption{Das Anscombe-Quartett}\label{fig:fig-anscombe}
\end{figure}

Offenbar ``passieren'' in den vier Datensätzen gänzlich unterschiedliche
Dinge. Dies haben die Statistiken nicht aufgedeckt; erst die
Visualisierung erhellte uns\ldots{} Kurz: Die Visualisierung ist ein
unverzichtbares Werkzeug, um zu verstehen, was in einem Datensatz (und
damit in der zugrunde liegenden ``Natur'') passiert.

Eine coole Variante mit der gleichen Botschaft findet sich
\href{https://www.autodeskresearch.com/publications/samestats}{hier}
bzw. mit einer Animation
\href{https://d2f99xq7vri1nk.cloudfront.net/DinoSequentialSmaller.gif}{hier};
vgl. Matejka und Fitzmaurice
(\protect\hyperlink{ref-matejka2017same}{2017}).

Es gibt viele Möglichkeiten, Daten zu visualisieren (in R). Wir werden
uns hier auf einen Weg bzw. ein Paket konzentrieren, der komfortabel,
aber mächtig ist und gut zum Prinzip des Durchpfeifens passt:
\texttt{ggplot2}\footnote{``gg'' steht für ``grammer of graphics'' nach
  einem Buch von
  Wilkinson(\protect\hyperlink{ref-wilkinson2006grammar}{2006});
  ``plot'' steht für ``to plot'', also ein Diagramm erstellen
  (``plotten''); vgl. \url{https://en.wikipedia.org/wiki/Ggplot2}}.

\section{Die Anatomie eines
Diagramms}\label{die-anatomie-eines-diagramms}

\texttt{ggplot2} unterscheidet folgende Bestandteile (``Anatomie'')
eines Diagramms (vgl. Abb. \ref{fig:fig-anatomie}):

\begin{itemize}
\tightlist
\item
  Daten
\item
  Abbildende Aspekte (Achsen, Farben, \ldots{})
\item
  Geome (statistische Bilder wie Punkte, Linien, Boxplots, \ldots{})
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/visualisieren/anatomie_diagramm_crop} 

}

\caption{Anatomie eines Diagramms}\label{fig:fig-anatomie}
\end{figure}

Bei \emph{Daten} muss ein Dataframe angegeben werden. Zu den
\emph{abbildenden Aspekte} (in \texttt{ggplot2} als ``aesthetics'' bzw.
\texttt{aes} bezeichnet) zählen vor allem die Achsen, aber auch Farben
u.a. Was ist mit abbildend gemeint? Weist man einer Achse einen Variable
zu, so wird jede Ausprägung der Variablen einer Ausprägung der Achse
zugeordnet (welcher Wert genau entscheidet \texttt{ggplot2} für uns,
wenn wir es nicht explizieren). Mit \emph{Geom} ist das eigentlich Art
von ``Bild'' gemeint, wie Punkt, Linie oder Boxplot (vgl. Abschnitt
\ref{geome}).

\begin{quote}
Erstellt \texttt{ggplot2} ein Diagramm, so ordnet es Spalten den
Bestandteilen des zu erzeugenden Diagramms zu (auch ``mapping''
genannt).
\end{quote}

\section{\texorpdfstring{Einstieg in \texttt{ggplot2} -
\texttt{qplot}}{Einstieg in ggplot2 - qplot}}\label{einstieg-in-ggplot2---qplot}

Los geht's! Laden wir zuerst den Datensatz \texttt{movies}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(movies, }\DataTypeTok{package =} \StringTok{"ggplot2movies"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Betrachten Sie zum Einstieg das Diagramm \ref{fig:fig-movies}.

\BeginKnitrBlock{rmdexercises}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Welche Variable steht auf der X-Achse?
\item
  Welche Variable steht auf der Y-Achse?
\item
  Was wird gemalt? Linien, Boxplots, Punkte?
\item
  Wie heißt der Datensatz, aus dem die Daten gezogen werden?

  \EndKnitrBlock{rmdexercises}
\end{enumerate}

Der Befehl, der dieses Diagramm erzeugte, heißt \texttt{qplot}. Es ist
ziemlich genau die Antwort auf die Übungsfragen von gerade eben:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ year, }
      \DataTypeTok{y =}\NormalTok{ budget, }
      \DataTypeTok{geom =} \StringTok{"point"}\NormalTok{, }
      \DataTypeTok{data =}\NormalTok{ movies)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{050_Daten_visualisieren_files/figure-latex/fig-movies-1} 

}

\caption{Mittleres Budget pro Jahr}\label{fig:fig-movies}
\end{figure}

Schauen wir uns den Befehl \texttt{qplot} etwas näher an. Wie ist er
aufgebaut?

\BeginKnitrBlock{rmdpseudocode}
\texttt{qplot}: Erstelle schnell (q wie quick in \texttt{qplot}) mal
einen Plot (engl. ``plot'': Diagramm).\\
\texttt{x}: Der X-Achse soll die Variable ``year'' zugeordnet werden.\\
\texttt{y}: Der Y-Achse soll die Variable ``budget'' zugeorndet
werden.\\
\texttt{geom}: (``geometriches Objekt'') Gemalt werden sollen Punkte und
zwar pro Beobachtung (hier: Film) ein Punkt; nicht etwa Linien oder
Boxplots. \texttt{data}: Als Datensatz bitte \texttt{movies} verwenden.
\EndKnitrBlock{rmdpseudocode}

Offenbar geht die Schwere in den Budgets auseinander; außerdem scheint
das Budget größer zu werden. Genau kann man es aber schlecht erkennen in
diesem Diagramm. Besser ist es vielleicht die Daten pro Jahr
zusammenzufassen in einem Geom und dann diese Geome zu vergleichen:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{factor}\NormalTok{(year), }
      \DataTypeTok{y =}\NormalTok{ budget, }
      \DataTypeTok{geom =} \StringTok{"boxplot"}\NormalTok{, }
      \DataTypeTok{data =}\NormalTok{ movies)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{050_Daten_visualisieren_files/figure-latex/unnamed-chunk-5-1} \end{center}

Übrigens: \texttt{factor(year)} wird benötigt, um aus \texttt{year} eine
nominalskalierte Variable zu machen. Nur bei nominalskalierten Variablen
auf der X-Achse zeichnet \texttt{qplot} mehrere Boxplots nebeneinander.
\texttt{qplot} bzw. \texttt{ggplot2} denkt sich: ``Hey, nur wenn es
mehrere Gruppen gibt, macht es Sinn, die Gruppen anhand von Boxplots zu
vergleichen. Also brauchst du eine Gruppierungsvariable - Faktor oder
Text - auf der X-Achse!''.

Es sind zu viele Jahre, das macht das Diagramm unübersichtlich. Besser
wäre es, Jahrzehnte dazustellen. Ein Jahrzehnt ist so etwas wie eine
Jahreszahl, von der die letzte Ziffer abgeschnitten (d.h. durch 10
teilen und runden) und dann durch eine Null ersetzt wurde (d.h. mit 10
multiplizieren):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{movies }\OperatorTok{%>%}\StringTok{  }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Jahrzehnt =}\NormalTok{ year }\OperatorTok{/}\StringTok{ }\DecValTok{10}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Jahrzehnt =} \KeywordTok{trunc}\NormalTok{(Jahrzehnt)) }\OperatorTok{%>%}\StringTok{  }\CommentTok{# trunkieren, abrunden}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Jahrzehnt =}\NormalTok{ Jahrzehnt }\OperatorTok{*}\StringTok{ }\DecValTok{10}\NormalTok{) ->}\StringTok{ }\NormalTok{movies}
\end{Highlighting}
\end{Shaded}

Schauen Sie sich die ersten Werte von \texttt{Jahrzehnt} mal an:
\texttt{movies\ \%\textgreater{}\%\ select(Jahrzehnt)\ \%\textgreater{}\%\ head}.

Ok, auf ein neues Bild (Abb. \ref{fig:fig-movies-jahrzehnt}):

\begin{Shaded}
\begin{Highlighting}[]

\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{factor}\NormalTok{(Jahrzehnt), }
      \DataTypeTok{y =}\NormalTok{ budget, }
      \DataTypeTok{geom =} \StringTok{"boxplot"}\NormalTok{, }
      \DataTypeTok{data =}\NormalTok{ movies) }
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{050_Daten_visualisieren_files/figure-latex/fig-movies-jahrzehnt-1} 

}

\caption{Film-Budgets über die die Jahrzehnte}\label{fig:fig-movies-jahrzehnt}
\end{figure}

Aha, gut. Interessanterweise sanken die Budgets gegen Ende unserer
Datenreihe; das ist aber vielleicht nur ein Zufallsrauschen.

``q'' in \texttt{qplot} steht für ``quick''. Tatsächlich hat
\texttt{qplot} einen großen Bruder, \texttt{ggplot}\footnote{Achtung:
  Nicht \texttt{qqplot}, nicht \texttt{ggplot2}, nicht
  \texttt{gplot}\ldots{}}, der deutlich mehr Funktionen aufweist - und
daher auch die umfangreichere (komplexere) Syntax. Fangen wir mit
\texttt{qplot} an.

Diese Syntax des letzten Beispiels ist recht einfach, nämlich:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qplot}\NormalTok{ (}\DataTypeTok{x =}\NormalTok{ X_Achse, }
       \DataTypeTok{y =}\NormalTok{ Y_Achse, }
       \DataTypeTok{data =}\NormalTok{ mein_dataframe, }
       \DataTypeTok{geom =} \StringTok{"ein_geom"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Wir definieren mit \texttt{x}, welche Variable der X-Achse des Diagramms
zugewiesen werden soll, z.B. \texttt{month}; analog mit Y-Achse. Mit
\texttt{data} sagen wir, in welchem Dataframe die Spalten ``wohnen'' und
als ``geom'' ist die Art des statistischen ``\emph{geom}etrischen
Objects'' gemeint, also Punkte, Linien, Boxplots, Balken\ldots{}

\section{Häufige Arten von
Diagrammen}\label{haufige-arten-von-diagrammen}

Unter den vielen Arten von Diagrammen und vielen Arten, diese zu
klassifizieren greifen wir uns ein paar häufige Diagramme heraus und
schauen uns diese der Reihe nach an.

\subsection{Eine kontinuierliche
Variable}\label{eine-kontinuierliche-variable}

Schauen wir uns die Verteilung von Filmbudgets aus \texttt{movies} an
(s. Abb. \ref{fig:fig-budget-movies}).

\begin{Shaded}
\begin{Highlighting}[]

\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ budget, }\DataTypeTok{data =}\NormalTok{ movies)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{050_Daten_visualisieren_files/figure-latex/fig-budget-movies-1} 

}

\caption{Verteilung des Budgets von Filmen}\label{fig:fig-budget-movies}
\end{figure}

Weisen wir nur der X-Achse (aber nicht der Y-Achse) eine kontinuierliche
Variable zu, so wählt \texttt{ggplot2} automatisch als Geom automatisch
ein Histogramm; wir müssen daher nicht explizieren, dass wir ein
Histogramm als Geom wünschen (aber wir könnten es hinzufügen).

\BeginKnitrBlock{rmdcaution}
Was heißt das kleine `e', das man bei wissenschaftlichen Zahlen hin und
wieder sieht (wie im Diagramm \ref{fig:fig-budget-movies})?

Zum Beispiel: \texttt{5.0e+07}. Das \(e\) sagt, wie viele Stellen im
Exponenten (zur Basis 10) stehen: hier \(10^{07}\). Eine große Zahl -
eine \(1\) gefolgt von \emph{sieben} Nullern: 10000000. Die schöne Zahl
soll noch mit 5 multipliziert werden: also 50000000. Bei so vielen
Nullern kann man schon mal ein Flimmern vor den Augen bekommen\ldots{}
Daher ist die ``wissenschaftliche'' Notation ganz praktisch, wenn die
Zahlen sehr groß (oder sehr klein) werden. Sehr kleine Zahlen werden mit
dieser Notation so dargestellt: \texttt{5.0e-07} heißt
\(frac{1}{10^7}\). Eine Zahl sehr nahe bei Null. Das Minuszeichen zeigt
hier, dass wir den Kehrwert der großen Zahl nehmen sollen.
\EndKnitrBlock{rmdcaution} Alternativ wäre ein Dichtediagramm hier von
Interesse:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ budget, }\DataTypeTok{data =}\NormalTok{ movies, }\DataTypeTok{geom =} \StringTok{"density"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{050_Daten_visualisieren_files/figure-latex/unnamed-chunk-9-1} \end{center}

Was man sich merken muss, ist, dass hier nur das Geom mit
Anführungsstrichen zu benennen ist, die übrigen Parameter \emph{ohne}.

Vielleicht wäre es noch schön, beide Geome zu kombinieren in einem
Diagramm. Das ist etwas komplizierter; wir müssen zum großen Bruder
\texttt{ggplot} umsteigen, da \texttt{qplot} nicht diese Funktionen
anbietet.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ movies) }\OperatorTok{+}
\StringTok{  }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ budget) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ ..density..), }\DataTypeTok{alpha =}\NormalTok{ .}\DecValTok{7}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_density}\NormalTok{(}\DataTypeTok{color =} \StringTok{"blue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{050_Daten_visualisieren_files/figure-latex/unnamed-chunk-10-1} \end{center}

Zuerst haben wir mit dem Parameter \texttt{data} den Dataframe benannt.
\texttt{aes} definiert, welche Variablen welchen Achsen (oder auch z.B.
Füllfarben) zugewiesen werden. Hier sagen wir, dass die Schuhgröße auf
X-Achse stehen soll. Das \texttt{+}-Zeichen trennt die einzelnen
Bestandteile des \texttt{ggplot}-Aufrufs voneinander. Als nächstes sagen
wir, dass wir gerne ein Histogram hätten: \texttt{geom\_histogram}.
Dabei soll aber nicht wie gewöhnlich auf der X-Achse die Häufigkeit
stehen, sondern die Dichte. \texttt{ggplot} berechnet selbständig die
Dichte und nennt diese Variable \texttt{..density..}; die vielen Punkte
sollen wohl klar machen, dass es sich nicht um eine ``normale'' Variable
aus dem eigenen Datenframe handelt, sondern um eine ``interne'' Variable
von \texttt{ggplot} - die wir aber nichtsdestotrotz verwenden können.
\texttt{alpha} bestimmt die ``Durchsichtigkeit'' eines Geoms; spielen
Sie mal etwas damit herum. Schließlich malen wir noch ein blaues
Dichtediagramm \emph{über} das Histogramm.

Wünsche sind ein Fass ohne Boden\ldots{} Wäre es nicht interessant,
einzelne Zeiträume (Jahrzehnte) zu vergleichen? Schauen wir uns die
letzten Jahrzehnte im Vergleich an.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{movies2 <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(movies, Jahrzehnt }\OperatorTok{>}\StringTok{ }\DecValTok{1980}\NormalTok{)}

\NormalTok{movies2 }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Jahrzehnt =} \KeywordTok{factor}\NormalTok{(.}\OperatorTok{$}\NormalTok{Jahrzehnt)) ->}\StringTok{ }\NormalTok{movies2}

\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ budget, }
      \DataTypeTok{data =}\NormalTok{ movies2, }
      \DataTypeTok{geom =} \StringTok{"density"}\NormalTok{, }
      \DataTypeTok{fill =}\NormalTok{ Jahrzehnt, }
      \DataTypeTok{alpha =} \KeywordTok{I}\NormalTok{(.}\DecValTok{7}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{050_Daten_visualisieren_files/figure-latex/unnamed-chunk-11-1} \end{center}

\texttt{qplot} erwartet immer \emph{Variablen} als Parameter; wollen wir
mal keine Variable, sondern eine fixen Wert, wie 0.7, übergeben, so
können wir das mit dem Befehl \texttt{I} (wie ``identity'') tun.

Hier sollten vielleicht noch die Extremwerte entfernt werden, um den
Blick auf das Gros der Werte nicht zu verstellen:

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{movies2 }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(budget }\OperatorTok{<}\StringTok{ }\FloatTok{1e08}\NormalTok{) ->}\StringTok{ }\NormalTok{movies2}

\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ budget, }
      \DataTypeTok{data =}\NormalTok{ movies2,      }
      \DataTypeTok{geom =} \StringTok{"density"}\NormalTok{, }
      \DataTypeTok{fill =}\NormalTok{ Jahrzehnt, }
      \DataTypeTok{alpha =} \KeywordTok{I}\NormalTok{(.}\DecValTok{7}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{050_Daten_visualisieren_files/figure-latex/unnamed-chunk-12-1} \end{center}

Besser. Man kann das Durchpfeifen auch bis zu \texttt{qplot}
weiterführen:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{movies }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(budget }\OperatorTok{<}\StringTok{ }\FloatTok{1e+08}\NormalTok{, Jahrzehnt }\OperatorTok{>=}\StringTok{ }\DecValTok{1990}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Jahrzehnt =} \KeywordTok{factor}\NormalTok{(Jahrzehnt)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ budget, }\DataTypeTok{data =}\NormalTok{ ., }\DataTypeTok{geom =} \StringTok{"density"}\NormalTok{, }
        \DataTypeTok{fill =}\NormalTok{ Jahrzehnt, }\DataTypeTok{alpha =} \KeywordTok{I}\NormalTok{(.}\DecValTok{7}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{050_Daten_visualisieren_files/figure-latex/unnamed-chunk-13-1} \end{center}

Die Pfeife versucht im Standard, das Endprodukt des letzten
Arbeitsschritts an den \emph{ersten} Parameter des nächsten Befehls
weiterzugeben. Ein kurzer Blick in die Hilfe von \texttt{qplot} zeigt,
dass der erste Parameter nicht \texttt{data} ist, sondern \texttt{x}.
Daher müssen wir explizit sagen, an welchen Parameter wir das Endprodukt
des letzen Arbeitsschritts geben wollen. Netterweise müssen wir dafür
nicht viel tippen: Mit einem schlichten Punkt \texttt{.} können wir
sagen ``nimm den Dataframe, so wie er vom letzten Arbeitsschritt
ausgegeben wurde''.

Mit \texttt{fill\ =\ Jahrzehnt} sagen wir \texttt{qplot}, dass er für
jedes Jahrzehnt jeweils ein Dichtediagramm erzeugen soll; jedem
Dichtediagramm wird dabei eine Farbe zugewiesen (die uns
\texttt{ggplot2} im Standard voraussucht). Mit anderen Worten: Die Werte
von \texttt{Jahrzehnt} werden der \emph{Füllfarbe} der Histogramme
zugeordnet. Anstelle der Füllfarbe hätten wir auch die Linienfarbe
verwenden können; die Syntax wäre dann: \texttt{color\ =\ sex}. Man
beachte, dass die Variable für \texttt{fill} oder \texttt{color} eine
nominale Variable (\texttt{factor} oder \texttt{character}) sein muss,
damit \texttt{ggplot2} tut, was will wollen.

\subsection{Zwei kontinuierliche
Variablen}\label{zwei-kontinuierliche-variablen}

Ein Streudiagramm ist die klassische Art, zwei metrische Variablen
darzustellen. Das ist mit \texttt{qplot} einfach:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p <-}\StringTok{ }\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ budget, }\DataTypeTok{y =}\NormalTok{ rating, }\DataTypeTok{data =}\NormalTok{ movies2)}
\NormalTok{p}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{050_Daten_visualisieren_files/figure-latex/unnamed-chunk-14-1} \end{center}

Wir weisen wieder der X-Achse und der Y-Achse eine Variable zu; handelt
es sich in beiden Fällen um Zahlen, so wählt \texttt{ggplot2}
automatisch ein Streudiagramm - d.h. Punkte als Geom
(\texttt{geom\ =\ "point"}).

Es ist nicht wirklich ein Trend erkennbar: Teuere Filme sind nicht
unbedingt beliebter bzw. besser bewertet. Zeichnen wir eine Trendgerade
ein.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{050_Daten_visualisieren_files/figure-latex/unnamed-chunk-15-1} \end{center}

Synonym könnten wir auch schreiben:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wo_men }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(height }\OperatorTok{>}\StringTok{ }\DecValTok{150}\NormalTok{, height }\OperatorTok{<}\StringTok{ }\DecValTok{210}\NormalTok{, shoe_size }\OperatorTok{<}\StringTok{ }\DecValTok{55}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ height, }\DataTypeTok{y =}\NormalTok{ shoe_size) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Da \texttt{ggplot} als \emph{ersten} Parameter die Daten erwartet, kann
die Pfeife hier problemlos durchgereicht werden. \emph{Innerhalb} eines
\texttt{ggplot}-Aufrufs werden die einzelne Teile durch ein Pluszeichen
\texttt{+} voneinander getrennt. Nachdem wir den Dataframe benannt
haben, definieren wir die Zuweisung der Variablen zu den Achsen mit
\texttt{aes} (``aes'' wie ``aesthetics'', also das ``Sichtbare'' eines
Diagramms, die Achsen etc., werden definiert). Ein ``Smooth-Geom'' ist
eine Linie, die sich schön an die Punkte anschmiegt, in diesem Falls als
Gerade (lineares Modell, \texttt{lm}).

Bei sehr großen Datensätze, sind Punkte unpraktisch, da sie sich
überdecken (``overplotting''). Ein Abhilfe ist es, die Punkte nur
``schwach'' zu färben. Dazu stellt man die ``Füllstärke'' der Punkte
über \texttt{alpha} ein: \texttt{geom\_point(alpha\ =\ 1/100)}. Um einen
passablen Alpha-Wert zu finden, bedarf es häufig etwas Probierens. Zu
beachten ist, dass es mitunter recht lange dauert, wenn \texttt{ggplot}
viele (\textgreater{}100.000) Punkte malen soll.

Probieren Sie auch Folgendes aus: Fügen Sie bei \texttt{aes} den
Parameter \texttt{color\ =\ sex} hinzu.

Bei noch größeren Datenmengen bietet sich an, den Scatterplot als
``Schachbrett'' aufzufassen, und das Raster einzufärben, je nach Anzahl
der Punkte pro Schachfeld; zwei Geome dafür sind \texttt{geom\_hex()}
und \texttt{geom\_bin2d()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{nrow}\NormalTok{(movies)  }\CommentTok{# groß, ein bisschen wenigstens}
\CommentTok{#> [1] 58788}

\KeywordTok{ggplot}\NormalTok{(movies) }\OperatorTok{+}
\StringTok{  }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ year, }\DataTypeTok{y =}\NormalTok{ budget) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_hex}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{050_Daten_visualisieren_files/figure-latex/flights_hexbin-1} \end{center}

Wenn man dies verdaut hat, wächst der Hunger nach einer Aufteilung in
Gruppen.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wo_men }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{filter}\NormalTok{(height }\OperatorTok{>}\StringTok{ }\DecValTok{150}\NormalTok{, height }\OperatorTok{<}\StringTok{ }\DecValTok{210}\NormalTok{, shoe_size }\OperatorTok{<}\StringTok{ }\DecValTok{55}\NormalTok{) ->}\StringTok{ }\NormalTok{wo_men2}

\NormalTok{wo_men2 }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ height, }\DataTypeTok{y =}\NormalTok{ shoe_size, }\DataTypeTok{color =}\NormalTok{ sex, }\DataTypeTok{data =}\NormalTok{ .)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{050_Daten_visualisieren_files/figure-latex/fig-aes-color-1} \end{center}

Mit \texttt{color\ =\ sex} sagen wir, dass die Linienfarbe (der Punkte)
entsprechend der Stufen von \texttt{sex} eingefärbt werden sollen. Die
genaue Farbwahl übernimmt \texttt{ggplot2} für uns.

Alternativ kann man auch zwei ``Teil-Bildchen'' (``facets'') erstellen,
eines für Frauen und eines für Männer:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wo_men }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{filter}\NormalTok{(height }\OperatorTok{>}\StringTok{ }\DecValTok{150}\NormalTok{, height }\OperatorTok{<}\StringTok{ }\DecValTok{210}\NormalTok{, shoe_size }\OperatorTok{<}\StringTok{ }\DecValTok{55}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ height, }\DataTypeTok{y =}\NormalTok{ shoe_size, }\DataTypeTok{facets =} \StringTok{"~sex"}\NormalTok{, }\DataTypeTok{color =}\NormalTok{ sex, }\DataTypeTok{data =}\NormalTok{ .)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{050_Daten_visualisieren_files/figure-latex/unnamed-chunk-17-1} \end{center}

Man beachte die Tilde \texttt{\textasciitilde{}}, die vor die
``Gruppierungsvariable'' \texttt{sex} zu setzen ist.

\subsection{Eine nominale Variable}\label{eine-nominale-variable}

Bei nominalen Variablen, geht es in der Regel darum, Häufigkeiten
auszuzählen. Ein Klassiker: Wie viele Männer und Frauen finden sich in
dem Datensatz? Wie viele Studenten haben (nicht) bestanden?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"data/stats_test.csv"}\NormalTok{)}
\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ bestanden, }\DataTypeTok{data =}\NormalTok{ stats_test)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{050_Daten_visualisieren_files/figure-latex/unnamed-chunk-18-1} \end{center}

Falls nur die X-Achse definiert ist und dort eine Faktorvariable oder
eine Textvariable steht, dann nimmt \texttt{qplot} automatisch ein
Balkendiagramm als Geom (es steht uns frei, trotzdem
\texttt{geom\ =\ bar} anzugeben).

Wir könnten uns jetzt die Frage stellen, wie viele Nicht-Interessierte
und Hoch-Interessierte es in der Gruppe, die bestanden hat
(\texttt{bestanden\ ==\ "yes"}) gibt; entsprechend für die Gruppe, die
nicht bestanden hat.

\begin{Shaded}
\begin{Highlighting}[]

  
\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ bestanden, }\DataTypeTok{fill =} \KeywordTok{factor}\NormalTok{(interest), }\DataTypeTok{data =}\NormalTok{ stats_test)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{050_Daten_visualisieren_files/figure-latex/unnamed-chunk-19-1} \end{center}

Hier haben wir \texttt{qplot} gesagt, dass der die Balken entsprechend
der Häufigkeit von \texttt{interest} füllen soll. Damit \texttt{qplot}
(und \texttt{ggplot}) sich bequemt, die Füllung umzusetzen, müssen wir
aus \texttt{interet} eine nominalskalierte Variablen machen -
\texttt{factor} macht das für uns.

Schön wäre noch, wenn die Balken Anteile (Prozentwerte) angeben würden.
Das geht mit \texttt{qplot} (so) nicht; wir schwenken auf
\texttt{ggplot} um. Und, um die Story zuzuspitzen, schauen wir uns nur
die Extremwerte von \texttt{interest} an.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(interest }\OperatorTok{==}\StringTok{ }\DecValTok{1} \OperatorTok{|}\StringTok{ }\NormalTok{interest }\OperatorTok{==}\StringTok{ }\DecValTok{6}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ bestanden, }\DataTypeTok{fill =} \KeywordTok{factor}\NormalTok{(interest)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{position =} \StringTok{"fill"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{050_Daten_visualisieren_files/figure-latex/unnamed-chunk-20-1} \end{center}

Der Lehrer freut sich: In der Gruppe, die bestanden hat, ist der Anteil
der \sout{freaks} Hoch-Interessierten größer als bei den Durchfallern.

Schauen wir uns die Struktur des Befehls \texttt{ggplot} näher an.

\BeginKnitrBlock{rmdpseudocode}
\texttt{stats\_test}: Hey R, nimm den Datensatz \texttt{stats\_test} UND
DANN\ldots{} \texttt{ggplot()} : Hey R, male ein Diagramm von Typ
\texttt{ggplo}t (mit dem Datensatz aus dem vorherigen Pfeifen-Schritt,
d.h. aus der vorherigen Zeile, also \texttt{stats\_test})!\\
\texttt{filter}: wir wollen nur Zeilen (Studenten), für die gilt
\texttt{interest\ ==\ 1} oder \texttt{interest\ ==\ 6}. Der horizontale
Strich heißt `oder'.\\
\texttt{+}: Das Pluszeichen grenzt die Teile eines ggplot-Befehls
voneinander ab.\\
\texttt{aes}: von ``aethetics'', also welche Variablen des Datensatzes
den sichtbaren Aspekten (v.a. Achsen, Farben) zugeordnet werden.\\
\texttt{x}: Der X-Achse (Achtung, \texttt{x} wird klein geschrieben
hier) wird die Variable \texttt{bestanden} zugeordnet.\\
\texttt{y}: gibt es nicht??? Wenn in einem ggplot-Diagramm \emph{keine}
Y-Achse definiert wird, wird ggplot automatisch ein Histogramm bzw. ein
Balkendiagramm erstellen. Bei diesen Arten von Diagrammen steht auf der
Y-Achse keine eigene Variable, sondern meist die Häufigkeit des
entsprechenden X-Werts (oder eine Funktion der Häufigkeit, wie relative
Häufigkeit).\\
\texttt{fill} Das Diagramm (die Balken) sollen so gefüllt werden, dass
sich die Häufigkeit der Werte von \texttt{interest} darin widerspiegelt.
\texttt{geom\_XYZ}: Als ``Geom'' soll ein Balken (``bar'') gezeichnet
werden. Ein Geom ist in ggplot2 das zu zeichnende Objekt, also ein
Boxplot, ein Balken, Punkte, Linien etc. Entsprechend wird gewünschte
Geom mit \texttt{geom\_bar}, \texttt{geom\_boxplot},
geom\_point\texttt{etc.\ gewählt.}position =
fill\texttt{:}position\_fill\texttt{will\ sagen,\ dass\ die\ Balken\ alle\ eine\ Höhe\ von\ 100\%\ (1)\ haben,\ d.h.\ gleich\ hoch\ sind.\ Die\ Balken\ zeigen\ also\ nur\ die\ Anteile\ der\ Werte\ der}fill`-Variablen.
\EndKnitrBlock{rmdpseudocode}

Die einzige Änderung in den Parametern ist \texttt{position\ =\ "fill"}.
Dieser Parameter weist \texttt{ggplot} an, die Positionierung der Balken
auf die Darstellung von Anteilen auszulegen. Damit haben alle Balken die
gleiche Höhe, nämlich 100\% (1). Aber die ``Füllung'' der Balken
schwankt je nach der Häufigkeit der Werte von \texttt{groesse\_gruppe}
pro Balken (d.h. pro Wert von \texttt{sex}).

Wir sehen, dass die Anteile von großen bzw. kleinen Menschen bei den
beiden Gruppen (Frauen vs.~Männer) \emph{unterschiedlich hoch} ist. Dies
spricht für einen \emph{Zusammenhang} der beiden Variablen; man sagt,
die Variablen sind \emph{abhängig} (im statistischen Sinne).

\begin{quote}
Je unterschiedlicher die ``Füllhöhe'', desto stärker sind die Variablen
(X-Achse vs.~Füllfarbe) voneinander abhängig (bzw. desto stärker der
Zusammenhang).
\end{quote}

\subsection{Zwei nominale Variablen}\label{zwei-nominale-variablen}

Arbeitet man mit nominalen Variablen, so sind Kontingenztabellen Täglich
Brot. Z.B.: Welche Produkte wurden wie häufig an welchem Standort
verkauft? Wie viele Narzissten gibt es in welcher Management-Stufe? Wie
ist die Verteilung von Alkoholkonsum und Körperform bei Menschen einer
Single-Börse?. Bleiben wir bei letztem Beispiel.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(profiles, }\DataTypeTok{package =} \StringTok{"okcupiddata"}\NormalTok{)}

\NormalTok{profiles }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{count}\NormalTok{(drinks, body_type) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{ggplot }\OperatorTok{+}
\StringTok{  }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ drinks, }\DataTypeTok{y =}\NormalTok{ body_type, }\DataTypeTok{fill =}\NormalTok{ n) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_tile}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{axis.text.x =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{angle =} \DecValTok{90}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{050_Daten_visualisieren_files/figure-latex/unnamed-chunk-21-1} \end{center}

Was haben wir gemacht? Also:

\BeginKnitrBlock{rmdpseudocode}
Nehme den Datensatz ``profiles'' UND DANN\\
Zähle die Kombinationen von ``drinks'' und ``body\_type'' UND DANN\\
Erstelle ein ggplot-Plot UND DANN\\
Weise der X-Achse ``drinks'' zu, der Y-Achse ``body\_type'' und der
Füllfarbe ``n'' UND DANN\\
Male Fliesen UND DANN\\
Passe das Thema so an, dass der Winkel für Text der X-Achse auf 90 Grad
steht.
\EndKnitrBlock{rmdpseudocode}

Diese Art von Diagramm nennt man auch `Mosaicplot', weil es an ein
Mosaic erinnert (wer hätt's gedacht).

Was sofort ins Auge sticht, ist dass ``soziales Trinken'', nennen wir es
mal so, am häufigsten ist, unabhängig von der Körperform. Ansonsten
scheinen die Zusammenhäng nicht sehr stark zu sein.

\subsection{Zusammenfassungen zeigen}\label{zusammenfassungen-zeigen}

Manchmal möchten wir \emph{nicht} die Rohwerte einer Variablen
darstellen, sondern z.B. die Mittelwerte pro Gruppe. Mittelwerte sind
eine bestimmte \emph{Zusammenfassung} einer Spalte; also fassen wir
zuerst die Körpergröße zum Mittelwert zusammen - gruppiert nach
Geschlecht.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(bestanden) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{interest_mw =} \KeywordTok{mean}\NormalTok{(interest, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)) ->}\StringTok{ }\NormalTok{stats_test_summary}

\NormalTok{stats_test_summary}
\CommentTok{#> # A tibble: 2 x 2}
\CommentTok{#>   bestanden interest_mw}
\CommentTok{#>      <fctr>       <dbl>}
\CommentTok{#> 1        ja        3.26}
\CommentTok{#> 2      nein        2.97}
\end{Highlighting}
\end{Shaded}

Diese Tabelle schieben wir jetzt in \texttt{ggplot2}; natürlich hätten
wir das gleich in einem Rutsch durchpfeifen können.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test_summary }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ bestanden, }\DataTypeTok{y =}\NormalTok{ interest_mw, }\DataTypeTok{data =}\NormalTok{ .)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{050_Daten_visualisieren_files/figure-latex/unnamed-chunk-23-1} \end{center}

Das Diagramm besticht nicht durch die Tiefe und Detaillierung.
Bereichern wir das Diagramm um die Frage, wie viel (jeder Student
gelernt hat (\texttt{study\_time}). Schauen wir uns aber der Einfachheit
halber nur die Studenten an, die ganz viel oder ganz wenig gelernt
haben.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(bestanden, study_time) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{interest_mw =} \KeywordTok{mean}\NormalTok{(interest, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ bestanden, }\DataTypeTok{y =}\NormalTok{ interest_mw, }\DataTypeTok{data =}\NormalTok{ ., }\DataTypeTok{color =} \KeywordTok{factor}\NormalTok{(study_time)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{group =} \KeywordTok{factor}\NormalTok{(study_time)))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{050_Daten_visualisieren_files/figure-latex/unnamed-chunk-24-1} \end{center}

In Pseudosyntax:

\BeginKnitrBlock{rmdpseudocode}
Nehme den Datensatz ``stats\_test'' UND DANN\\
gruppiere nach den Variablen \texttt{bestanden} und \texttt{study\_time}
UND DANN fasse für diese Gruppen jeweils die Spalte \texttt{interest}
zum Mittelwert zusammen UND DANN\\
male einen schnellen Plot mit diesen Daten UND DANN füge ein
Liniendiagramm dazu, wobei jede Stufe von \texttt{study\_time} eine
Gruppe ist. Und Punkte einer Gruppe sollen verbunden werden.
\EndKnitrBlock{rmdpseudocode}

Warum steht der arme pinkfarbene Punkt bei `ja' und \textasciitilde{}4.5
so für sich alleine oder Linie?\footnote{es gibt kein
  \texttt{study\_time\ ==\ 5} bei den Durchfallen, d.h. bei
  \texttt{bestanden\ ==\ nein}.}

Alternativ, und deutlich informationsreicher (besser) sind hier
Boxplots.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ bestanden, }
      \DataTypeTok{y =}\NormalTok{ interest,}
      \DataTypeTok{data =}\NormalTok{ stats_test,}
      \DataTypeTok{geom =} \StringTok{"boxplot"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{050_Daten_visualisieren_files/figure-latex/unnamed-chunk-25-1} \end{center}

Hm, wie Sie sehen, sehen Sie nix. Kein Unterschied im Median zwischen
den Gruppen. Vergleichen wir mal die Punkte zwischen den einzelnen
Interessenstufen.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{factor}\NormalTok{(interest),}
      \DataTypeTok{y =}\NormalTok{ score,}
      \DataTypeTok{data =}\NormalTok{ stats_test,}
      \DataTypeTok{geom =} \StringTok{"boxplot"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{050_Daten_visualisieren_files/figure-latex/unnamed-chunk-26-1} \end{center}

Das \texttt{factor(interest)} brauchen wir, weil \texttt{ggplot2} nur
dann mehrere Boxplots malt, wenn es Gruppen zum Vergleichen (auf der
X-Achse) gibt - sprich wenn auf der X-Achse eine Faktor- oder
Textvariable steht.

\subsection{Überblick zu häufigen
Diagrammtypen}\label{uberblick-zu-haufigen-diagrammtypen}

Die Tabelle \ref{tab:diagrammtypen} und Abbildung
\ref{fig:fig-diagrammtypen} fassen die gerade besprochenen Diagrammtypen
zusammen.

\begin{table}

\caption{\label{tab:diagrammtypen}Häufige Diagrammtypen}
\centering
\begin{tabular}[t]{l|l|l}
\hline
X-Achse & Y-Achse & Diagrammtyp\\
\hline
kontinuierliche Variable & - & Histogramm, Dichtediagramm\\
\hline
kontinuierliche Variable & kontinuierliche Variable & Punkte, Schachbrett-Diagramme\\
\hline
nominale Variable & - & Balkendiagramm\\
\hline
nominale Variable & nominale Variable & Mosaicplot (Fliesendiagramm)\\
\hline
nominale Variable & metrische Variable & Punktediagramm für Zusammenfassungen\\
\hline
nominale Variable & metrische Variable & Boxplots (besser)\\
\hline
\end{tabular}
\end{table}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{050_Daten_visualisieren_files/figure-latex/fig-diagrammtypen-1} 

}

\caption{Überblick zu häufigen Diagrammtypen}\label{fig:fig-diagrammtypen}
\end{figure}

\section{\texorpdfstring{Die Gefühlswelt von
\texttt{ggplot2}}{Die Gefühlswelt von ggplot2}}\label{die-gefuhlswelt-von-ggplot2}

\begin{itemize}
\tightlist
\item
  Geben Sie eine \emph{diskrete X-Achs}e an und \emph{keine Y-Achse}, so
  greift \texttt{qplot} im Standard auf das Geom \texttt{bar} zurück
  (Balkendiagramm), falls Sie \emph{kein} Geom angeben:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ score, }\DataTypeTok{data =}\NormalTok{ stats_test)  }\CommentTok{# identisch zu}
\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ score, }\DataTypeTok{data =}\NormalTok{ stats_test, }\DataTypeTok{geom =} \StringTok{"bar"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Geben Sie eine \emph{kontinuierliche X-Achse} an und \emph{keine
  Y-Achse}, so greift qplot im Standard auf das Geom \texttt{histogram}
  zurück (Histogramm).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ score, }\DataTypeTok{data =}\NormalTok{ stats_test)  }\CommentTok{# identisch zu}
\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ score, }\DataTypeTok{data =}\NormalTok{ stats_test, }\DataTypeTok{geom =} \StringTok{"histogram"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Geben Sie eine \emph{kontinuierliche X-Achse} an und eine
  \emph{kontinuierliche Y-Achse} an, so greift qplot im Standard auf das
  Geom \texttt{point} zurück (Streudiagramm).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ score, }\DataTypeTok{y =}\NormalTok{ self}\OperatorTok{-}\NormalTok{eval, }\DataTypeTok{data =}\NormalTok{ stats_test)  }\CommentTok{# identisch zu}
\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ score, }\DataTypeTok{y=}\NormalTok{  self}\OperatorTok{-}\NormalTok{eval, }\DataTypeTok{data =}\NormalTok{ stats_test, }\DataTypeTok{geom =} \StringTok{"point"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Möchten Sie mehrere Geome für eine Variable darstellen, so muss die
  Gruppierungs-Variable diskret sein:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#oh no: }
\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ rating, }\DataTypeTok{y =}\NormalTok{ affairs, }\DataTypeTok{geom =} \StringTok{"boxplot"}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ Affairs)}

\CommentTok{#oh yes: }
\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{factor}\NormalTok{(rating), }\DataTypeTok{y =}\NormalTok{ affairs, }\DataTypeTok{geom =} \StringTok{"boxplot"}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ Affairs)}

\CommentTok{#oh yes: }
\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ gender, }\DataTypeTok{y =}\NormalTok{ affairs, }\DataTypeTok{geom =} \StringTok{"boxplot"}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ Affairs)}
\end{Highlighting}
\end{Shaded}

\section{Aufgaben}\label{aufgaben-9}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Erzählen Sie einer vertrauenswürdigen Person jeweils eine
  ``Geschichte'', die das Zustandekommen der vier Plots von Anscombe
  (Abb. \ref{fig:fig-anscombe}) erklärt!
\item
  Abb. \ref{fig:fig-movies-jahrzehnt} stellt das mittlerer Budget von
  Filmen dar; als ``Geom'' wird ein Boxplot verwendet. Andere Geome
  wären auch möglich - aber wie sinnvoll wären sie?
\item
  Erstellen Sie ein Diagramm, welches Histogramme der Verspätung
  verwendet anstelle von Boxplots! Damit das Diagramm nicht so groß
  wird, nehmen Sie zur Gruppierung nicht \texttt{carrier} sondern
  \texttt{origin}.
\item
  Ist das Histogramm genauso erfolgreich wie der Boxplot, wenn es darum
  geht, viele Verteilungen vergleichend zu präsentieren? Warum?
\item
  Erstellen Sie ein sehr grobes und ein sehr feines Histogramm für die
  Schuhgröße!
\item
  Vertiefung: Erstellen Sie ein Diagramm, das sowohl eine
  Zusammenfassung (Mittelwert) der Körpergrößen nach Geschlecht
  darstellt als auch die einzelnen Werte darstellt!
\end{enumerate}

\section{Lösungen}\label{losungen}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  :-)
\item
  :
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ budget, }\DataTypeTok{geom =} \StringTok{"histogram"}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ movies, }\DataTypeTok{facets =} \OperatorTok{~}\KeywordTok{factor}\NormalTok{(Jahrzehnt))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{050_Daten_visualisieren_files/figure-latex/movies-histogram-1} 

}

\caption{Film-Budgets mit Histogrammen}\label{fig:movies-histogram}
\end{figure}

Der Boxplot ist besser geeignet als das Histogramm, um mehrere
Verteilungen vergleichend zu präsentieren (vgl. Abb.
\ref{fig:movies-histogram}). Durch die gleiche Ausrichtung der Boxplots
ist es dem Auge viel einfacher, Vergleiche anzustellen im Vergleich zu
den Histogrammen. Einen optisch schöneren Effekt könnte man mit
\texttt{geom\_jitter} anstelle von \texttt{geom\_point}erreichen. Auch
die Reihenfolge der beiden Geome könnte man umdrehen. Natürlich ist auch
an Form, Größe und Farbe der Geome noch zu feilen.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  :
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ shoe_size, }\DataTypeTok{data =}\NormalTok{ wo_men, }\DataTypeTok{bins =} \DecValTok{10}\NormalTok{)}
\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ shoe_size, }\DataTypeTok{data =}\NormalTok{ wo_men, }\DataTypeTok{bins =} \DecValTok{50}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{050_Daten_visualisieren_files/figure-latex/unnamed-chunk-31-1} \includegraphics[width=0.7\linewidth]{050_Daten_visualisieren_files/figure-latex/unnamed-chunk-31-2} \end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  :
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wo_men2 }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(sex) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{height =} \KeywordTok{mean}\NormalTok{(height)) ->}\StringTok{ }\NormalTok{wo_men3}


\NormalTok{wo_men3 }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ sex, }\DataTypeTok{y =}\NormalTok{ height) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{color =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{8}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ wo_men2, }\DataTypeTok{color =} \StringTok{"grey80"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{050_Daten_visualisieren_files/figure-latex/unnamed-chunk-32-1} \end{center}

Der ``Trick'' ist hier, erst die zusammengefassten Daten in ein Geom zu
stecken (\texttt{wo\_men3}). Dann werden die Rohdaten
(\texttt{wo\_men2}) ebenfalls in ein Geom gepackt. Allerdings muss die
Achsen-Beschriftung bei beiden Geomen identisch sein, sonst gibt es eine
Fehlermeldung.

\section[Richtig oder Falsch]{\texorpdfstring{Richtig oder
Falsch\footnote{R, R, F, F, R}}{Richtig oder Falsch}}\label{richtig-oder-falsch}

\BeginKnitrBlock{rmdexercises}
Richtig oder Falsch!?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Diese Geome gehören zum (Standard-) ggplot2: bar, histogram, point,
  density, jitter, boxplot.
\item
  \texttt{qplot} ist eine Funktion im Paket \texttt{ggplot2}.
\item
  Mi \texttt{aes} definiert man, wie ``ästethisch'' das Diagramm sein
  soll (z.B. grauer Hintergrund vs.~weißer Hintergrund, Farbe der Achsen
  etc.).
\item
  Diese Geome gehören zum (Standard-) ggplot2: smooth, line, boxwhisker,
  mosaicplot.
\item
  Möchte man ein Diagramm erstellen, welches auf der X-Achse
  \texttt{total\_bill}, auf der Y-Achse \texttt{tip} darstellt, als Geom
  Punkte verwendet und die Daten aus der Tabelle \texttt{tips} bezieht,
  so ist folgende Syntax korrekt: `qplot(x = total, bill, y = tip, geom
  = ``point'', data = tips)
\end{enumerate}
\EndKnitrBlock{rmdexercises}

\section{Befehlsübersicht}\label{befehlsubersicht-4}

Tabelle \ref{tab:befehle-vis} fasst die R-Funktionen dieses Kapitels
zusammen.

\begin{table}

\caption{\label{tab:befehle-vis}Befehle des Kapitels 'Daten visualisieren'}
\centering
\begin{tabular}[t]{l|l}
\hline
Paket::Funktion & Beschreibung\\
\hline
ggplot2::qplot & Malt schnell mal einen Plot\\
\hline
ggplot2::ggplot & Malt einen Plot\\
\hline
factor & Wandelt einen Vektor in den Typ factor um\\
\hline
\end{tabular}
\end{table}

\section{Vertiefung: Geome bei ggplot2}\label{geome}

Einen guten Überblick über Geome bietet das Cheatsheet von
ggplot2\footnote{\url{https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf}}.

Verschiedenen Taxonomien von statistischen ``Bildchen'' sind denkbar;
eine einfache ist die folgende; es wird nur ein Teil der verfügbaren
Geome dargestellt.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Eine kontinuierliche Variable
\end{enumerate}

\begin{center}\includegraphics[width=0.7\linewidth]{050_Daten_visualisieren_files/figure-latex/unnamed-chunk-33-1} \end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Zwei kontinuierliche Variablen
\end{enumerate}

\begin{center}\includegraphics[width=0.7\linewidth]{050_Daten_visualisieren_files/figure-latex/unnamed-chunk-34-1} \end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Eine diskrete Variable (X-Achse)
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]

\KeywordTok{ggplot}\NormalTok{(wo_men2) }\OperatorTok{+}
\StringTok{  }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ sex) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{050_Daten_visualisieren_files/figure-latex/unnamed-chunk-35-1} \end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Eine diskrete Variable auf der X-Achse und eine kontinuierliche
  Y-Achse
\end{enumerate}

\begin{center}\includegraphics[width=0.7\linewidth]{050_Daten_visualisieren_files/figure-latex/unnamed-chunk-36-1} \end{center}

\section{Verweise}\label{verweise-3}

\begin{itemize}
\item
  Einen Befehlsüberblick zu \texttt{ggplot2} findet sich hier:
  \url{http://ggplot2.tidyverse.org/reference/}.
\item
  Edward Tufte gilt als Grand Seigneur der Datenvisualisierung; er hat
  mehrere lesenswerte Bücher zu dem Thema geschrieben (Tufte
  \protect\hyperlink{ref-1930824130}{2001}; Tufte
  \protect\hyperlink{ref-1930824165}{2006}; Tufte
  \protect\hyperlink{ref-1930824149}{1990}).
\item
  William Cleveland, ein amerikanischer Statistiker ist bekannt für
  seine grundlegenden, und weithin akzeptierten Ansätze für Diagramme,
  die die wesentliche Aussage schnörkellos transportieren (Cleveland
  \protect\hyperlink{ref-Cleveland}{1993}).
\item
  Die (graphische) Auswertung von Umfragedaten basiert häufig auf
  Likert-Skalen. Ob diese metrisches Niveau aufweisen, darf bezweifelt
  werden. Hier findet sich einige vertiefenden Überlegungen dazu und zur
  Frage, wie Likert-Daten ausgewertet werden könnten:
  \url{https://bookdown.org/Rmadillo/likert/}.
\item
  Es finden sich viele Tutorials online zu \texttt{ggplot2}; ein
  deutschsprachiger Tutorial findet sich hier:
  \url{http://md.psych.bio.uni-goettingen.de/mv/unit/ggplot2/ggplot2.html}.
\end{itemize}

\chapter{Fallstudie zur
Visualisierung}\label{fallstudie-zur-visualisierung}

\BeginKnitrBlock{rmdcaution}
Lernziele:

\begin{itemize}
\tightlist
\item
  Diagramme für nominale Variablen erstellen können.
\item
  Balkendiagramme mit Prozentpunkten auf der Y-Achse erstellen können.
\item
  Balkendiagramme drehen können.
\item
  Text-Labels an Balkendiagramme anfügen können.
\item
  Farbschemata von Balkendiagrammen ändern können.
\end{itemize}
\EndKnitrBlock{rmdcaution}

Benötigte Pakete:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(corrr)}
\KeywordTok{library}\NormalTok{(GGally)}
\end{Highlighting}
\end{Shaded}

Eine recht häufige Art von Daten in der Wirtschaft kommen von Umfragen
in der Belegschaft. Diese Daten gilt es dann aufzubereiten und graphisch
wiederzugeben. Das ist der Gegenstand dieser Fallstudie.

\section{Daten einlesen}\label{daten-einlesen}

Hier laden wir einen Datensatz von einer Online-Umfrage:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{extra <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"data/extra.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Alternativ können Sie den Datensatz aus dem Paket \texttt{prada} laden
(mit \texttt{data(extra,\ package\ =\ "prada")}) oder aus dem Internet
herunterladen:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prada_extra_url <-}
\StringTok{  }\KeywordTok{paste0}\NormalTok{(}\StringTok{"https://raw.github.com/"}\NormalTok{,  }\CommentTok{# Webseite}
         \StringTok{"sebastiansauer/"}\NormalTok{,  }\CommentTok{# Nutzer}
         \StringTok{"Praxis_der_Datenanalyse/"}\NormalTok{,  }\CommentTok{# Projekt/Repositorium}
         \StringTok{"master/"}\NormalTok{,  }\CommentTok{# Variante}
         \StringTok{"data/extra.csv"}\NormalTok{)  }\CommentTok{# Ordner und Dateinamen}

\NormalTok{extra <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(prada_extra_url)}
\end{Highlighting}
\end{Shaded}

Der Datensatz besteht aus 10 Extraversionsitems (B5T nach
Satow\footnote{\url{https://www.zpid.de/pub/tests/PT_9006357_B5T_Forschungsbericht.pdf}})
sowie einigen Verhaltenskorrelaten (zumindest angenommenen). Uns
interessieren also hier nur die 10 Extraversionsitems, die zusammen
Extraversion als Persönlichkeitseigenschaft messen (sollen). Wir werden
die Antworte der Befragten darstelle, aber uns hier keine Gedanken über
Messqualität u.a. machen.

Die Umfrage kann hier\footnote{\url{https://docs.google.com/forms/d/e/1FAIpQLSfD4wQuhDV_edx1WBfN3Qos7XqoVbe41VpiKLRKtGLeuUD09Q/viewform}}
eingesehen werden. Schauen wir uns die Daten mal an:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{glimpse}\NormalTok{(extra)}
\end{Highlighting}
\end{Shaded}

\section{Daten umstellen}\label{daten-umstellen}

Wir haben ein Diagramm vor Augen (s.u.), bei dem auf der X-Achse die
Items stehen (1,2,\ldots{},n) und auf der Y-Achse die Anzahl der Kreuze
nach Kategorien.

Viele Grafik-Funktionen sind nun so aufgebaut, dass auf der X-Achsen nur
\emph{eine} Variable steht. \texttt{ggplot2}, das wir hier verwenden,
ist da keine Ausnahme. Wir müssen also die ``breite'' Tabelle (10
Spalten, pro Item eine) in eine ``lange Spalte'' umbauen: Eine Spalte
heißt dann ``Itemnummer'' und die zweite ``Wert des Items'' oder so
ähnlich.

Also, los geht's: Zuerst wählen wir aus der Fülle der Daten, die
Spalten, die uns interessieren: Die 10 Extraversionsitems, in diesem
Fall (Spalten 3 bis 12).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{extra_items <-}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(extra, }\DecValTok{3}\OperatorTok{:}\DecValTok{12}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Dann stellen wir die Daten von ``breit'' nach ``lang'' um, so dass die
Items eine Variable bilden und damit für \texttt{ggplot2} gut zu
verarbeiten sind.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{extra_long <-}\StringTok{ }\KeywordTok{gather}\NormalTok{(extra_items, }\DataTypeTok{key =}\NormalTok{ items, }\DataTypeTok{value =}\NormalTok{ Antwort)}

\NormalTok{extra_long}\OperatorTok{$}\NormalTok{Antwort <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(extra_long}\OperatorTok{$}\NormalTok{Antwort)}
\end{Highlighting}
\end{Shaded}

Den Befehl mit \texttt{factor} brauchen wir für zum Diagramm erstellen
im Folgenden. Dieser Befehl macht aus den Zahlen bei der Variable
\texttt{Antwort} eine nominale Variable (in R: \texttt{factor}) mit
Text-Werten ``1'', ``2'' und so weiter. Wozu brauchen wir das? Der
Digrammbefehl unten kann nur mit nominalen Variablen Gruppierungen
durchführen. Wir werden in dem Diagramm die Anzahl der Antworten
darstellen - die Anzahl der Antworten nach Antwort-Gruppe (Gruppe mit
Antwort ``1'' etc.).

Keine Sorge, wenn sich das reichlich ungewöhnlich anhört. Sie müssen es
an dieser Stelle nicht erfinden :-)

Man gewöhnt sich daran einerseits; und andererseits ist es vielleicht
auch so, dass diese Funktionen nicht perfekt sind, oder nicht aus
unserer Sicht oder nur aus Sicht des Menschen, der die Funktion
geschrieben hat. Jedenfalls brauchen wir hier eine \texttt{factor}
Variable zur Gruppierung\ldots{}

Damit haben wir es schon! Jetzt wird gemalt.

\section{Diagramme für Anteile}\label{diagramme-fur-anteile}

Stellen wir die Anteile der Antworten anhand von farbig gefüllten Balken
dar (s. Abbildung \ref{fig:vis1}). Beachten Sie, dass die Balken alle
auf 1 (100\%) skaliert sind; es werden also \emph{relative} Häufigkeiten
dargestellt. Absolute Häufigkeiten bleiben hier außen vor.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ extra_long) }\OperatorTok{+}
\StringTok{  }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ items)  }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{fill =}\NormalTok{ Antwort), }\DataTypeTok{position =} \StringTok{"fill"}\NormalTok{) }

\NormalTok{p1}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{056_Fallstudie_Visualisierung_files/figure-latex/vis1-1} 

}

\caption{Relative Häufigkeiten dargestellt anhand von Balkendiagrammen}\label{fig:vis1}
\end{figure}

Was macht dieser \texttt{ggplot} Befehl? Schauen wir es uns in Einzelnen
an:

\begin{itemize}
\tightlist
\item
  \texttt{ggplot(data\ =\ ...)}: Wir sagen ``Ich möchte gern die
  Funktion ggplot nutzen, um den Datensatz \ldots{} zu plotten''.
\item
  \texttt{aes(...)}: Hier definieren wir die ``aesthetics'' des
  Diagramms, d.h. alles ``Sichtbare''. Wir ordnen in diesem Fall der
  X-Achse die Variable \texttt{items} zu. Per Standardeinstellung geht
  \texttt{ggplot} davon aus, dass sie die Häufigkeiten der X-Werte auf
  der Y-Achse haben wollen, wenn Sie nichts über die Y-Achse sagen.
  Jetzt haben wir ein Koordinatensystem definiert (das noch leer ist).
\item
  \texttt{geom\_bar()}: ``Hey R oder ggplot, jetzt male mal einen
  barplot in den ansonsten noch leeren plot''.
\item
  \texttt{aes(fill\ =\ Antwort)}: Genauer gesagt nutzen wir \texttt{aes}
  um einen sichtbaren Aspekte des Diagramms (wie die X-Achse) eine
  Variable des Datensatzes zuzuordnen. Jetzt sagen wir, dass die Füllung
  (im Balkendiagramm) durch die Werte von \texttt{Antwort} definiert
  sein sollen (also ``1'', ``2'' etc.).
\item
  \texttt{position\ =\ "fill"} sagt, dass die Gesamt-Höhe des Balken
  aufgeteilt werden soll mit den ``Teil-Höhen'' der Gruppen
  (Antwort-Kategorien 1 bis 4); wir hätten die Teil-Höhen auch
  nebeneinander stellen können.
\end{itemize}

Vielleicht ist es schöner, die NAs erst zu entfernen.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{extra_long <-}\StringTok{ }\KeywordTok{na.omit}\NormalTok{(extra_long)}
\end{Highlighting}
\end{Shaded}

Plotten Sie das Diagramm dann noch mal:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ extra_long) }\OperatorTok{+}
\StringTok{  }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ items)  }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{fill =}\NormalTok{ Antwort), }\DataTypeTok{position =} \StringTok{"fill"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\section{Rotierte Balkendiagramme}\label{rotierte-balkendiagramme}

Dazu ergänzen wir die Zeile \texttt{+\ coord\_flip()}; das heißt so viel
wie ``flippe das Koordinatensystem''.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 }\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_flip}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\section{Text-Labels für die Items}\label{text-labels-fur-die-items}

Wir definieren die Texte (``Labels'') für die Items:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{item_labels <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Ich bin das erste Item"}\NormalTok{,}
                 \StringTok{"Das zweite Item"}\NormalTok{,}
                 \StringTok{"Item 3 sdjfkladsjk"}\NormalTok{,}
                 \StringTok{"Ich bin ein krasser Couch-Potato UMKODIERT"}\NormalTok{,}
\StringTok{"i5 asf"}\NormalTok{, }\StringTok{"i6 sdf"}\NormalTok{, }\StringTok{"adfjks"}\NormalTok{, }\StringTok{"sfjlkd"}\NormalTok{, }\StringTok{"sdfkjl"}\NormalTok{, }\StringTok{"sdfjkl"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Jetzt hängen wir die Labels an die Items im Diagramm (s. Abbildung
\ref{fig:vis3}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 }\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_flip}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_x_discrete}\NormalTok{(}\DataTypeTok{labels =}\NormalTok{ item_labels)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{056_Fallstudie_Visualisierung_files/figure-latex/vis3-1} 

}

\caption{Rotiertes Balkendiagramm mit Item-Label}\label{fig:vis3}
\end{figure}

Man kann auch einen Zeilenumbruch in den Item-Labels erzwingen\ldots{}
wobei das führt uns schon recht weit, aber gut, zum Abschluss :-)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{item_labels <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Ich bin das erste Item"}\NormalTok{,}
                 \StringTok{"Das zweite Item"}\NormalTok{,}
                 \StringTok{"Item 3 sdjfkladsjk"}\NormalTok{,}
                 \StringTok{"Ich bin ein krasser }\CharTok{\textbackslash{}n}\StringTok{Couch-Potato***mit Zeilenumbruch***"}\NormalTok{,}
\StringTok{"i5 asf"}\NormalTok{, }\StringTok{"i6 sdf"}\NormalTok{, }\StringTok{"adfjks"}\NormalTok{, }\StringTok{"sfjlkd"}\NormalTok{, }\StringTok{"sdfkjl"}\NormalTok{, }\StringTok{"sdfjkl"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Plotten Sie das dann wieder:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ extra_long) }\OperatorTok{+}
\StringTok{  }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ items)  }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{fill =}\NormalTok{ Antwort), }\DataTypeTok{position =} \StringTok{"fill"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_flip}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_x_discrete}\NormalTok{(}\DataTypeTok{labels =}\NormalTok{ item_labels, }\DataTypeTok{name =} \StringTok{"Extraversionsitems"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Anteile"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Diagramm mit Häufigkeiten}\label{diagramm-mit-haufigkeiten}

Ach so, schön wäre noch die echten Zahlen an der Y-Achse, nicht Anteile.
Dafür müssen wir unseren Diagrammtyp ändern, bzw. die Art der Anordnung
ändern. Mit \texttt{position\ =\ "fill"} wird der Anteil (also mit einer
Summe von 100\%) dargestellt. Wir können auch einfach die
Zahlen/Häufigkeiten anzeigen, in dem wir die Kategorien ``aufeinander
stapeln''. Probieren Sie dazu die folgende Syntax.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p2 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ extra_long) }\OperatorTok{+}
\StringTok{  }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ items)  }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{fill =}\NormalTok{ Antwort), }\DataTypeTok{position =} \StringTok{"stack"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_flip}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_x_discrete}\NormalTok{(}\DataTypeTok{labels =}\NormalTok{ item_labels) }

\NormalTok{p2}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{056_Fallstudie_Visualisierung_files/figure-latex/vis5-1} \end{center}

\section{Farbschemata}\label{farbschemata}

Ja, die Wünsche hören nicht auf\ldots{} Also, noch ein anderes
Farbschema (s. Abbildung \ref{fig:vis6}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_brewer}\NormalTok{(}\DataTypeTok{palette =} \DecValTok{17}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{056_Fallstudie_Visualisierung_files/figure-latex/vis6-1} 

}

\caption{... Mit der Brewer-Palette 17}\label{fig:vis6}
\end{figure}

Das Paket \texttt{viridris} hat ein gutes Farbschema. Probieren Sie es
mal aus:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_viridis}\NormalTok{(}\DataTypeTok{discrete =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\part{Modellieren}

\chapter{Grundlagen des Modellierens}\label{mod1}

\begin{center}\includegraphics[width=0.3\linewidth]{images/FOM} \end{center}

\begin{center}\includegraphics[width=0.1\linewidth]{images/licence} \end{center}

\BeginKnitrBlock{rmdcaution}
Lernziele:

\begin{itemize}
\tightlist
\item
  Erläutern können, was man unter einem Modell versteht.
\item
  Die Ziele des Modellieren aufzählen und erläutern können.
\item
  Die Vor- und Nachteile von einfachen vs.~komplexen Modellen
  vergleichen können.
\item
  Wissen, was man unter ``Bias-Varianz-Abwägung'' versteht.
\item
  Um die Notwendigkeit von Trainings- und Test-Stichproben wissen.
\item
  Wissen, was man unter Modellgüte versteht.
\item
  Um die Schwierigkeiten der Prädiktorenauswahl wissen.
\end{itemize}
\EndKnitrBlock{rmdcaution}

In diesem Kapitel benötigen wir diese Pakete:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{images/modellieren/Modellieren} \end{center}

\section{Was ist ein Modell? Was ist Modellieren?}\label{Modellieren}

In diesem Kapitel geht es um \emph{Modelle} und \emph{Modellieren}; aber
was ist das eigentlich? Seit dem 16. Jahrhundert wird mit dem
italienischen Begriff \emph{modelle} ein \emph{verkleinertes Muster},
\emph{Abbild} oder Vorbild für ein Handwerksstück benannt (Gigerenzer
\protect\hyperlink{ref-gigerenzer1980}{1980}). Prototypisch für ein
Modell ist - wer hätt's gedacht - ein Modellauto (s. Abb.
\ref{fig:vwmodell}; (Spurzem
\protect\hyperlink{ref-spurzem_vw_2017}{2017})).

\begin{figure}

{\centering \includegraphics[width=0.4\linewidth]{images/modellieren/vw_modell} 

}

\caption{Ein Modell eines VW-Käfers als Prototyp eines Modells}\label{fig:vwmodell}
\end{figure}

In die Wissenschaft kam der Begriff in der Zeit nach Kant, als man sich
klar wurde, dass (physikalische) Theorien nicht die Wirklichkeit als
solche zeigen, sondern ein \emph{Modell} davon. Modellieren ist eine
grundlegenden Tätigkeit, derer sich Menschen fortlaufend bedienen, um
die Welt zu \emph{verstehen}. Denn das Leben ist schwer\ldots{} oder
sagen wir: komplex. Um einen Ausschnitt der Wirklichkeit zu verstehen,
erscheint es daher sinnvoll, sich einige als wesentlich erachteten
Aspekte ``herauszugreifen'' bzw. auszusuchen und sich nur noch deren
Zusammenspiel näher anzuschauen. Modelle sind häufig vereinfachend: es
wird nur ein Ausschnitt der Wirklichkeit in einfacher Form
berücksichtigt.

\begin{quote}
Da wir die Natur bzw. die Wirklichkeit oft nicht komplett erfassen,
erschaffen wir uns ein Abbild von der Wirklichkeit, ein Modell.
\end{quote}

Manche Aspekte der Wirklichkeit sind wirklicher als andere: Interessiert
man sich für den Zusammenhang von Temperatur und Grundwasserspiegel, so
sind diese Dinge direkt beobachtbar. Interessiert man sich hingegen für
Lebensqualität und Zufriedenheit, so muss man diese
Untersuchungsgegenstände erst konstruieren, da Lebensqualität nicht
direkt beobachtbar ist. Sprechen wir daher von Wirklichkeit lieber
vorsichtiger vom \emph{Gegenstandsbereich}, also den \emph{konstruierten
Auszug der Wirklichkeit} für den sich die forschende Person
interessiert. Bestenfalls (er)findet man eine \emph{Annäherung} an die
Wirklichkeit, schlechterenfalls eine \emph{verzerrte}, gar \emph{grob
falsche} Darstellung . Da keine Wiedergabe der Wirklichkeit perfekt ist,
sind streng genommen alle Modelle ``falsch'' in diesem Sinne.

Gegenstandsbereich und Modelle stehen in einer Beziehung miteinander
(vgl. Abb. \ref{fig:modellieren-plot}, das Foto stammt von Unrau
(\protect\hyperlink{ref-unrau1}{2017})).

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/modellieren/Modell} 

}

\caption{Modellieren}\label{fig:modellieren-plot}
\end{figure}

Damit verstehen wir \emph{Modellieren als eine typische Aktivität von
Menschen} (Gigerenzer \protect\hyperlink{ref-gigerenzer1980}{1980}),
genauer \emph{eines Menschen} mit einem \emph{bestimmten Ziel}. Wir
können gar nicht anders, als uns ein Modell unserer Umwelt zu machen;
entsprechend kann (und muss man) von \emph{mentalen Modellen} sprechen.
Vielfältige Medien kommen dazu in Frage: Bilder, Geschichten, Logik,
Gleichungen. Wir werden uns hier auf eine bestimmte Art formalisierter
Modelle, \emph{numerische} Modelle, konzentrieren, weil es dort am
einfachsten ist, die Informationen auf präzise Art und Weise
herauszuziehen. Allgemein gesprochen ist hier unter Modellieren der
Vorgang gemeint, ein Stück Wirklichkeit (``Empirie'') in eine
\emph{mathematische Struktur} zu übersetzen.

Wirklichkeit kann dabei als \emph{empirisches System} bezeichnet werden,
welches aus einer oder mehr Mengen von Objekten besteht, die zu einander
in bestimmten Beziehungen stehen. Ein Bespiel wäre eine Reihe von
Personen, die in bestimmten Größe-Relationen zueinander stehen oder eine
Reihe von Menschen, bei denen die Füße tendenziell größer werden, je
größer die Körpergröße ist.

Mit mathematische Struktur ist ein formalisiertes Pendant zum
empirischen System gemeint, daher spricht man von einem
\emph{numerischen System}. Im Gegensatz zur empirischen System ist das
numerische System rein theoretisch, also ausgedacht, nicht empirisch.
Auch hier gibt es eine Reihe von Objekten, aber mathematischer Art, also
z.B. Zahlen oder Vektoren. Diese mathematischen Objekten stehen wiederum
in gewissen Relationen zueinander. Der springende Punkt ist: Im Modell
sollen die Beziehungen zwischen den mathematischen Objekten die
Beziehungen zwischen den empirischen Objekten widerspiegeln. Was heißt
das? Stellen wir uns vor, der Klausurerfolg steigt mit der
Lernzeit\footnote{wieder ein typisches Dozentenbeispiel}. Fragen wir das
Modell, welchen Klausurerfolg Alois hat (er hat sehr viel gelernt), so
sollte das Modell erwidern, dass Alois einen hohen Klausurerfolg hat
(Modelle geben in diesem Fall gerne eine im Verhältnis große Zahl von
sich). Damit würde das Modell korrekt die Empirie widerspiegeln.

\begin{quote}
Modellieren bedeutet ein Verfahren zu erstellen, welches empirische
Sachverhalte adäquat in numerische Sachverhalte umsetzt.
\end{quote}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/modellieren/Modellieren_formal_crop} 

}

\caption{Formaleres Modell des Modellierens}\label{fig:modellieren-formal}
\end{figure}

Etwas spitzfindig könnte man behaupten, es gibt keine Modelle - es gibt
nur Modelle \emph{von} etwas; dieser Satz soll zeigen, dass zwar ein
empirisches System für sich alleine stehen kann, aber ein Modell nicht.
Ein Modell verweist immer auf ein empirisches System.

Abb. \ref{fig:modellieren-formal} stellt diese formalere Sichtweise des
Modellierens dar; das empirische System \emph{E} wird dem numerische
System \emph{Z} zugeordnet. Dabei besteht E aus einer Menge von Objekten
\emph{O} sowie einer Menge von \emph{Relationen}\index{Relation} \(R_E\)
(Relationen meint hier nichts mehr als irgendwelche Beziehungen zwischen
den Elementen von O). Analog besteht Z aus einer Menge von numerischen
Objekten \emph{Z} sowie einer Menge von Relationen \(R_Z\) (Relationen
zwischen den Elementen von Z)\footnote{Diese Sichtweise des Modellierens
  basiert auf der Repräsentationstheorie des Messens nach Suppes und
  Zinnes (\protect\hyperlink{ref-suppes1962basic}{1962}) zurück; vgl.
  Gigerenzer (\protect\hyperlink{ref-gigerenzer1980}{1980})}.

\section{Ein Beispiel zum Modellieren in der
Datenanalyse}\label{ein-beispiel-zum-modellieren-in-der-datenanalyse}

Schauen wir uns ein Beispiel aus der Datenanalyse an; laden Sie dazu
zuerst den Datensatz zur Statistikklausur.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{060_Modellieren_files/figure-latex/plot-stats-1} 

}

\caption{Ein Beispiel für Modellieren}\label{fig:plot-stats}
\end{figure}

Im linken Plot (A; Abb. \ref{fig:plot-stats} sehen wir - schon übersetzt
in eine Datenvisualisierung - den Gegenstandsbereich. Dort sind einige
Objekte zusammen mit ihren Relationen abgebildet (Körpergröße und
Schuhgröße). Der rechte Plot spezifiziert nun diesen Einfluss: Es wird
ein \emph{linearer Zusammenhang} (eine Gerade) zwischen Körpergröße und
Schuhgröße unterstellt.

Im rechten Plot (B; Abb. \ref{fig:mod-beispiel}) sehen wir ein Schema
dazu, ein sog. \emph{Pfadmodell}. Noch ist das Modell recht
unspezifisch; es wird nur postuliert, dass Körpergröße auf Schuhgröße
einen linearen Einfluss habe. Linear heißt hier, dass der Einfluss von
Körpergröße auf Schuhgröße immer gleich groß ist, also unabhängig vom
Wert der Körpergröße.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/modellieren/Modellieren_Bsp1} 

}

\caption{Ein Beispiel für ein Pfadmodell}\label{fig:mod-beispiel}
\end{figure}

Ein etwas aufwändigeres Modell könnte so aussehen (Abb.
\ref{fig:plot-modell-bsp2}:

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/modellieren/Modellieren_Bsp2} 

}

\caption{Ein etwas aufwändigeres Modell}\label{fig:plot-modell-bsp2}
\end{figure}

Allgemeiner formuliert, haben wir einen oder mehrere
\emph{Eingabegrößen}\index{Einflussgrößen} bzw.
\emph{Prädiktoren}\index{Prädiktoren}, von denen wir annehmen, dass sie
einen Einfluss haben auf genau eine \emph{Zielgröße}
(\emph{Ausgabegröße}) bzw. \emph{Kriterium}\index{Kriterium}.

\BeginKnitrBlock{rmdcaution}
Einfluss ist hier nicht (notwendig) kausal gemeint, auch wenn es das
Wort so vermuten lässt. Stattdessen ist nur ein statistischer Einfluss
gemeint; letztlich nichts anderes als ein Zusammenhang. In diesem Sinne
könnte man postulieren, dass die Größe des Autos, das man fährt einen
``Einfluss'' auf das Vermögen des Fahrers habe. Empirisch ist es gut
möglich, dass man Belege für dieses Modell findet. Jedoch wird dieser
Einfluss nicht kausal sein (man informiere mich, wenn es anders sein
sollte).
\EndKnitrBlock{rmdcaution}

Modelle, wie wir sie betrachten werden, berechnen eine quantitativen
Zusammenhang zwischen diesen beiden Arten von Größen - Prädiktoren und
Kriterien. Damit lassen sich unsere Modelle in drei Aspekte gliedern.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/modellieren/Modell_Blackbox} 

}

\caption{Modelle mit schwarzer Kiste}\label{fig:fig-blackbox}
\end{figure}

Die Einflussgrößen werden in einer ``schwarzen Kiste'', die wir hier
noch nicht näher benennen, irgendwie verwurstet, will sagen, verrechnet,
so dass ein \emph{geschätzter} Wert für das Kriterium, eine
\emph{Vorhersage} ``hinten bei rauskommt''\footnote{das ist schließlich
  entscheidend - frei nach Helmut Kohl}. Wir gehen dabei nicht davon
aus, dass unsere Modelle perfekt sind, sondern dass Fehler passieren.
Mathematischer ausgedrückt:

\[Y = f(X) + \epsilon\]

Hier stehen \(Y\) für das Kriterium, \(X\) für den oder die Prädiktoren,
\(f\) für die ``schwarze Kiste'' und \(\epsilon\) für den Fehler, den
wir bei unserer Vorhersage begehen. Durch den Fehlerterm in der
Gleichung ist das Modell \emph{nicht}
\emph{deterministisch}\index{deterministisch}, sondern beinhaltet
erstens einen funktionalen Term (\(Y=f(x)\)) und zweitens einen
\emph{stochastischen} Term (\(\epsilon\)). Die schwarze Kiste könnte man
auch als eine \emph{datengenerierende
Maschine}\index{datengenerierende Maschine} oder datengenerierenden
Prozess bezeichnen.

Übrigens: Auf das Skalenniveau der Eingabe- bzw. Ausgabegrößen
(qualitativ vs.~quantitativ) kommt es hier nicht grundsätzlich an; es
gibt Modelle für verschiedene Skalenniveaus bzw. Modelle, die recht
anspruchslos sind hinsichtlich des Skalenniveaus (sowohl für Eingabe-
als auch Ausgabegrößen). Was die Ausgabegröße (das Kriterium) betrifft,
so ``fühlen'' qualitative Variablen von quantitativen Variablen anders
an. Ein Beispiel zur Verdeutlichung: ``Gehört Herr Bussi-Ness zur Gruppe
der Verweigerer oder der Wichtigmacher?'' (qualitatives Kriterium);
``Wie hoch ist der Wichtigmacher-Score von Herrn Bussi-Ness?''
(quantitatives Kriterium). Ein Modell mit qualitativem Kriterium
bezeichnet man auch als \emph{Klassifikation}\index{Klassifikation}; ein
Modell mit quantitativem Kriterium bezeichnet man auch als
\emph{Regression}\index{Klassifikation}. Bei letzterem Begriff ist zu
beachten, dass er \emph{doppelt} verwendet wird. Neben der gerade
genannten Bedeutung steht er auch für ein häufig verwendetes Modell -
eigentlich das prototypische Modell - für quantitative Kriterien.

\section{Taxonomie der Ziele des Modellierens}\label{Ziele}

Modelle kann man auf vielerlei Arten gliedern; für unsere Zwecke ist
folgende Taxonomie der Ziele von Modellieren nützlich.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{itemize}
\tightlist
\item
  Geleitetes Modellieren

  \begin{itemize}
  \tightlist
  \item
    Prädiktives Modellieren
  \item
    Explikatives Modellieren
  \end{itemize}
\item
  Ungeleitetes Modellieren

  \begin{itemize}
  \tightlist
  \item
    Dimensionsreduzierendes Modellieren
  \item
    Fallreduzierendes Modellieren
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Betrachten wir diese vier Ziele des Modellierens genauer.

\emph{Geleitetes Modellieren}\index{Geleitetes Modellieren} ist jede Art
des Modellierens, wo die Variablen in Prädiktoren und Kriterien
unterteilt werden, z.B. Abb. \ref{fig:mod-beispiel}. Man könnte diese
Modelle einfach darstellen als ``X führt zu Y''.

\emph{Prädiktives Modellieren}\index{Prädiktives Modellieren} könnte man
kurz als \emph{Vorhersagen}\index{Vorhersagen} bezeichnen. Hier ist das
Ziel, eine Black Box geschickt zu wählen, so dass der Vohersagefehler
möglichst klein ist. Man zielt also darauf ab, möglichst exakte
Vorhersagen zu treffen. Sicherlich wird der Vorhersagefehler kaum jemals
Null sein; aber je präziser, desto besser. Das Innenleben der
``schwarzen Kiste'' interessiert uns hier \emph{nicht}. Wir machen keine
Aussagen über Ursache-Wirkungs-Beziehungen. Ein Beispiel für ein
prädiktives Modell: ``Je größer das Auto, desto höher das Gehalt''.
Dabei werden wir wohl nicht annehmen, dass die Größe des Auto die
Ursache für die Höhe des Gehalts ist\footnote{bitte mir Bescheid geben,
  falls ich hier etwas übersehen haben sollte}. Wir sind - in diesem
Beispiel - lediglich daran interessiert, das Gehalt möglichst präzise zu
schätzen; die Größe des Autos dient uns dabei als Prädiktor, wir
verstehen sie nicht als Ursache. Ein altbekanntes Lamento der
Statistiklehrer lautet ``Korrelation heißt noch nicht Kausation!''. OK.

\emph{Explikatives Modellieren}\index{Explikatives Modellieren} oder
kurz \emph{Erklären}\index{Erklären} bedeutet, verstehen zu wollen,
\emph{wie} oder \emph{warum} sich ein Kriteriumswert so verändert, wie
er es tut. Auf \emph{welche Art} werden die Prädiktoren verrechnet, so
dass eine bestimmter Kriteriumswert resultiert? Welche Prädikatoren sind
dabei (besonders) wichtig? Ist die Art der Verrechnung abhängig von den
Werten der Prädiktoren? Hierbei interessiert uns vor allem die
\emph{Beschaffenheit} der schwarzen Kiste; die Güte der Vorhersage ist
zweitrangig. Oft, aber nicht immer, steht ein Interesse an den Ursache
hinter dieser Art der Modellierung. Ursache-Wirkungs-Beziehungen gehören
sicherlich zu den interessantesten und wichtigsten Dinge, die man
untersuchen kann. Die Wissenschaft ist (bzw. viele Wissenschaftler sind)
primär an Fragestellungen zur kausalen Beschaffenheit interessiert
(Shmueli \protect\hyperlink{ref-Shmueli2010}{2010}). Ein Beispiel für
diese Art von Modellierung wäre, ob Achtsamkeit zu weiniger intensiven
emotionalen Reaktionen führt (Sauer, Walach, und Kohls
\protect\hyperlink{ref-sauer2010gray}{2010}). Übrigens: Es ist erlaubt,
eine kausale Theorie zu vertreten, auch wenn eine Studie solche
Schlussfolgerungen nur eingeschränkt oder gar nicht erlaubt (Shmueli
\protect\hyperlink{ref-Shmueli2010}{2010}). Häufig werden
Beobachtungsstudien auf Korrelationsbasis angeführt, um kausale Theorien
zu testen. Natürlich ist der Preis für eine einfachere Studie, dass man
weniger Evidenz für eine Theorie mit Kausalanspruch einstreichen kann.
Aber irgendwo muss man ja anfangen (aber man sollte nicht bei einfachen
Studien stehen bleiben).

Vorhersagen und Erklären haben gemein, dass Eingabegrößen genutzt
werden, um Aussagen über einen Ausgabegröße zu treffen. Anders gesagt:
Es liegt eine Zielgröße mit bekannten Ausprägungen vor, zumindest für
eine Reihe von Fällen. Hat man einen Datensatz, so kann man prüfen,
\emph{wie gut} das Modell funktioniert, also wie genau man die
Ausgabewerte vorhergesagt hat. Das ist also eine Art ``Lernen mit
Anleitung'' oder \emph{angeleitetes Lernen}\index{angeleitetes Lernen}
oder \emph{geleitetes Modellieren} (engl. \emph{supervised learning}).
Abbildung \ref{fig:fig-blackbox} gibt diesen Fall wieder.

Beim \emph{ungeleiteten Modellieren}\index{Ungeleiteten Modellieren}
entfällt die Unterteilung zwischen Prädiktor und Kriterium. Ungeleitetes
Modellieren (\emph{Reduzieren}\index{Reduzieren}) meint, dass man die
Fülle des Datenmaterials verringert, in dem man ähnliche Dinge
zusammenfasst (vgl. Abb. \ref{fig:ungeleitetes-modellieren}).

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/modellieren/ungeleitetes_Modellieren_crop} 

}

\caption{Die zwei Arten des ungeleiteten Modellierens}\label{fig:ungeleitetes-modellieren}
\end{figure}

Fasst man \emph{Fälle} zusammen, so spricht man von
\emph{Fallreduzierendem
Modellieren}\index{Fallreduzierendes Modellieren}. Zum Beispiel könnte
man spektakulärerweise ``Britta'', ``Carla'' und ``Dina'' zu ``Frau''
und ``Joachim'', ``Alois'' und ``Casper'' zu ``Mann'' zusammen fassen.

Analog spricht man von \emph{Dimensionsreduzierendes
Modellieren}\index{Dimensionsreduzierendes Modellieren} wenn Variablen
zusammengefasst werden. Hat man z.B. einen Fragebogen zur
Mitarbeiterzufriedenheit mit den Items ``Mein Chef ist fair'', ``Mein
Chef ist kompetent'', ``Meinem Chef ist meine Karriere wichtig'', so
könnte man - wenn die Daten dies unterstützen - die Items zu einer
Variable ``Zufriedenheit mit Chef'' zusammenfassen.

Wenn also das Ziel des Modellieren lautet, die Daten zu reduzieren, also
z.B. Kunden nach Persönlichkeit zu gruppieren, so ist die Lage anders
als beim geleiteten Modellieren: Es gibt keine Zielgröße. Wir wissen
nicht, was die ``wahre Kundengruppe'' ist, zu der Herrn Casper
Bussi-Ness gehört. Wir sagen eher, ``OK, die drei Typen sind sich
irgendwie ähnlich, sie werden wohl zum selben Typen von Kunden
gehören''. Wir tappen (noch mehr) in Dunkeln, was die ``Wahrheit'' ist
im Vergleich zum angeleiteten Modellieren. Unser Modell muss ohne
Hinweise darauf, was richtig ist auskommen. Man spricht daher in diesem
Fall von \emph{Lernen ohne Anleitung}\index{Lernen ohne Anleitung} oder
\emph{ungeleitetes Modellieren} (engl. \emph{unsupervised
learning}\index{unsupervised learning}).

\section{Die vier Schritte des statistischen
Modellierens}\label{die-vier-schritte-des-statistischen-modellierens}

Modellieren ist in der Datenanalyse bzw. in der Statistik eine zentrale
Tätigkeit. Modelliert man in der Statistik, so führt man die zwei
folgenden Schritte aus:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Man wählt eines der vier Ziele des Modellierens (z.B. ein prädiktives
  Modell).
\item
  Man wählt ein Modell aus (genauer: eine Modellfamilie), z.B.
  postuliert man, dass die Körpergröße einen linearen Einfluss auf die
  Schuhgröße habe.
\item
  Man bestimmt (berechnet) die Details des Modells anhand der Daten: Wie
  groß ist die Steigung der Geraden und wo ist der Achsenabschnitt? Man
  sagt auch, dass man die \emph{Modellparameter} anhand der Daten
  schätzt (``Modellinstantiierung'' oder ``Modellanpassung'', engl.
  ``model fitting'').
\item
  Dann prüft man, wie gut das Modell zu den Daten passt (Modellgüte,
  engl. ``model fit''); wie gut lässt sich die Schuhgröße anhand der
  Körpergröße vorhersagen bzw. wie groß ist der Vorhersagefehler?
\end{enumerate}

\section{Einfache vs.~komplexe Modelle: Unter-
vs.~Überanpassung}\label{einfache-vs.komplexe-modelle-unter--vs.uberanpassung}

Je komplexer ein Modell, desto besser passt sie meistens auf den
Gegenstandsbereich. Eine grobe, Holzschnitt artige Theorie ist doch
schlechter als eine, die feine Nuancen berücksichtigt, oder nicht?
Einiges spricht dafür; aber auch einiges dagegen. Schauen wir uns ein
Problem mit komplexen Modellen an.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{060_Modellieren_files/figure-latex/overfitting-4-plots-1} 

}

\caption{Welches Modell (Teil B-D; rot, grün, blau) passt am besten zu den Daten (Teil A) ?}\label{fig:overfitting-4-plots}
\end{figure}

Der Plot A (links) von Abb. \ref{fig:overfitting-4-plots} zeigt den
Datensatz ohne Modell; Plot B legt ein lineares Modell (rote Gerade) in
die Daten. Plot C zeigt ein Modell, welches die Daten exakt erklärt -
die (blaue) Linie geht durch alle Punkte. Der 4. Plot zeigt ein Modell
(grüne Linie), welches die Punkte gut beschreibt, aber nicht exakt
trifft.

Welchem Modell würden Sie (am meisten) vertrauen? Das ``blaue Modell''
beschreibt die Daten sehr gut, aber hat das Modell überhaupt eine
``Idee'' vom Gegenstandsbereich, eine ``Ahnung'', wie Y und X
zusammenhängen, bzw. wie X einen Einfluss auf Y ausübt? Offenbar nicht.
Das Modell ist ``übergenau'' oder zu komplex. Man spricht von
\emph{Überanpassung}\index{Überanpassung} (engl.
\emph{overfitting}\index{overfitting}). Das Modell scheint zufälliges,
bedeutungsloses Rauschen zu ernst zu nehmen. Das Resultat ist eine zu
wackelige Linie - ein schlechtes Modell, da wir wenig Anleitung haben,
auf welche Y-Werte wir tippen müssten, wenn wir neue, unbekannte X-Werte
bekämen.

\begin{quote}
Beschreibt ein Modell (wie das blaue Modell hier) eine Stichprobe sehr
gut, heißt das noch \emph{nicht}, dass es auch zukünftige (und
vergleichbare) Stichproben gut beschreiben wird. Die Güte
(Vorhersagegenauigkeit) eines Modells sollte sich daher stets auf eine
neue Stichprobe beziehen (Test-Stichprobe), die nicht in der Stichprobe
beim Anpassen des Modells (Trainings-Stichprobe) enthalten war.
\end{quote}

Was das ``blaue Modell'' zu detailverliebt ist, ist das ``rote Modell''
zu simpel. Die Gerade beschreibt die Y-Werte nur sehr schlecht. Man
hätte gleich den Mittelwert von Y als Schätzwert für jedes einzelne
\(Y_i\) hernehmen können. Dieses lineare Modell ist
\emph{unterangepasst}\index{Unteranpassung}, könnte man sagen (engl.
\emph{underfittting}\index{underfitting}). Auch dieses Modell wird uns
wenig helfen können, wenn es darum geht, zukünftige Y-Werte
vorherzusagen (gegeben jeweils einen bestimmten X-Wert).

Ah! Das \emph{grüne Modell} scheint das Wesentliche, die ``Essenz'' der
``Punktebewegung'' zu erfassen. Nicht die Details, die kleinen
Abweichungen, aber die ``große Linie'' scheint gut getroffen. Dieses
Modell erscheint geeignet, zukünftige Werte gut zu beschreiben. Das
grüne Modell ist damit ein Kompromiss aus Einfachheit und Komplexität
und würde besonders passen, wenn es darum gehen sollte, zyklische
Veränderungen zu erklären\footnote{Tatsächlich wurden die Y-Werte als
  Sinus-Funktion plus etwas normalverteiltes Rauschen simuliert.}.

\begin{quote}
Je komplexer ein Modell ist, desto besser beschreibt es einen bekannten
Datensatz (Trainings-Stichprobe). Allerdings ist das Modell, welches den
Trainings-Datensatz am besten beschreibt, nicht zwangsläufig das Modell,
welches neue, unbekannte Daten am besten beschreibt. Oft im Gegenteil!
\end{quote}

Je komplexer das Modell, desto kleiner der Fehler im
\emph{Trainings}-Datensatz. Allerdings: Die Fehler-Kurve im
\emph{Test-}Datensatz ist \emph{U-förmig}: Mit steigender Komplexität
wird der Fehler einige Zeit lang kleiner; ab einer gewissen Komplexität
steigt der Fehler im Test-Datensatz wieder (vgl. Abb.
\ref{fig:overfitting-schema}! Eine `mittlere' Komplexität ist daher am
besten; die Frage ist nur, wie viel `mittel' ist.

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/modellieren/overfitting} 

}

\caption{'Mittlere' Komplexität hat die beste Vorhersagegenauigkeit (am wenigsten Fehler) in der Test-Stichprobe}\label{fig:overfitting-schema}
\end{figure}

\section{Bias-Varianz-Abwägung}\label{bias-varianz-abwagung}

Einfache Modelle bilden (oft) verfehlen oft wesentliche Aspekte des
Gegenstandsbereich; die Wirklichkeit ist häufig zu komplex für einfache
Modelle. Die resultierende \emph{Verzerrung} in den vorhergesagten
Werten nennt man auch \emph{Bias}\index{Bias}. Mit anderen Worten: ist
ein Modell zu einfach, passt es zu wenig zu den Daten (engl.
\emph{underfitting}). Auf der anderen Seite ist das Modell aber
\emph{robust}\index{robust} in dem Sinne, dass sich die vorhergesagten
Werte kaum ändern, falls sich der Trainings-Datensatz etwas ändert.

Ist das Modell aber zu reichhaltig (``komplex''), bildet es alle Details
des Trainings-Datensatzes ab, wird es auch zufällige Variation des
Datensatzes vorhersagen; Variation, die nicht relevant ist, der nichts
Eigentliches abbildet. Das Modell ist ``überangepasst'' (engl.
\emph{overfitting}); geringfügige Änderungen im Datensatz können das
Modell stark verändern. Das Model ist nicht robust. Auf der positiven
Seite werden die Nuancen der Daten gut abgebildet; der Bias ist gering
bzw. tendenziell geringer als bei einfachen Modellen.

\begin{quote}
Einfache Modelle: Viel Bias, wenig Varianz. Komplexe Modelle: Wenig
Bias, viel Varianz.
\end{quote}

Dieser Sachverhalt ist in folgendem Diagramm dargestellt (vgl. Abb.
\ref{fig:plot-bias-variance}; vgl. Kuhn \& Johnson
(\protect\hyperlink{ref-kuhn2013applied}{2013})).

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{060_Modellieren_files/figure-latex/plot-bias-variance-1} 

}

\caption{Der Spagat zwischen Verzerrung und Varianz}\label{fig:plot-bias-variance}
\end{figure}

Der linke Plot zeigt ein komplexes Modell\footnote{Genauer gesagt ein
  Polynom von Grad 5.}; das Modell (blaue Linie) erscheint ``zittrig'';
kleine Änderungen in den Daten können große Auswirkungen auf das Modell
(Verlauf der blauen Linie) haben. Darüber hinaus sind einige Details des
Modells unplausibel: es gibt viele kleine ``Hügel'', die nicht
augenscheinlich plausibel sind.

Der Plot auf der rechten Seiten hingegen ist sehr einfach und robust.
Änderungen in den Daten werden vergleichsweise wenig Einfluss auf das
Modell (die beiden roten Linien) haben.

\section{Training-
vs.~Test-Stichprobe}\label{training--vs.test-stichprobe}

Wie wir gerade gesehen haben, kann man \emph{immer} ein Modell finden,
welches die \emph{vorhandenen} Daten sehr gut beschreibt. Das gleicht
der Tatsache, dass man im Nachhinein (also bei vorhandenen Daten) leicht
eine Erklärung findet. Ob diese Erklärung sich in der Zukunft, bei
unbekannten Daten bewahrheitet, steht auf einem ganz anderen Blatt.

Daher sollte man \emph{immer} sein Modell an einer Stichprobe
\emph{entwickeln} (``trainieren'' oder ``üben'') und an einer zweiten
Stichprobe \emph{testen}. Die erste Stichprobe nennt man auch
\emph{training sample} (Trainings-Stichprobe) und die zweite \emph{test
sample} (Test-Stichprobe). Entscheidend ist, dass das Test-Sample beim
Entwickeln des Modells unbekannt war bzw. nicht verwendet wurde.

\begin{quote}
Die Güte des Modells sollte nur anhand eines - bislang nicht verwendeten
- Test-Samples überprüft werden. Das Test-Sample darf bis zur
Modellüberprüfung nicht analysiert werden.
\end{quote}

Die Modellgüte ist im Trainings-Sample meist deutlich besser als im
Test-Sample (vgl. die Fallstudie dazu: \ref{overfitting-casestudy}).

Zum Aufteilen verfügbarer Daten in eine Trainings- und eine
Test-Stichprobe gibt es mehrere Wege. Einer sieht so aus:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train <-}\StringTok{ }\KeywordTok{slice}\NormalTok{(stats_test, }\DecValTok{1}\OperatorTok{:}\DecValTok{200}\NormalTok{)}
\NormalTok{test <-}\StringTok{ }\KeywordTok{slice}\NormalTok{(stats_test, }\DecValTok{201}\OperatorTok{:}\DecValTok{306}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\texttt{dplyr::slice} schneidet eine `Scheibe' aus einem Datensatz. Mit
\texttt{anti\_join} kann man einen generellen Ansatz wählen:
`anti\_join´ vereinigt die \emph{nicht-gleichen Zeilen} zweier
Datensätze. Die Gleichheit wird geprüft anhand aller Spalten mit
identischem Namen in beiden Datensätzen.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train <-}\StringTok{ }\NormalTok{stats_test }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{sample_frac}\NormalTok{(.}\DecValTok{8}\NormalTok{, }\DataTypeTok{replace =} \OtherTok{FALSE}\NormalTok{)  }\CommentTok{# Stichprobe von 80%, ohne Zurücklegen}

\NormalTok{test <-}\StringTok{ }\NormalTok{stats_test }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{anti_join}\NormalTok{(train)  }\CommentTok{# Alle Zeilen von "stats_test", die nicht in "train" vorkommen}
\end{Highlighting}
\end{Shaded}

Damit haben wir ein Trainings-Sample (\texttt{train}), in dem wir ein
oder besser mehrere Modelle entwickeln können.

So schön wie dieses Vorgehen auch ist, es ist nicht perfekt. Ein
Nachteil ist, dass unsere Modellgüte wohl \emph{anders} wäre, hätten wir
andere Fälle im Test-Sample erwischt. Würden wir also ein neues
Trainings-Sample und ein neues Test-Sample aus diesen Datensatz ziehen,
so hätten wir wohl andere Ergebnisse. Was wenn diese Ergebnisse nun
deutlich von den ersten abweichen? Dann würde unser Vertrauen in die die
Modellgüte sinken. Wir bräuchten also noch ein Verfahren, welches
\emph{Variabilität} in der Modellgüte widerspiegelt.

\section{Wann welches Modell?}\label{wann-welches-modell}

Tja, mit dieser Frage lässt sich ein Gutteil des Kopfzerbrechens in
diesem Metier erfassen. Die einfache Antwort lautet: Es gibt kein
``bestes Modell'', aber es mag für \emph{einen bestimmten
Gegenstandsbereich}, in \emph{einem bestimmten (historisch-kulturellen)
Kontext}, für \emph{ein bestimmtes Ziel} und mit \emph{einer bestimmten
Stichprobe} ein best mögliches Modell geben. Dazu einige Eckpfeiler:

\begin{itemize}
\item
  Unter sonst gleichen Umständen sind einfachere Modelle den komplexeren
  vorzuziehen. Gott sei Dank.
\item
  Je nach Ziel der Modellierung ist ein erklärendes Modell oder ein
  Modell mit reinem Vorhersage-Charakter vorzuziehen.
\item
  Man sollte stets mehrere Modelle vergleichen, um abzuschätzen, welches
  Modell in der aktuellen Situation geeigneter ist.
\end{itemize}

\section{Modellgüte}\label{modellgute}

Wie ``gut'' ist mein Modell? Modelle bewerten bzw. vergleichend bewerten
ist einer der wichtigsten Aufgaben beim Modellieren. Die Frage der
Modellgüte hat viele feine technisch-statistische Verästelungen, aber
einige wesentlichen Aspekte kann man einfach zusammenfassen.

\begin{quote}
Kriterium der theoretischen Plausibilität: Ein statistisches Modell
sollte theoretisch plausibel sein.
\end{quote}

Anstelle ``alles mit allem'' durchzuprobieren, sollte man sich auf
Modelle konzentrieren, die theoretisch plausibel sind. Die Modellwahl
ist theoretisch zu begründen.

\begin{quote}
Kriterium der guten Vorhersage: Die Vorhersagen eines Modells sollen
präzise und überraschend sein.
\end{quote}

Dass ein Modell die Wirklichkeit präzise vorhersagen soll, liegt auf der
Hand. Hier verdient nur der Term \emph{vorher}sagen Beachtung. Es ist
einfach, im Nachhinein Fakten (Daten) zu erklären. Jede Nachbesprechung
eines Bundesliga-Spiels liefert reichlich Gelegenheit, \emph{posthoc}
Erklärungen zu hören. Schwieriger sind Vorhersagen\footnote{Gerade wenn
  sie die Zukunft betreffen; ein Bonmot, das Yogi Berra nachgesagt wird.}.
Die Modellgüte ist also idealerweise an \emph{in der Zukunft liegende}
Ereignisse bzw. deren Vorhersage zu messen. Zur Not kann man auch schon
in der Vergangenheit angefallene Daten hernehmen. Dann müssen diese
Daten aber \emph{für das Modell} neu sein.

Was ist mit überraschend gemeint? Eine Vorhersage, dass die Temperatur
morgen in Nürnberg zwischen -30 und +40°C liegen wird, ist sicherlich
sehr treffend, aber nicht unbedingt präzise und nicht wirklich
überraschend. Die Vorhersage, dass der nächste Chef der Maurer-Innung
(wenn es diese geben sollte) ein Mann sein wird, und nicht eine Frau,
kann zwar präzise sein, ist aber nicht überraschend. Wir werden also in
dem Maße unseren Hut vor dem Modell ziehen, wenn die Vorhersagen sowohl
präzise als auch überraschen sind. Dazu später mehr Details.

\begin{quote}
Kriterium der Situationsangemessenheit: Die Güte des Modells ist auf die
konkrete Situation abzustellen.
\end{quote}

Ein Klassifikationsmodell muss anders beurteilt werden als ein
Regressionsmodell. Reduktionsmodelle müssen wiederum anders beurteilt
werden. In den entsprechenden Kapiteln werden diese Unterschiede
präzisiert.

\section{Auswahl von Prädiktoren}\label{auswahl-von-pradiktoren}

Wie oben diskutiert, stellen wir ein (geleitetes) Modell gerne als ein
Pfaddiagramm des Typs \(X \rightarrow Y\) dar (wobei X ein Vektor sein
kann). Nehmen wir an das Kriterium \(Y\) als gesetzt an; bleibt die
Frage: Welche Prädiktoren (\(X\)) wählen wir, um das Kriterium möglichst
gut vorherzusagen?

Eine einfache Frage. Keine leichte Antwort. Es gibt zumindest drei
Möglichkeiten, die Prädiktoren zu bestimmen: theoriegeleitet,
datengetrieben oder auf gut Glück.

\begin{itemize}
\item
  theoriegeleitet: Eine starke Theorie macht präzise Aussagen, welche
  Faktoren eine Rolle spielen und welche nicht. Auf dieser Basis wählen
  wir die Prädiktoren. Diese Situation ist wünschenswert; nicht nur,
  weil Sie Ihnen das Leben leicht macht, sondern weil es nicht die
  Gefahr gibt, die Daten zu ``overfitten'', ``Rauschen als Muster'' zu
  bewerten - kurz: zu optimistisch bei der Interpretation von
  Statistiken zu sein.
\item
  datengetrieben: Kurz gesagt werden die Prädiktoren ausgewählt, welche
  das Kriterium am besten vorhersagen. Das ist einerseits stimmig,
  andererseits birgt es die Gefahr, dass Zufälligkeiten in den Daten für
  echte Strukturen, die sich auch in zukünftigen Stichproben finden
  würden, missverstanden werden.
\item
  auf gut Glück: tja, kann man keine Theorie zu Rate ziehen und sind die
  Daten wenig aussagekräftig oder man nicht willens ist, sie nicht genug
  zu \sout{quälen} analysieren, so neigen Menschen dazu, zuerst sich
  selbst und dann andere von der Plausibilität der Entscheidung zu
  überzeugen. Keine sehr gute Strategie.
\end{itemize}

In späteren Kapiteln betrachten wir Wege, um Prädiktoren für bestimmte
Modelle auszuwählen.

\section{Aufgaben}\label{aufgaben-10}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Erfolg beim Online-Dating
\end{enumerate}

Lesen Sie diesen\footnote{\url{https://thewinnower.com/papers///5202-the-effect-of-a-status-symbol-on-success-in-online-dating-//an-experimental-study-data-paper?review_it=true}}
Artikel (Sauer und Wolff \protect\hyperlink{ref-sauer_wolff}{2016}).
Zeichnen Sie ein Pfaddiagramm zum Modell!\footnote{Status
  \(\rightarrow\) Erfolg beim Online-Dating}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Ziele des Modellierens
\end{enumerate}

Welche drei Ziele des Modellierens kann man unterscheiden?\footnote{\ref{Ziele}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Bias-Varianz-Abwägung
\end{enumerate}

Betrachten Sie Abb. \ref{fig:plot-bias-variance2}. Welches der beiden
Modelle (visualiert im jeweiligen Plot) ist wahrscheinlich\ldots{}

\begin{itemize}
\tightlist
\item
  mehr bzw. weniger robust gegenüber Änderungen im Datensatz?
\item
  mehr oder weniger präzise?
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{060_Modellieren_files/figure-latex/plot-bias-variance2-1} 

}

\caption{Bias-Varianz-Abwägung. Links: Wenig Bias, viel Varianz. Rechts: Viel Bias, wenig Varianz.}\label{fig:plot-bias-variance2}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Richtig oder falsch?\footnote{R, F, F, F, R}
\end{enumerate}

\BeginKnitrBlock{rmdexercises}
Richtig oder Falsch!?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Die Aussage ``Pro Kilo Schoki steigt der Hüftumfang um einen
  Zentimeter'' kann als Beispiel für ein deterministisches Modell
  herhalten.
\item
  Gruppiert man Kunden nach ähnlichen Kaufprofilen, so ist man insofern
  an ``Reduzieren'' der Datenmenge interessiert.
\item
  Grundsätzlich gilt: Je komplexer ein Modell, desto besser.
\item
  Mit ``Bias'' ist gemeint, dass ein Modell ``zittrig'' oder
  ``wackelig'' ist - sich also bei geringer Änderung der
  Stichprobendaten massiv in den Vorhersagen ändert.
\item
  In der Gleichung \(Y=f(x)+\epsilon\) steht \(\epsilon\) für den Teil
  der Kriteriums, der nicht durch das Modell erklärt wird.
\end{enumerate}
\EndKnitrBlock{rmdexercises}

\section{Befehlsübersicht}\label{befehlsubersicht-5}

Tabelle \ref{tab:befehle-modellieren} fasst die R-Funktionen dieses
Kapitels zusammen.

\begin{table}

\caption{\label{tab:befehle-modellieren}Befehle des Kapitels 'Modellieren'}
\centering
\begin{tabular}[t]{l|l}
\hline
Paket..Funktion & Beschreibung\\
\hline
dplyr::sample\_frac & Zielt eine Stichprobe von x\% aus einem Dataframe\\
\hline
dplyr::anti\_join & Behält alle Zeilen von df1, die *nicht* in df2 vorkommen\\
\hline
dplyr::slice & Schneidet eine 'Scheibe' aus einem Datensatz\\
\hline
\end{tabular}
\end{table}

\section{Verweise}\label{verweise-4}

\begin{itemize}
\item
  Einige Ansatzpunkte zu moderner Statistik (``Data Science'') finden
  sich bei Peng und Matsui (\protect\hyperlink{ref-peng2015art}{2015}).
\item
  Chester Ismay erläutert einige Grundlagen von R und RStudio, die für
  Modellierung hilfreich sind:
  \url{https://bookdown.org/chesterismay/rbasics/}.
\item
  Eine klassische und sehr gute Einführung findet sich bei James,
  Witten, Hastie \& Tibshirani (James, Witten, Hastie, und Tibshirani
  \protect\hyperlink{ref-introstatlearning}{2013}\protect\hyperlink{ref-introstatlearning}{b}).
  Dieses Buch bietet ein
  \href{http://www-bcf.usc.edu/~gareth/ISL/ISLR\%20Sixth\%20Printing.pdf}{frei
  verfügbares PDF}.
\end{itemize}

\chapter{Der p-Wert, Inferenzstatistik und
Alternativen}\label{der-p-wert-inferenzstatistik-und-alternativen}

\begin{center}\includegraphics[width=0.3\linewidth]{images/FOM} \end{center}

\begin{center}\includegraphics[width=0.1\linewidth]{images/licence} \end{center}

\BeginKnitrBlock{rmdcaution}
Lernziele:

\begin{itemize}
\tightlist
\item
  Den p-Wert erläutern können.
\item
  Den p-Wert kritisieren können.
\item
  Alternativen zum p-Wert kennen.
\item
  Inferenzstatistische Verfahren für häufige Fragestellungen kennen.
\end{itemize}
\EndKnitrBlock{rmdcaution}

In diesem Kapitel werden folgende Pakete benötigt:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(pwr)  }\CommentTok{# Powerberechnung}
\KeywordTok{library}\NormalTok{(compute.es)  }\CommentTok{# Effektstärken}
\KeywordTok{library}\NormalTok{(tidyverse)  }\CommentTok{# Datenjudo}
\KeywordTok{library}\NormalTok{(broom)  }\CommentTok{# Anova-Ergebnisse aufräumen}
\KeywordTok{library}\NormalTok{(BayesFactor)  }\CommentTok{# Bayes-Faktor berechnen}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.2\linewidth]{images/inferenz/Ronald_Fisher} 

}

\caption{Der größte Statistiker des 20. Jahrhunderts (p < .05)}\label{fig:sir-fisher}
\end{figure}

\section{Der p-Wert sagt nicht das, was viele
denken}\label{der-p-wert-sagt-nicht-das-was-viele-denken}

Der p-Wert\index{p-Wert}, entwickelt von Sir Ronald Fisher (Abb.
\ref{fig:sir-fisher}), ist die heilige Kuh der Forschenden. Das ist
nicht normativ, sondern deskriptiv gemeint. Der p-Wert entscheidet
(häufig) darüber, was publiziert wird, und damit, was als Wissenschaft
sichtbar ist - und damit, was Wissenschaft ist (wiederum deskriptiv,
nicht normativ gemeint). Kurz: Dem p-Wert kommt viel Bedeutung zu bzw.
ihm wird viel Bedeutung zugemessen (vgl. Abb. \ref{fig:who-said}).

\begin{figure}

{\centering \includegraphics[width=0.35\linewidth]{images/inferenz/p_value_who_said} 

}

\caption{Der p-Wert wird oft als wichtig erachtet}\label{fig:who-said}
\end{figure}

Der p-Wert ist der tragende Ziegelstein in einem Theoriegebäude, das als
\emph{Nullhypothesen-Signifikanztesten}\index{Nullhypothesen-Signifikanztesten}
(NHST\footnote{Der Term `Signifikanz-Hypothesen-Inferenz-Testen' hat
  sich nicht durchgesetzt}) bezeichnet wird. Oder kurz als
`Inferenzstatistik' bezeichnet. Was sagt uns der p-Wert? Eine gute
intuitive Definition ist:

\begin{quote}
Der p-Wert sagt, wie gut die Daten zur Nullhypothese passen.
\end{quote}

Die (genaue) Definition des p-Werts ist kompliziert; man kann sie leicht
missverstehen:

\begin{quote}
Der p-Wert - P(D\textbar{}H) - gibt die Wahrscheinlichkeit P unserer
Daten D an (und noch extremerer), unter der Annahme, dass die getestete
Hypothese H wahr ist (und wenn wir den Versuch unendlich oft wiederholen
würden, unter identischen Bedingungen und ansonsten zufällig).
\end{quote}

Mit anderen Worten: Je \emph{größer p}, desto \emph{besser} passen die
Daten zur \emph{Nullhypothese}. Mit Nullhypothese\index{Nullhypothese}
(H0) bezeichnet man die getestete Hypothese. Der Name Nullhypothese
rührt vom Begriff `nullifizieren' (verwerfen) her, da (nach dem
Falsifikationismus) eine These immer nur verworfen, nie bestätigt werden
kann. Da viele die eigene Hypothese nur ungern verwerfen wollen, wird
die `gegnerische Hypothese', die man loswerden will, getestet. Fällt p
unter die magische Zahl von 5\%, so proklamiert man Erfolg
(\emph{Signifikanz}\index{Signifikanz}) und verwirft die H0.

Der p-Wert ist weit verbreitet. Er bietet die Möglichkeit, relativ
objektiv zu quantifizieren, wie gut ein Kennwert, mindestens so extrem
wie der aktuell vorliegende, zu einer Hypothese passt. Allerdings hat
der p-Wert seine Probleme. Vor allem: Er wird missverstanden. Jetzt kann
man sagen, dass es dem p-Wert (dem armen) nicht anzulasten, dass andere/
einige ihn missverstehen. Auf der anderen Seite finde ich, dass sich
Technologien dem Nutzer anpassen sollten (soweit als möglich) und nicht
umgekehrt.

Viele Menschen - inkl. Professoren und Statistik-Dozenten - haben
Probleme mit dieser Definition (Gigerenzer
\protect\hyperlink{ref-Gigerenzer2004}{2004}). Das ist nicht deren
Schuld: Die Definition ist kompliziert. Vielleicht denken viele, der
p-Wert sage das, was tatsächlich interessant ist: die Wahrscheinlichkeit
der (getesteten) Hypothese H, gegeben der Tatsache, dass bestimmte Daten
D vorliegen. Leider ist das \emph{nicht} die Definition des p-Werts.
Also:

\[ P(D|H) \ne P(H|D) \]

\subsection{Von Männern und Päpsten}\label{von-mannern-und-papsten}

Formeln haben die merkwürdige Angewohnheit vor dem inneren Auge zu
verschwimmen; Bilder sind für viele Menschen klarer, scheint's.
Übersetzen wir die obige Formel in folgenden Satz:

\begin{quote}
Wahrscheinlichkeit, Mann zu sein, wenn man Papst ist UNGLEICH zur
Wahrscheinlichkeit, Papst zu sein, wenn man Mann ist.
\end{quote}

Oder kürzer:

\[ P(M|P) \ne P(P|M) \]

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/inferenz/maenner_papst-crop} 

}

\caption{Mann und Papst zu sein ist nicht das gleiche.}\label{fig:moslems-terroristen}
\end{figure}

Das Bild (Abb. \ref{fig:moslems-terroristen}) zeigt den Anteil der
Männer an den Päpsten (sehr hoch). Und es zeigt den Anteil der Päpsten
von allen Männern (sehr gering). Dabei können wir uns Anteil mit
Wahrscheinlichkeit übersetzen. Kurz: Die beiden Anteile
(Wahrscheinlichkeiten) sind nicht gleich. Man denkt leicht, der p-Wert
sei die \emph{Wahrscheinlichkeit, Papst zu sein, wenn man Mann ist}. Das
ist falsch. Der p-Wert ist die \emph{Wahrscheinlichkeit, Papst zu sein,
wenn man Mann ist}. Ein großer Unterschied.

\section{Der p-Wert ist eine Funktion der
Stichprobengröße}\label{der-p-wert-ist-eine-funktion-der-stichprobengroe}

Der p-Wert ist für weitere Dinge kritisiert worden (Wagenmakers
\protect\hyperlink{ref-Wagenmakers2007}{2007}, Briggs
(\protect\hyperlink{ref-uncertainty}{2016})); z.B. dass die
``5\%-Hürde'' einen zu schwachen Test für die getestete Hypothese
bedeutet. Letzterer Kritikpunkt ist aber nicht dem p-Wert anzulasten,
denn dieses Kriterium ist beliebig, könnte konservativer gesetzt werden
und jegliche mechanisierte Entscheidungsmethode kann ausgenutzt werden.
Ähnliches kann man zum Thema ``P-Hacking'' argumentieren (Head u.~a.
\protect\hyperlink{ref-Head2015}{2015}, Wicherts u.~a.
(\protect\hyperlink{ref-Wicherts2016}{2016})): andere statistische
Verfahren können auch gehackt werden. ``Hacken'' soll hier sagen, dass
man - Kreativität und Wille vorausgesetzt - immer Wege finden kann, um
einen Kennwert in die gewünschte Richtung zu drängen.

Ein anderer Anklagepunkt lautet, dass der p-Wert nicht nur eine Funktion
der Effektgröße sei, sondern auch der Stichprobengröße. Sprich: Bei
großen Stichproben wird jede Hypothese signifikant. Das ist richtig. Das
schränkt die praktische Nützlichkeit ein (vgl. Abb.
\ref{fig:einfluss-pwert}. Die Details der Simulation, die hinter Abb.
\ref{fig:einfluss-pwert} sind etwas umfangreicher und hier nicht so
wichtig, daher nicht angegeben\footnote{s. hier für Details:
  \url{https://sebastiansauer.github.io/pvalue_sample_size/}}.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/inferenz/einfluss_pwert-crop} 

}

\caption{Zwei Haupteinflüsse auf den p-Wert}\label{fig:einfluss-pwert}
\end{figure}

\begin{quote}
Egal wie klein die Effektstärke ist, es existiert eine Stichprobengröße,
die diesen Effekt beliebig signifikant werden lässt.
\end{quote}

Die Verteidigung argumentiert hier, dass das ``kein Bug, sondern ein
Feature'' sei: Wenn man z.B. die Hypothese prüfe, dass der
Gewichtsunterschied zwischen Männern und Frauen 0,000000000kg sei und
man findet 0,000000123kg Unterschied, ist die getestete Hypothese
falsch. Punkt. Der p-Wert gibt demnach das korrekte Ergebnis. Meiner
Ansicht nach ist die Antwort zwar richtig, geht aber an den
Anforderungen der Praxis vorbei.

\section{Mythen zum p-Wert}\label{mythen-zum-p-wert}

Falsche Lehrmeinungen sterben erst aus, wenn die beteiligten Professoren
in Rente gehen, heißt es. Jedenfalls halten sich eine Reihe von Mythen
hartnäckig; sie sind alle falsch.

\begin{quote}
Wenn der p-Wert kleiner als 5\% ist, dann ist meine Hypothese (H1)
sicher richtig.
\end{quote}

Falsch. Richtig ist: ``Wenn der p-Wert kleines ist als 5\% (oder
allgemeiner: kleiner als \(\alpha\), dann sind die Daten (oder noch
extremer) unwahrscheinlich, vorausgesetzt die H0 gilt''.

\begin{quote}
Wenn der p-Wert kleiner als 5\% ist, dann ist meine Hypothese (H1)
höchstwahrscheinlich richtig.
\end{quote}

Falsch. Richtig ist: Wenn der p-Wert kleiner ist als \(alpha\), dann
sind die Daten unwahrscheinlich, \emph{falls} die H0 gilt. Ansonsten
(wenn H0 nicht gilt) können die Daten sehr wahrscheinlich sein.

\begin{quote}
Wenn der p-Wert kleiner als 5\% ist, dann ist die Wahrscheinlichkeit der
H0 kleiner als 5\%.
\end{quote}

Falsch. Der p-Wert gibt \emph{nicht} die Wahrscheinlichkeit einer
Hypothese an. Richtig ist: Ist der p-Wert kleiner als 5\%, dann sind
meine Daten (oder noch extremere) unwahrscheinlich (\textless{}5\%),
\emph{wenn} die H0 gilt.

\begin{quote}
Wenn der p-Wert kleiner als 5\% ist, dann habe ich die Ursache eines
Phänomens gefunden.
\end{quote}

Falsch. Richtig ist: Keine Statistik kann für sich genommen eine Ursache
erkennen. Bestenfalls kann man sagen: hat man alle konkurrierenden
Ursachen ausgeschlossen \emph{und} sprechen die Daten für die Ursache
\emph{und} sind die Daten eine plausible Erklärung, so erscheint es der
beste Schluss, anzunehmen, dass man \emph{eine} Ursache gefunden hat -
im Rahmen des Geltungsbereichs einer Studie.

\begin{quote}
Wenn der p-Wert kleiner als 5\% ist, dann kann ich meine Studie
veröffentlichen.
\end{quote}

Richtig. Leider entscheidet zu oft (nur) der p-Wert über das Wohl und
Wehe einer Studie. Wichtiger wäre zu prüfen, wie ``gut'' das Modell ist
- wie präzise sind die Vorhersagen? Wie theoretisch befriedigend ist das
Modell?

\begin{quote}
Wenn der p-Wert \emph{größer} als 5\% ist, dann ist das ein Beleg
\emph{für} die H0.
\end{quote}

Falsch. Richtig ist: Ein großer p-Wert ist ein Beleg, dass die Daten
plausibel unter der H0 sind. Wenn es draußen regnet, ist es plausibel,
dass es Herbst ist. Das heißt aber nicht, dass andere Hypothesen nicht
auch plausibel sind. Ein großer p-Wert ist also Abwesenheit von klarer
Evidenz -- \emph{nicht} Anwesenheit von klarer Evidenz zugunsten der H0.
Schöner ausgedrückt: ``No evidence of effect ist not the same as
evidence of no effect''. Für die Wissenschaft ist das insofern ein
großes Problem, als dass sich Zeitschriften weigern, nicht-signifikante
Studien aufzunehmen: ``Das ist eine unklare Befundlage. Kein Mehrwert.''
so die Haltung. Das führt dazu, dass die wissenschaftliche Literatur
einer großen Verzerrung unterworfen ist.

\section{Wann welcher Inferenztest?}\label{wannwelcher}

In der Praxis ist es eine häufige Frage, wann man welchen statistischen
Test verwenden soll. Bei Eid, Gollwitzer, und Schmitt
(\protect\hyperlink{ref-eid2010statistik}{2010}) findet man eine
umfangreiche Tabelle dazu; auch online wird man schnell fündig (z.B. bei
der
\href{http://www.methodenberatung.uzh.ch/de/datenanalyse.html}{Methodenberatung
der Uni Zürich} oder beim
\href{https://www.aerzteblatt.de/archiv/74880/Auswahl-statistischer-Testverfahren}{Ärzteblatt},
Prel u.~a. (\protect\hyperlink{ref-welchertest}{2010})).

Die folgende Auflistung gibt einen \emph{kurzen} Überblick zu
gebräuchlichen Verfahren. Entscheidungskriterium ist hier (etwas
vereinfacht) das Skalenniveau der Variablen (unterschieden in Input- und
Outputvariablen).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  2 nominale Variablen: \(\chi^2\)-Test - \texttt{chisq.test}
\item
  Output: 1 metrisch, Input: 1 dichotom: t-Test - \texttt{t.test}
\item
  Output: 1 oder mehr metrisch, 1 nominal: Varianzanalyse - \texttt{aov}
\item
  2 metrische Variablen: Korrelation - \texttt{cor.test}
\item
  Output: 1 metrisch, Input: 1 oder mehr nominal oder metrisch:
  Regression - \texttt{lm}
\item
  Output: 1 ordinal, Input: 1 dichotom: Wilcoxon (Mann-Whitney-U-Test) -
  \texttt{wilcox.test}
\item
  Output: 1 ordinal, Input: 1 nominal: Kruskal-Wallis-Test -
  \texttt{kruskal.test}
\item
  1 metrisch (Test auf Normalverteilung): Shapiro-Wilk-Test -
  \texttt{shapiro.test}
\item
  Output: 1 dichotom, Input 1 oder mehr nominal oder metrisch:
  logistische (klassifikatorische) Regression:
  \texttt{glm(...,\ family\ =\ "binomial")}
\item
  2 ordinal: Spearmans Rangkorrelation -
  \texttt{cor.test(x,\ y,\ method\ =\ "spearman")}
\end{enumerate}

\section{Vertiefung: Beispiele für häufige
Inferenztests}\label{vertiefung-beispiele-fur-haufige-inferenztests}

Schauen wir uns für jeden Test aus Kapitel \ref{wannwelcher} ein
Anwendungsbeispiel an.

\subsection{\texorpdfstring{\(\chi^2\)-Test}{\textbackslash{}chi\^{}2-Test}}\label{chi2-test}

Laden wir den Datensatz \texttt{extra}. Ob es wohl einen Zusammenhang
gibt zwischen (der Anzahl von) Geschlecht und extremen Alkoholgenuss?
Definieren wir `extrem' durch mehr als 10 Kater.

\emph{Forschungsfrage: Gibt es einen Zusammenhang zwischen Geschlecht
und extremen Alkoholgenuss?}

Synonym wäre zu fragen, ob sich die Stufen von Geschlecht (die
Geschlechter, also Mann und Frau) hinsichtlich \sout{Saufen}extremen
Alkoholgenuss unterscheiden.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{extra <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"data/extra.csv"}\NormalTok{)}

\NormalTok{extra}\OperatorTok{$}\NormalTok{viel_saeufer <-}\StringTok{ }\NormalTok{extra}\OperatorTok{$}\NormalTok{n_hangover }\OperatorTok{>}\StringTok{ }\DecValTok{10}

\KeywordTok{chisq.test}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ extra}\OperatorTok{$}\NormalTok{sex, extra}\OperatorTok{$}\NormalTok{viel_saeufer)}
\CommentTok{#> }
\CommentTok{#>  Pearson's Chi-squared test with Yates' continuity correction}
\CommentTok{#> }
\CommentTok{#> data:  extra$sex and extra$viel_saeufer}
\CommentTok{#> X-squared = 30, df = 1, p-value = 4e-08}
\end{Highlighting}
\end{Shaded}

Achtung, falls Ihre Daten in aggregierter Form vorliegen, müssen Sie sie
folgendermaßen übergeben werden:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ extra}\OperatorTok{$}\NormalTok{sex, extra}\OperatorTok{$}\NormalTok{viel_saeufer) }\OperatorTok{%>%}\StringTok{ }\NormalTok{chisq.test}
\CommentTok{#> }
\CommentTok{#>  Pearson's Chi-squared test with Yates' continuity correction}
\CommentTok{#> }
\CommentTok{#> data:  .}
\CommentTok{#> X-squared = 30, df = 1, p-value = 4e-08}
\end{Highlighting}
\end{Shaded}

\subsection{t-Test}\label{t-test}

\emph{Forschungsfrage: Sind Männer im Schnitt extrovertierter als
Frauen?}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{extra }\OperatorTok{%>%}\StringTok{ }
\StringTok{ }\KeywordTok{filter}\NormalTok{(sex }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Frau"}\NormalTok{, }\StringTok{"Mann"}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{sex =} \KeywordTok{factor}\NormalTok{(.}\OperatorTok{$}\NormalTok{sex)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{t.test}\NormalTok{(extra_mean }\OperatorTok{~}\StringTok{ }\NormalTok{sex, }\DataTypeTok{data =}\NormalTok{ ., }\DataTypeTok{alternative =} \StringTok{"less"}\NormalTok{)}
\CommentTok{#> }
\CommentTok{#>  Welch Two Sample t-test}
\CommentTok{#> }
\CommentTok{#> data:  extra_mean by sex}
\CommentTok{#> t = 1, df = 500, p-value = 0.9}
\CommentTok{#> alternative hypothesis: true difference in means is less than 0}
\CommentTok{#> 95 percent confidence interval:}
\CommentTok{#>   -Inf 0.104}
\CommentTok{#> sample estimates:}
\CommentTok{#> mean in group Frau mean in group Mann }
\CommentTok{#>               2.91               2.86}
\end{Highlighting}
\end{Shaded}

Auf Deutsch liest sich der letzte Befehlsblock so:

\BeginKnitrBlock{rmdpseudocode}
Nimm den Datensatz \texttt{extra} UND DANN\\
filtere nur Zeilen heraus, in denen bei Geschlecht `Mann' oder `Frau'
steht (es gibt Zeilen mit `\,``''\,' als Wert) UND DANN\\
definiere \texttt{sex} als Faktor und zwar so, dass es nur Faktorstufen
gibt, die es auch in den Daten gibt (`Frau oder 'Mann') UND DANN führe
einen gerichteten t-Test durch mit
´extra\_mean\texttt{als\ Output-Variable\ und}sex` als
Gruppierungsvariable.
\EndKnitrBlock{rmdpseudocode}

Der Punkt \texttt{.} meint hier den Datensatz in aktueller Form, so
also, wie er aus der letzten (vorherigen) Zeile herausgekommen ist.

Hinweise:

\begin{itemize}
\tightlist
\item
  Der t-Test testet im Standard \emph{un}gerichtet.
\item
  Wird eine Gruppierungsvariable (wie Geschlecht) vom Typ
  \texttt{factor} angegeben, so muss diese 2 Faktorstufen haben. Allein
  durch filtern wird man zusätzliche Faktorstufen nicht los (im
  Gegensatz zu Variablen vom Typ \texttt{character}, Text). Man muss die
  Faktorvariable neu als Faktorvariable definieren. Dann werden nur die
  existierenden Werte als Faktorstufen herangezogen.
\item
  Bei gerichteten Hypothesen sieht \texttt{t.test} zwei Möglichkeiten
  vor: \texttt{less} und \texttt{greater}. Woher weiß man, welches von
  beiden man nehmen muss? Die Antwort lautet: Bei Textvariablen sind die
  Stufen alphabetisch geordnet. R sagt also sozusagen:
  \texttt{Frau\ ?\ Mann}. Und für \texttt{?} müssen wir das richtige
  Ungleichheitszeichen einsetzen (\textless{} oder \textgreater{}), so
  dass es unserer Hypothese entspricht. In diesem Fall glauben wir, dass
  Frauen weniger (bzw. Männer mehr) trinken, also haben wir
  \texttt{less} gewählt.
\item
  Liegt der Datensatz nicht tidy vor, also gibt es z.B. eine Spalte mit
  Extraversionswerten für Männer und eine für Frauen, so darf man
  \emph{nicht} die Formelsyntax (Kringel, Tilde ``\textasciitilde{}''``)
  nehmen, sondern benennt die Spalten mit X und Y:
  \texttt{t.text(x\ =\ df\$extra\_maenner,\ y\ =\ df\$extra\_frauen)}.
\end{itemize}

\subsection{Varianzanalyse}\label{varianzanalyse}

\emph{Forschungsfrage: Unterscheiden sich Menschen mit unterschiedlich
viel Kundenkontakt in ihrer Extraversion?}

Der Kundenkontakt wurde mit einer Likertskala gemessen, die mehrere
Stufen von ``weniger als einmal pro Quartal'' bis ``im Schnitt mehrfach
pro Tag'' reichte. Wir gehen nicht davon aus, dass diese Skala
Intervallniveau aufweist. Obwohl Ordinalskalenniveau plausibel ist,
bleiben wir bei der ANOVA (Varianzanalyse, AOV), die nur nominales
Niveau ausschöpft. Beachten Sie, dass die entsprechende Variable
\texttt{clients\_freq} als Ganzzahl in R definiert ist, obwohl die
Abstände nicht sicher gleich sind. Es ist zwar erlaubt, den Stufen einer
nominalen Variablen Zahlen zuzuordnen, aber wir sollten nicht vergessen,
dass die Zahlen ``keine echten Zahlen'' sind, also nicht metrisches
Niveau aufweisen (zumindest ist das nicht sicher).

Die verschiedenen Stufen einer Variablen kann man sich so ausgeben
lassen:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{extra }\OperatorTok{%>%}\StringTok{ }\KeywordTok{distinct}\NormalTok{(clients_freq)}
\end{Highlighting}
\end{Shaded}

Jetzt die ANOVA:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{aov}\NormalTok{(extra_mean }\OperatorTok{~}\StringTok{ }\NormalTok{sex, }\DataTypeTok{data =}\NormalTok{ extra) }\OperatorTok{%>%}\StringTok{ }\NormalTok{glance}
\CommentTok{#>   r.squared adj.r.squared sigma statistic p.value df logLik  AIC  BIC}
\CommentTok{#> 1   0.00247       0.00125  0.45      2.02   0.156  2   -505 1017 1031}
\CommentTok{#>   deviance df.residual}
\CommentTok{#> 1      165         813}
\end{Highlighting}
\end{Shaded}

\texttt{glance} räumt das Ergebnis der ANOVA etwas auf, so dass die
Ausgabe ein Dataframe ist und die ``Überblick-Koeffizienten'' (daher
`glance', engl. `Blick') ausgegeben werden. Ganz interessantes Ergebnis:
statistisch signifikant (p\textless{}.05), aber \(R^2\) ist sehr klein.
Der F-Wert ist als \texttt{stastistic} bezeichnet.

\subsection{Korrelationen auf Signifikanz
prüfen}\label{korrelationen-auf-signifikanz-prufen}

\emph{Forschungsfrage: Ist der Extraversion-Mittelwert und die Anzahl
der Facebook-Freunde korreliert?}

Der Test prüft, ob diese Korrelation 0 ist.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cor.test}\NormalTok{(extra}\OperatorTok{$}\NormalTok{extra_mean, extra}\OperatorTok{$}\NormalTok{n_facebook_friends)}
\CommentTok{#> }
\CommentTok{#>  Pearson's product-moment correlation}
\CommentTok{#> }
\CommentTok{#> data:  extra$extra_mean and extra$n_facebook_friends}
\CommentTok{#> t = 1, df = 700, p-value = 0.2}
\CommentTok{#> alternative hypothesis: true correlation is not equal to 0}
\CommentTok{#> 95 percent confidence interval:}
\CommentTok{#>  -0.0303  0.1208}
\CommentTok{#> sample estimates:}
\CommentTok{#>    cor }
\CommentTok{#> 0.0455}
\end{Highlighting}
\end{Shaded}

Man kann auch - wie beim t-Test - gerichtet testen mit der gleichen
Syntax, vgl. \texttt{?cor.test}.

\subsection{Regression}\label{regression}

\emph{Forschungsfrage: Wie groß ist der Einfluss von der Anzahl von
Parties auf die Anzahl der Kater?}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{lm}\NormalTok{(n_hangover }\OperatorTok{~}\StringTok{ }\NormalTok{n_party, }\DataTypeTok{data =}\NormalTok{ extra) }\OperatorTok{%>%}\StringTok{ }\NormalTok{tidy}
\CommentTok{#>          term estimate std.error statistic  p.value}
\CommentTok{#> 1 (Intercept)    0.159     1.401     0.113 9.10e-01}
\CommentTok{#> 2     n_party    0.539     0.054     9.983 3.62e-22}
\KeywordTok{lm}\NormalTok{(n_hangover }\OperatorTok{~}\StringTok{ }\NormalTok{n_party, }\DataTypeTok{data =}\NormalTok{ extra) }\OperatorTok{%>%}\StringTok{ }\NormalTok{glance}
\CommentTok{#>   r.squared adj.r.squared sigma statistic  p.value df logLik  AIC  BIC}
\CommentTok{#> 1     0.113         0.112  29.2      99.7 3.62e-22  2  -3752 7510 7524}
\CommentTok{#>   deviance df.residual}
\CommentTok{#> 1   665751         781}
\end{Highlighting}
\end{Shaded}

\texttt{statistic} ist bei dieser Ausgabe übrigens der F-Wert und
\texttt{sigma} die SD der Residualstreuung.
\texttt{estimate\ ist\ die\ Steigung\ der\ Regressionsgeraden}.

\subsection{Wilcoxon-Test}\label{wilcoxon-test}

\emph{Forschungsfrage: Unterscheiden sich die Geschlechter in ihrer
mittleren Extraversion?}

Hier nehmen wir nicht an, dass Extraversion metrisch ist, sondern
begnügen uns mit der schwächeren Annahme eines ordinalen Niveaus.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{extra }\OperatorTok{%>%}\StringTok{ }
\StringTok{ }\KeywordTok{filter}\NormalTok{(sex }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Frau"}\NormalTok{, }\StringTok{"Mann"}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{sex =} \KeywordTok{factor}\NormalTok{(.}\OperatorTok{$}\NormalTok{sex)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{wilcox.test}\NormalTok{(extra_mean }\OperatorTok{~}\StringTok{ }\NormalTok{sex, }\DataTypeTok{data =}\NormalTok{ .)}
\CommentTok{#> }
\CommentTok{#>  Wilcoxon rank sum test with continuity correction}
\CommentTok{#> }
\CommentTok{#> data:  extra_mean by sex}
\CommentTok{#> W = 80000, p-value = 0.4}
\CommentTok{#> alternative hypothesis: true location shift is not equal to 0}
\end{Highlighting}
\end{Shaded}

\subsection{Kruskal-Wallis-Test}\label{kruskal-wallis-test}

\emph{Forschungsfrage: Unterscheiden sich Menschen mit unterschiedlich
viel Kundenkontakt in ihrer Extraversion?}

Genau wie beim Wilcoxon-Test gehen wir wieder nur von ordinalem Niveau
bei Extraversion aus.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{extra }\OperatorTok{%>%}\StringTok{ }
\StringTok{ }\KeywordTok{filter}\NormalTok{(sex }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Frau"}\NormalTok{, }\StringTok{"Mann"}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{sex =} \KeywordTok{factor}\NormalTok{(.}\OperatorTok{$}\NormalTok{sex)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{kruskal.test}\NormalTok{(extra_mean }\OperatorTok{~}\StringTok{ }\NormalTok{sex, }\DataTypeTok{data =}\NormalTok{ .)}
\CommentTok{#> }
\CommentTok{#>  Kruskal-Wallis rank sum test}
\CommentTok{#> }
\CommentTok{#> data:  extra_mean by sex}
\CommentTok{#> Kruskal-Wallis chi-squared = 0.6, df = 1, p-value = 0.4}
\end{Highlighting}
\end{Shaded}

\subsection{Shapiro-Test}\label{shapiro-test}

\emph{Forschungsfrage: Ist Extraversion normalverteilt?}

Wahrscheinlich ist es sinnvoller, diese Frage mit einem Histogramm (oder
QQ-Plot) zu beantworten, weil der Test bei großen Stichproben (zu)
schnell signifikant wird. Aber machen wir es mal:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{shapiro.test}\NormalTok{(extra}\OperatorTok{$}\NormalTok{extra_mean)}
\CommentTok{#> }
\CommentTok{#>  Shapiro-Wilk normality test}
\CommentTok{#> }
\CommentTok{#> data:  extra$extra_mean}
\CommentTok{#> W = 1, p-value = 9e-09}
\end{Highlighting}
\end{Shaded}

Signifikant. Die Variable ist also \emph{nicht} (exakt) normalverteilt.
Böse Zungen behaupten, die Normalverteilung sei ungefähr so häufig wie
Einhörner (Micceri \protect\hyperlink{ref-Micceri1989}{1989}). Trotzdem
setzen viele Verfahren sie voraus. Glücklicherweise reicht es häufig,
wenn eine Variable \emph{einigermaßen} normalverteilt ist (wobei es hier
keine klaren Grenzen gibt).

\subsection{Logistische Regression}\label{logistische-regression}

\emph{Forschungsfrage: Kann man anhand der Extraversion vorhersagen, ob
eine Person Extremtrinker ist?}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{glm}\NormalTok{(viel_saeufer }\OperatorTok{~}\StringTok{ }\NormalTok{extra_mean, }\DataTypeTok{data =}\NormalTok{ extra, }\DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\NormalTok{tidy}
\CommentTok{#>          term estimate std.error statistic  p.value}
\CommentTok{#> 1 (Intercept)   -4.207     0.661     -6.37 1.93e-10}
\CommentTok{#> 2  extra_mean    0.942     0.218      4.33 1.51e-05}
\end{Highlighting}
\end{Shaded}

\subsection{Spearmans Korrelation}\label{spearmans-korrelation}

\emph{Forschungsfrage: Ist die Extraversion assoziiert mit der Anzahl
der Kundenbesuche?}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cor.test}\NormalTok{(extra}\OperatorTok{$}\NormalTok{extra_single_item, extra}\OperatorTok{$}\NormalTok{clients_freq, }\DataTypeTok{method =} \StringTok{"spearman"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Zur Philosophie des p-Werts:
Frequentismus}\label{zur-philosophie-des-p-werts-frequentismus}

Der p-Wert basiert auf der Idee, dass man ein Experiment
\emph{unendlich} oft wiederholen könnte (wer die Zeit hat, nicht wahr);
und das unter \emph{zufälligen} aber \emph{ansonsten komplett gleichen}
Bedingungen; das ist eine Kernidee des sog. `Frequentismus' (Neyman und
Pearson \protect\hyperlink{ref-Neyman1933}{1933}). Diese Philosophie
betrachtet Wahrscheinlichkeit als der Anteil, der sich bei unendlich
häufiger Wiederholung eines Experiments ergibt. Ein Münzwurf hingegen
ist das klassische Modell der frequentistischen Idee der
Wahrscheinlichkeit (vgl. Abb. \ref{fig:muenzwurf}). Wirft man eine faire
Münze oft, so nähert sich der relative Anteil von `Kopf' an 50\% an.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{061_Inferenzstatistik_files/figure-latex/muenzwurf-1} 

}

\caption{Anteil von 'Kopf' bei wiederholtem Münzwurf}\label{fig:muenzwurf}
\end{figure}

Ob es im Universum irgendetwas gibt, das unendlich ist, ist streitbar
(Rucker \protect\hyperlink{ref-ruckerinfinity}{2004}, Briggs
(\protect\hyperlink{ref-uncertainty}{2016})). Jedenfalls ist die
Vorstellung, das Experiment unendlich oft zu wiederholen, unrealistisch.
Inwieweit Zufälligkeit und Vergleichbarkeit hergestellt werden kann, ist
auch fragwürdig (Briggs \protect\hyperlink{ref-uncertainty}{2016}).

Die frequentistische Idee der Wahrscheinlichkeit darf Aussagen wie
dieser keine Wahrscheinlichkeit zuweisen: ``5 von 10 Marsianer trinken
gerne Bier und Schorsch ist Marsianer'' (Briggs
\protect\hyperlink{ref-uncertainty}{2016}; Neyman und Pearson
\protect\hyperlink{ref-neyman1992problem}{1992}; Neyman und Pearson
\protect\hyperlink{ref-Neyman1933}{1933}). Häufigkeitsaussagen a la
Frequentismus machen hier offenbar wenig Sinn. Trotzdem fühlen sich
manche unter uns geneigt, die Wahrscheinlichkeit, dass Schorsch der
Marsianer gern Bier trinkt, auf 50\% zu bemessen. Ein anderes, weniger
fernes Beispiel: Ich werfe eine Münze hoch, fange sie auf, verdeckt. Wie
hoch ist die Wahrscheinlichkeit, dass die Münze mit Kopf nach oben
liegt? 50\%? Moment, einzelne Ereignisse haben keine Wahrscheinlichkeit,
sagt der Frequentismus. Wer sich geneigt fühlt (wie ich), hier doch eine
Wahrscheinlichkeit zuzuordnen (50\%), der tut dies offenbar nicht auf
Basis des Frequentismus. Eine theoretische Position, die
Wahrscheinlichkeiten erlaubt, kann man als \emph{epistemologische
Wahrscheinlichkeit} bezeichnen (Briggs
\protect\hyperlink{ref-uncertainty}{2016}): Alle möglichen von \(n\)
Ergebnissen erscheinen uns gleich plausibel. Daher schließen wir, dass
die Wahrscheinlichkeit des Ereignisses \(k\) 1 durch \(n\) (\(1/n\))
beträgt.

\section{Alternativen zum p-Wert}\label{alternativen-zum-p-wert}

Eine Reihen von Alternativen (oder Ergänzungen zum p-Wert) wurden
vorgeschlagen.

\subsection{Konfidenzintervalle}\label{konfidenzintervalle}

Konfidenzintervalle\index{Konfidenzintervalle} (Zu) einfach gesagt, gibt
ein 95\%- Konfidenzintervall an, wie groß der Bereich ist, mit dem der
gesuchte Parameter zu 95\% Wahrscheinlichkeit liegt (oder allgemeiner
das \(1-\alpha\) -Konfidenzintervall. Das kennt man aus dem
Wetterbericht, wenn es heißt, dass die Höchsttemperatur morgen zwischen
20 und 24 Grad liegen werde.

Etwas genauer gesagt ist es nach den Urhebern des Konfidenzintervalls,
Neyman und Pearson, gar nicht möglich, für ein einzelnes Ereignis eine
Wahrscheinlichkeit anzugeben (Clopper und Pearson
\protect\hyperlink{ref-clopper1934use}{1934}; Neyman
\protect\hyperlink{ref-neyman1935problem}{1935}). Wenn ich eine Münze
hochwerfe und sie auffange, wie groß ist die Wahrscheinlichkeit, dass
sie auf Kopf gelandet ist? 50\%? Falsch, sagen `Frequentisten' a la
Neyman und Pearson, entweder ist die Münze auf Kopf gelandet, dann kann
man höchstens sagen, \(p(K)=1\) oder auf Zahl, dann entsprechend
\(p(Z)=1\). Eine Wahrscheinlichkeit macht nur Sinn nach diesem
Verständnis, wenn man den Versuch \emph{oft} (unendlich) wiederholt.
Daher lautet eine genauere Definition:

\begin{quote}
Das 95\%-Konfidenzintervall ist der Bereich, in dem der Parameter in
95\% der Fälle fallen würde bei sehr häufiger Wiederholung des Versuchs.
\end{quote}

Mit Parameter ist hier der Mittelwert der Population gemeint (auch
bezeichnet als `wahrer Mittelwert'). Das Konfidenzintervall macht also
Aussagen zur \emph{über ein Verfahren} (einen Bereich berechnen auf
Basis von Stichprobendaten), \emph{nicht über den wahren Mittelwert}.

Hier findet sich eine schöne
\href{http://rpsychologist.com/d3/CI/}{Visualisierung zum
Konfidenzintervall}.

Genau wie der p-Wert werden Konfidenzintervalle häufig missverstanden
(sie sind Blutsbrüder im Geiste). Die Studie von Hoekstra, Morey, Rouder
und Wagenmakerks (\protect\hyperlink{ref-hoekstra2014robust}{2014})
zeigt das auf amüsante Weise. In der Studie legten die Autoren einigen
Studenten und Wissenschaftlern sechs Fragen zum
Wissens-Konfidenzintervall vor, die beantwortet werden sollten. Es wurde
ein Kontext vorgestellt, etwa so ``Professor Bumbledorf führt ein
Experiment durch. Das Ergebnis fasst er in einem 95\%-Konfidenzintervall
für den Mittelwert zusammen, welches von 0,1 bis 0,4 reicht''. Dann
folgten sechs Aussagen, die mit \emph{stimmt} oder \emph{stimmt nicht}
zu beantworten waren. Beurteilen auch Sie diese Aussagen\footnote{alle
  sechs sind falsch}.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Die Wahrscheinlichkeit, dass der wahre Mittelwert größer als 0 ist,
  liegt bei mindestens 95\%.
\item
  Die Wahrscheinlichkeit, dass der wahre Mittelwert gleich 0 ist, ist
  kleiner als 5\%.
\item
  Die Nullhypothese, dass der wahre Mittelwert 0 ist, ist wahrscheinlich
  falsch.
\item
  Die Wahrscheinlichkeit, dass der wahre Mittelwert zwischen 0,1 und 0,4
  liegt, beträgt 95\%.
\item
  Wir können zu 95\% sicher sein, dass der wahre Mittelwert zwischen 0,1
  und 0,4 liegt.
\item
  Wenn wir das Experiment immer wieder wiederholen würden, dann würde
  der wahre Mittelwert in 95\% der Fälle zwischen 0,1 und 0,4 fallen.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Aussagen 1, 2, 3 und 4 behaupten, der Hypothese bzw. dem Parameter eine
Wahrscheinlichkeit zuweisen zu können. Innerhalb des NHST ist das nicht
erlaubt, für da Konfidenzintervall sowenig wie für den p-Wert. Aussagen
5 trifft eine Aussage über den wahren Wert, aber Konfidenzintervalle
treffen Aussagen über ein Verfahren. Aussage 6 behauptet, dass der wahre
Wert variieren könne, tut der aber nicht. Die richtige Aussage, die
nicht dabei stand, ist: ``Wenn man den Versuch immer wiederholen würden,
würden 95\% der Intervalle den wahren Mittelwert enthalten''. Im Schnitt
wurden etwa 3,5 Antworten mit \emph{stimmt} angekreuzt (die
Wissenschaftler waren nicht besser als die Studenten).

\subsection{Effektstärke}\label{effektstarke}

Eine weitere Alternative sind Maße der
\emph{Effektstärke}\index{Effektstärke} (Cohen
\protect\hyperlink{ref-Cohen1992}{1992}). Effektstärkemaße geben an, wie
sehr sich zwei Parameter unterscheiden: ``Deutsche Männer sind im
Schnitt 13cm größer als Frauen'' (Wikipedia
\protect\hyperlink{ref-wiki:groesse}{2017}). Oder: ``In Deutschland ist
die Korrelation von Gewicht und Größe um 0,12 Punkte höher als in den
USA'' (frei erfunden). Im Gegensatz zu p-Werten wird keine Art von
Wahrscheinlichkeitsaussage angestrebt, sondern die Größe von
Parameter(unterschieden) quantifiziert. Effektstärken sind, im Gegensatz
zum p-Wert, auch nicht abhängig von der Stichprobengröße. Man kann
Effektstärken in nicht-standardisierte (wie Unterschiede in der Größe)
oder standardisierte (wie Unterschiede in der Korrelation) einteilen.

Nicht-standardisierte Effektstärken haben den Vorteil der
Anschaulichkeit. Standardisierte Effektgrößen sind präziser, aber
unanschaulicher. Bei Variablen mit unanschaulichen Metriken (wie
psychologische Variablen und Umfragen) ist ein standardisiertes Maß
häufig nützlicher.

\begin{quote}
Anschauliche Variablen sind oft mit unstandardisiertes Effektstärken
adäquat dargestellt. Variablen mit wenig anschaulichen Metriken
profitieren von standardisierten Effektstärkemaßen.
\end{quote}

Um zwei Mittelwerte zu vergleichen, ist \emph{Cohens d}\index{Cohens d}
gebräuchlich. Es gibt den Unterschied der Mittelwert standardisiert an
der Standardabweichung an (Cohen
\protect\hyperlink{ref-cohen_statistical_1988}{1988}). Das ist oft
sinnvoll, denn 5\$ Preisunterschied können viel oder weniger sein: Bei
Eiskugeln wäre der Unterschied enorm (die Streuung ist viel weniger als
5\euro{}), bei Sportwagen wäre der Unterschied gering (die Streuung ist
viel höher als 5\euro{}).

\subsubsection{Typische
Effektstärkemaße}\label{typische-effektstarkemae}

Zu den typischen Effektstärkemaßen zählen die folgenden (vgl. Eid,
Gollwitzer, und Schmitt
(\protect\hyperlink{ref-eid2010statistik}{2010})):

\begin{itemize}
\tightlist
\item
  \(d\) (Cohens d) wird zur Bemessung des Unterschieds der Überlappung
  zweier Verteilungen verwendet, z.B. um die Effektstärke eines t-Werts
  zu quantifizieren. \(d\) berechnet sich im einfachsten Fall als:
  \(d = \frac{\mu_1 - \mu_2}{sd}\).
\item
  \(r\) Pearsons R ist ein Klassiker, um die Stärke des linearen
  Zusammenhangs zweier metrischen Größen zu quantifizieren. \(r\)
  berechnet sich als: \(r = mw(\sum z_x z_y)\), wobei \(mw\) für den
  Mittelwert steht und \(z\) für einen z-Wert.
\item
  \(R^2\), \(\eta^2\) sind Maße für den Anteil aufgeklärter Varianz; sie
  finden in der Varianzanalyse oder der Regressionsanalyse Verwendung.
  \(R^2\) wird u.a. so berechnet: \(R^2 = \frac{QS_F}{QS_T}\), wobei
  \(QS\) für die Quadratsummen stehen und \(QS_F\) für die Varianz, die
  auf den Faktor (uanbhängige Variable) zurückgeht und \(QS_T\) für die
  Gesamtvarianz.
\item
  \(f^2\) ist ein Maß, dass aus der erklärten Varianz abgeleitet ist. Es
  gibt das Verhältnis von erklärter zu nicht-erklärter Varianz wieder
  (auch `signal-noise-ratio' genannt). Es berechnet sich als
  \(f^2 = \frac{R^2}{1-R^2}\).
\item
  \(\omega\) (Cohens Omega) ist ein Maß für die Stärke des Zusammenhangs
  zweier nominaler Variablen, abgeleitet vom \(\chi^2\)-Test. Es
  berechnet sich als \(\omega = \sqrt{\chi^2}\).
\item
  \(OR\) (Odds Ratio) ist ebenfalls ein Maß für die Stärke des
  Zusammenhangs zweier nominaler Variablen, allerdings \emph{binärer}
  (zweistufige) Variablen. Es berechnet sich als \(OR = \frac{c}{1-c}\),
  wobei \(c\) die Chancen für ein Ereignis \(E\) angeben (z.B. 9:1).
  \(OR\) kann aus \(\omega\) abgleitet werden.
\end{itemize}

Tabelle \ref{tab:effectsizes} gibt einen groben Überblick über
Effektstärken (nach Cohen
(\protect\hyperlink{ref-cohen_statistical_1988}{1988}) und Eid, Schmitt
und Gollwitzer (\protect\hyperlink{ref-eid2010statistik}{2010}). Zu
beachten ist, dass die Einschätzung was ein `großer' oder `kleiner'
Effekt ist, nicht pauschal übers Knie gebrochen werden sollte. Besser
ist es, die Höhe der Effektstärke im eigenen Datensatz mit relevanten
anderen Datensätzen zu vergleichen.

\begin{table}

\caption{\label{tab:effectsizes}Überblick über gängige Effektstärkemaße}
\centering
\begin{tabular}[t]{l|l|l|l}
\hline
Name & kleiner Effekt & mittlerer Effekt & großer Effekt\\
\hline
Cohens d & .2-.5 & .5-.8 & >.8\\
\hline
r & 0.1 & 0.3 & 0.5\\
\hline
\$R\textasciicircum{}2\$, \$\textbackslash{}eta\textasciicircum{}2\$ & 0.01 & 0.06 & 0.14\\
\hline
\$f\textasciicircum{}2\$ & 0.02 & 0.15 & 0.35\\
\hline
\$\textbackslash{}omega\$ & 0.1 & 0.3 & 0.5\\
\hline
OR & 1.5 & 3 & 9\\
\hline
\end{tabular}
\end{table}

\begin{quote}
Was ein ``kleiner'' oder ``großer'' Effekt ist, sollte im Einzelfall
entschieden werden.
\end{quote}

Mit dem Paket \texttt{pwr} kann man sich Cohens Konventionen der
Effektstärkehöhen in Erinnerung rufen lassen. Er bietet folgende
Optionen:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cohen.ES}\NormalTok{(}\DataTypeTok{test =} \KeywordTok{c}\NormalTok{(}\StringTok{"p"}\NormalTok{, }\StringTok{"t"}\NormalTok{, }\StringTok{"r"}\NormalTok{, }\StringTok{"anov"}\NormalTok{, }\StringTok{"chisq"}\NormalTok{, }\StringTok{"f2"}\NormalTok{),}
    \DataTypeTok{size =} \KeywordTok{c}\NormalTok{(}\StringTok{"small"}\NormalTok{, }\StringTok{"medium"}\NormalTok{, }\StringTok{"large"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\subsubsection{Effektstärken berechnen}\label{effektstarken-berechnen}

Möchte man sich Effektstärken berechnen lassen, ist das Paket
\texttt{compute.es} hilfreich. Im Folgenden sind
Effektstärkeberechnungen für gängige Inferenztests vorgestellt, in
Fortsetzung zu den Beispielen oben.

\begin{itemize}
\tightlist
\item
  \(\chi^2\)-Test: \texttt{compute.es::chies(30,\ n\ \ =\ 826)}
\item
  t-Test: \texttt{compute.es::tes(t\ =\ 1,\ n.1\ =\ 529,\ n.2\ =\ 286)}
\item
  Varianzanalyse (F-Test): \texttt{glance} gibt \(R^2\) aus; mit
  \texttt{etaSquared(mein\_aov)} ebenfalls. Möchte man \(f^2\)
  berechnen, so tut man das am besten per Hand, z.B.
  \texttt{0.002\ /\ (1-0.002)}.
\item
  Korrelation: Der Korrelationswert \(r\) ist schon ein Maß der
  Effekstärke. Yeah.
\item
  Regression: Die Steigung der Regressionsgeraden (\(b\)) ist ein
  (unstandardisiertes) Maß für die Stärke des (``Netto''-)Einflusses
  eines Prädiktors. \(R^2\) hingegen ein Maß für die relative
  Varianzaufklärung aller Prädiktoren gemeinsam.
\item
  Logistische Regression: Mit
  \texttt{BaylorEdPsych::PseudoR2(mein\_glm\_objekt)} kann man eine Art
  \(R^2\) bekommen (s. Kapitel \ref{Modellguete}).
\item
  Wilcoxon-Test/ Mann-Whitney-U-Test: Anteil der paarweisen Vergleiche,
  die hypothesenkonform sind (vgl. Kerby
  \protect\hyperlink{ref-Kerby2014}{2014}). Dazu kann man z.B. die
  Funktion \texttt{prop\_fav} aus dem Paket \texttt{prada} nutzen (vgl.
  \texttt{help(prop\_fav)}).
\end{itemize}

\subsection{Bayes-Statistik}\label{bayes-statistik}

Bayes' Ansatz verrechnet zwei Komponenten, um die Wahrscheinlichkeit
einer Hypothese im Lichte bestimmter Daten zu berechnen. Der Ansatz ist
elegant, mathematisch lupenrein und ist überhaupt eine tolle Sache.
Bayes' Theorem gibt uns das, was uns eigentlich interessiert: Die
Wahrscheinlichkeit der getesteten Hypothese, im Lichte der vorliegenden
Daten: \(p(H|D)\). Diesen Wert nennt man auch den \emph{Vorhersagewert}.
Zur Erinnerung: Der p-Wert gibt die Wahrscheinlichkeit der Daten an,
unter Annahme der getesteten Hypothese: \(p(D|H)\). Offenbar sind beide
Terme nicht identisch.

Die Bayes-Statistik zieht zwei Komponenten zur Berechnung von \(p(H|D)\)
heran. Zum einen die Grundrate einer Hypothese \(p(H)\) zum anderen die
relative Plausibilität der Daten unter meiner Hypothese im Vergleich zur
Plausibilität der Daten unter konkurrierenden Hypothesen. Betrachten wir
ein Beispiel. Die Hypothese ``Ich bin krank'' sei unter Betrachtung
(jetzt noch keine vorschnellen Einschätzungen). Die Grundrate der
fraglichen Krankheit sei 10 von 1000 (1\%). Der Test, der zur Diagnose
der Krankheit verwendet wird, habe eine Sicherheit von 90\%. Von 100
Kranken wird der Test demnach 90 identifizieren (auch
\emph{Sensitivität} genannt) und 10 werden übersehen (ein Überseh- oder
\emph{Betafehler} von 10\%). Umgekehrt wird der Test von 100 Gesunden
wiederum 90 als Gesund, und demnach korrekt diagnostizieren
(\emph{Spezifität}); 10 werden fälschlich als krank einschätzt
(\emph{Fehlalarm} oder \emph{Alpha-Fehler}).

Jetzt Achtung: Der Test sagt, ich sei krank. Die Gretchen-Frage lautet,
wie hoch ist die Wahrscheinlichkeit, dass diese Hypothese, basierend auf
den vorliegenden Daten, korrekt ist?

Abbildung \ref{fig:bayes} stellt das Beispiel in Form eines
Baumdiagrammes dar.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/inferenz/bayes-crop} 

}

\caption{Die zwei Stufen der Bayes-Statistik in einem einfachen Beispieli}\label{fig:bayes}
\end{figure}

In der Medizin ist `positiv' zumeist eine schlechte Nachricht, es soll
sagen, dass der Test der Meinung ist, die getestete Person ist krank
(das getestete Kriterium trifft zu).

Wie man leicht nachrechnen kann, beträgt die Wahrscheinlichkeit,
\emph{in Wirklichkeit krank} zu sein, wenn der positiv ist,
\textasciitilde{}8\%: \(9 / (99+9) = \frac{9}{108} \approx 8\%\). Das
überrascht auf den ersten Blick, ist doch der Test so überaus zufällig
(jedenfalls zu 90\%)! Aber die Wahrscheinlichkeit, dass die Hypothese
`krank' zutrifft, ist eben nicht nur abhängig von der Sicherheit des
Tests, sondern auch von der Grundrate. Beide Komponenten sind nötig, um
den Vorhersagewert zu berechnen. Der p-Wert begnügt sich mit der
Aussage, ob der Test positiv oder negativ ist. Die Grundrate wird nicht
berücksichtigt.

\begin{quote}
Die Bayes-Statistik liefert die Wahrscheinlichkeit einer Hypothese H,
wenn wir die Daten D (d.h. ein gewisses Stichprobenergebnis) gefunden
habe: p(H\textbar{}D). Damit gibt die Bayes-Statistik die Antwort, die
sich die meisten Anwedner wünschen.
\end{quote}

Fairerweise muss man hinzufügen, dass die Grundrate für die Wissenschaft
oft nicht einfach zu bestimmen ist. Wer kennt schon die Grundrate der
`guten Ideen'? Vielleicht der liebe Gott, aber
\href{https://twitter.com/TheTweetOfGod/status/688035049187454976}{der
hilft uns nicht} (God \protect\hyperlink{ref-god_i_2016}{2016}). Wir
werden also eine Einschätzung treffen müssen, die subjektiv sein kann.
Diese Subjektivität ist von Kritikern moniert worden.

Auf der anderen Seite kann man diese Subjektivität umgehen, indem man
nur angibt, um welchen Faktor die H1 wahrscheinlicher ist als die H0,
durch die Daten der Studie. Das wird durch den sog. \emph{Bayes-Faktor}
\(BF\) ausgedrückt. Liegt \(BF\) bei 10, so eine gängige Konvention, so
ist dies ``starke'' Evidenz für H1 (da H1 dann 10 mal wahrscheinlicher
als die H0); entsprechend stark ist ein \(BF\) von 0.1 (1/10) -
zugunsten H0. Gängige Software (s. Abschnitt \ref{verweise-inferenz})
geben den Bayes-Faktor aus.

Ein t-Test a la Bayes kann z.B. so berechnet werden:

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{extra }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(sex) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\KeywordTok{mean}\NormalTok{(extra_mean, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{))}
\CommentTok{#> # A tibble: 3 x 2}
\CommentTok{#>      sex `mean(extra_mean, na.rm = TRUE)`}
\CommentTok{#>   <fctr>                            <dbl>}
\CommentTok{#> 1   Frau                             2.91}
\CommentTok{#> 2   Mann                             2.86}
\CommentTok{#> 3     NA                             2.73}

\NormalTok{extra }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(sex }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Mann"}\NormalTok{, }\StringTok{"Frau"}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{sex =} \KeywordTok{factor}\NormalTok{(sex)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{as.data.frame }\OperatorTok{%>%}\StringTok{  }\CommentTok{# 'ttestBF' verkraftet nur althergebrachte data.frames!}
\StringTok{  }\KeywordTok{ttestBF}\NormalTok{(}\DataTypeTok{formula =}\NormalTok{ extra_mean }\OperatorTok{~}\StringTok{ }\NormalTok{sex,}
        \DataTypeTok{data =}\NormalTok{ .)  }\CommentTok{# 'formula' musst hingeschrieben sein, sonst droht Fehlermeldung}
\CommentTok{#> Bayes factor analysis}
\CommentTok{#> --------------}
\CommentTok{#> [1] Alt., r=0.707 : 0.22 ±0%}
\CommentTok{#> }
\CommentTok{#> Against denominator:}
\CommentTok{#>   Null, mu1-mu2 = 0 }
\CommentTok{#> ---}
\CommentTok{#> Bayes factor type: BFindepSample, JZS}
\end{Highlighting}
\end{Shaded}

Hey, Sie haben gerade einen Bayes-Test gerechnet! Wow! Das Ergebnis
zeigt einen \(BF\) von 0.24; Evidenz \emph{zugunsten} der H0. Nicht
stark; sondern schwach. Keine überzeugende Evidenz für H1. Man beachte,
dass der Befehl hier ``indifferent'' gegenüber der H0 und der H1 war. A
priori wurden hier beide Hypothesen als gleich wahrscheinlich angesehen.
Jetzt ist unsere Überzeugung für die H1 gesunken bzw. für die H0
gestiegen und zwar etwa um den Faktor 1/4 auf 0.24.

\section[Aufgaben]{\texorpdfstring{Aufgaben\footnote{F, F, R, F, F, F,
  F, R, R, R}}{Aufgaben}}\label{aufgaben-11}

\BeginKnitrBlock{rmdexercises}
Richtig oder Falsch!?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Der p-Wert gibt die Wahrscheinlichkeit der H0 an unter der Annahme der
  Daten.
\item
  p(D\textbar{}H) = p(H\textbar{}D)
\item
  Der p-Wert sagt, wie gut die Daten zur Nullhypothese passen.
\item
  Bei sehr großen Stichproben werden nur sehr große Effekte signifikant.
\item
  Egal wie klein die Effektstärke ist, es existiert eine
  Stichprobengröße, die diesen Effekt beliebig signifikant werden lässt.
\item
  Wenn der p-Wert kleiner als 5\% ist, dann ist meine Hypothese (H1)
  höchstwahrscheinlich richtig.
\item
  Wenn der p-Wert größer als 5\% ist, dann ist das ein Beleg für die H0.
\item
  Der p-Wert basiert auf der Idee, dass man ein Experiment unendlich oft
  wiederholt; und das unter zufälligen aber ansonsten komplett gleichen
  Bedingungen.
\item
  Das 95\%-Konfidenzintervall ist der Bereich, in dem der Parameter in
  95\% der Fälle fallen würde bei sehr häufiger Wiederholung des
  Versuchs.
\item
  Der Vorhersagewert ist definiert als p(H\textbar{}D).
\end{enumerate}
\EndKnitrBlock{rmdexercises}

\section{Fazit}\label{fazit}

Der p-Wert ist eine häufig verwendete Methode, um datenbasiert zu
entscheiden, ob man eine Hypothese annimmt oder nicht. Allerdings hat
der p-Wert auch seine Probleme.

\begin{quote}
Der p-Wert sollte nicht als einziges Kriterium verwendet werden, um eine
Hypothese bzw. ein Modell zu beurteilen.
\end{quote}

Da der p-Wert aber immer noch der Platzhirsch auf vielen Forschungsauen
ist, führt kein Weg um ihn herum. Er muss genau verstanden werden: Was
er sagt und - wichtiger noch - was er nicht sagt.

Alternativen zum p-Wert sind

\begin{itemize}
\tightlist
\item
  Konfidenzintervalle
\item
  Effektstärkemaße inkl. Maße der Vorhersagegenauigkeit
\item
  Bayes-Theorem
\end{itemize}

\begin{center}\includegraphics[width=0.3\linewidth]{images/inferenz/meme_pwert_1iw22a_pvalue_dino} \end{center}

\section{Verweise}\label{verweise-inferenz}

\begin{itemize}
\item
  Eine Einführung zur Bayes-Statistik findet man z.B. bei Kruschke
  (\protect\hyperlink{ref-kruschke2010bayesian}{2010}) oder bei Etz
  u.~a. (\protect\hyperlink{ref-etz2016become}{2016}).
\item
  Eine ausführliche Darstellung der Inferenzstatistik und des p-Werts
  findet sich z.B. bei Lübke und Vogt
  (\protect\hyperlink{ref-lubke2014angewandte}{2014}) oder Eid,
  Gollwitzer, und Schmitt
  (\protect\hyperlink{ref-eid2010statistik}{2010}).
\item
  Eine vielversprechende, noch recht neue Software ist \href{}{JASP},
  die nicht nur schöne Diagramme erstellt, sondern auch auf Mausklick
  eine Reihe von bayesianischer (und frequentistischer) Tests
  durchrechnet.
\end{itemize}

\part{Geleitetes Modellieren}

\chapter{Lineare Regression}\label{lineare-regression}

\begin{center}\includegraphics[width=0.3\linewidth]{images/FOM} \end{center}

\begin{center}\includegraphics[width=0.1\linewidth]{images/licence} \end{center}

\BeginKnitrBlock{rmdcaution}
Lernziele:

\begin{itemize}
\tightlist
\item
  Wissen, was man unter Regression versteht.
\item
  Die Annahmen der Regression überprüfen können.
\item
  Regression mit kategorialen Prädiktoren durchführen können.
\item
  Die Modellgüte bei der Regression bestimmen können.
\item
  Interaktionen erkennen und ihre Stärke einschätzen können.
\end{itemize}
\EndKnitrBlock{rmdcaution}

Für dieses Kapitel benötigen Sie folgende Pakete:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(caret)  }\CommentTok{# Modellieren}
\KeywordTok{library}\NormalTok{(tidyverse)  }\CommentTok{# Datenjudo, Visualisierung,...}
\KeywordTok{library}\NormalTok{(gridExtra)  }\CommentTok{# Mehrere Plots kombinieren}
\KeywordTok{library}\NormalTok{(modelr)  }\CommentTok{# Residuen und Schätzwerte zum Datensatz hinzufügen}
\KeywordTok{library}\NormalTok{(broom)  }\CommentTok{# Regressionswerte geordnet ausgeben lassen}
\end{Highlighting}
\end{Shaded}

\section{Die Idee der klassischen
Regression}\label{die-idee-der-klassischen-regression}

Regression\index{Regression} ist eine bestimmte Art der
\emph{Modellierung} von Daten. Wir legen eine Gerade `schön mittig' in
die Daten; damit haben wir ein einfaches Modell der Daten (vgl. Abb.
\ref{fig:bsp-regression}). Die Gerade `erklärt' die Daten: Für jeden
X-Wert liefert sie einen Y-Wert als Vorhersage zurück.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"data/stats_test.csv"}\NormalTok{)}

\NormalTok{stats_test }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{ggplot }\OperatorTok{+}
\StringTok{  }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ study_time, }\DataTypeTok{y =}\NormalTok{ score) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_jitter}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_abline}\NormalTok{(}\DataTypeTok{intercept =} \DecValTok{24}\NormalTok{, }
              \DataTypeTok{slope =} \FloatTok{2.3}\NormalTok{, }
              \DataTypeTok{color =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{071_Regression_files/figure-latex/bsp-regression-1} 

}

\caption{Beispiel für eine Regression}\label{fig:bsp-regression}
\end{figure}

Wie wir genau die Regressionsgerade berechnet haben, dazu gleich mehr.
Fürs Erste begnügen wir uns mit der etwas groberen Beobachtung, dass die
Gerade `schön mittig' in der Punktewolke liegt.

Schauen wir uns zunächst die Syntax genauer an.

\BeginKnitrBlock{rmdpseudocode}
Lade die CSV-Datei mit den Daten als \texttt{stats\_test}.

Nehme \texttt{stats\_test} UND DANN\ldots{}\\
starte ein neues Diagramm mit ggplot UND\\
definiere das Diagramm (X-Achse, Y-Achse) UND DANN\\
zeichne das Geom ``Jitter'' (verwackeltes Punktediagramm) UND DANN\\
und zeichne danach eine Gerade (``abline'' in rot).
\EndKnitrBlock{rmdpseudocode}

Eine Regression zeigt anhand einer Regressionsgeraden einen ``Trend'' in
den Daten an (s. weitere Beispiele in Abb. \ref{fig:bsp-regression2}).

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{071_Regression_files/figure-latex/bsp-regression2-1} 

}

\caption{Zwei weitere Beispiele für Regressionen}\label{fig:bsp-regression2}
\end{figure}

Eine Regression lädt förmlich dazu ein, Vorhersagen zu treffen: Hat man
erstmal eine Gerade, so kann man für jeden X-Wert (``Prädiktor'') eine
Vorhersage für den Y-Wert (``Kriterium'') treffen. Anhand des Diagramms
kann man also für jede Person (d.h. jeden Wert innerhalb des
Wertebereichs von \texttt{study\_time} oder einem anderen Prädiktor)
einen Wert für \texttt{score} vorhersagen. Wie gut die Vorhersage ist,
steht erstmal auf einen anderen Blatt.

Man beachte, dass eine Gerade über ihre \emph{Steigung} und ihren
\emph{Achsenabschnitt} festgelegt ist; in Abb. \ref{fig:bsp-regression}
ist die Steigung 2.3 und der Achsenabschnitt 24. Der Achsenabschnitt
zeigt also an, wie viele Klausurpunkte man ``bekommt'', wenn man gar
nicht lernt (Gott bewahre); die Steigung gibt eine Art ``Wechselkurs''
an: Wie viele Klausurpunkte bekomme ich pro Stunde, die ich lerne.

Unser Modell ist übrigens einfach gehalten: Man könnte argumentieren,
dass der Zusatznutzen der 393. Stunde lernen geringer ist als der
Zusatznutzen der ersten paar Stunden. Aber dann müssten wir anstelle der
Gerade eine andere Funktion nutzen, um die Daten zu modellieren. Lassen
wir es erst einmal einfach hier.

Als ``Pseudo-R-Formel'' ausgedrückt:

\begin{verbatim}
score = achsenabschnitt + steigung*study_time
\end{verbatim}

Die Vorhersage für die Klausurpunkte (\texttt{score}) einer Person sind
der Wert des Achsenabschnitts plus das Produkt aus der Anzahl der
gelernten Stunden mal den Zusatznutzen pro gelernter Stunde.

Aber wie erkannt man, ob eine Regression ``gut'' ist - die Vorhersagen
also präzise?

In R kann man eine Regression so berechnen:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{study_time, }\DataTypeTok{data =}\NormalTok{ stats_test)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> lm(formula = score ~ study_time, data = stats_test)}
\CommentTok{#> }
\CommentTok{#> Coefficients:}
\CommentTok{#> (Intercept)   study_time  }
\CommentTok{#>       23.98         2.26}
\end{Highlighting}
\end{Shaded}

\texttt{lm} steht dabei für ``lineares Modell''; allgemeiner gesprochen
lautet die Rechtschreibung für diesen Befehl:

\begin{verbatim}
lm(kriterium ~ praediktor, data = meine_datentabelle)
\end{verbatim}

Um ausführlichere Informationen über das Regressionsmodell zu bekommen,
kann man die Funktion \texttt{broom::tidy} nutzen:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mein_lm <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(kriterium }\OperatorTok{~}\StringTok{ }\NormalTok{praediktor, }\DataTypeTok{data =}\NormalTok{ meine_datentabelle)}
\KeywordTok{tidy}\NormalTok{(mein_lm)}
\end{Highlighting}
\end{Shaded}

Natürlich kann das auch \sout{in der Pfeife rauchen} mit der Pfeife
darstellen:

\begin{verbatim}
lm(kriterium ~ praediktor, data = meine_datentabelle) %>% 
  summary
\end{verbatim}

\section{Vorhersagegüte}\label{vorhersagegute}

Der einfache Grundsatz lautet: Je geringer die Vorhersagefehler, desto
besser; Abb. \ref{fig:resids-plot} zeigt ein Regressionsmodell mit wenig
Vorhersagefehler (links) und ein Regressionsmodell mit viel
Vorhersagefehler (rechts).

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{071_Regression_files/figure-latex/resids-plot-1} 

}

\caption{Geringer (links) vs. hoher (rechts) Vorhersagefehler}\label{fig:resids-plot}
\end{figure}

In einem Regressionsmodell lautet die grundlegenden Überlegung zur
Modellgüte damit:

\begin{quote}
Wie groß ist der Unterschied zwischen Vorhersage und Wirklichkeit?
\end{quote}

Die Größe des Unterschieds (Differenz, ``Delta'') zwischen
vorhergesagten (geschätzten) Wert und Wirklichkeit, bezeichnet man als
\emph{Fehler}, \emph{Residuum} oder Vohersagefehler, häufig mit
\(\epsilon\) (griechisches e wie ``error'') abgekürzt.

Betrachten Sie die beiden Plots in Abb. \ref{fig:resids-plot}. Die rote
Linie gibt die \emph{vorhergesagten} (geschätzten) Werte wieder; die
Punkte die \emph{beobachteten} (``echten'') Werte. Je länger die blauen
Linien, desto größer die Vorhersagefehler. Je größer der
Vorhersagefehler, desto schlechter. Und umgekehrt.

\begin{quote}
Je kürzer die typische ``Abweichungslinie'', desto besser die Vohersage.
\end{quote}

Sagt mein Modell voraus, dass Ihre Schuhgröße 49 ist, aber in Wahrheit
liegt sie bei 39, so werden Sie dieses Modell als schlecht beurteilen,
wahrscheinlich.

Leider ist es nicht immer einfach zu sagen, wie groß der Fehler sein
muss, damit das Modell als ``gut'' bzw. ``schlecht'' gilt. Man kann
argumentieren, dass es keine wissenschaftliche Frage sei, wie viel
``viel'' oder ``genug'' ist (Briggs
\protect\hyperlink{ref-uncertainty}{2016}). Das ist zwar plausibel,
hilft aber nicht, wenn ich eine Entscheidung treffen muss. Stellen Sie
sich vor: Ich zwinge Sie mit der Pistole auf der Brust, meine Schuhgröße
zu schätzen.

Eine einfache Lösung ist, das beste Modell unter mehreren Kandidaten zu
wählen.

Ein anderer Ansatz ist, die Vorhersage in Bezug zu einem Kriterium zu
setzen. Dieses ``andere Kriterium'' könnte sein ``einfach die Schuhgröße
raten''. Oder, etwas intelligenter, Sie schätzen meine Schuhgröße auf
einen Wert, der eine gewisse Plausibilität hat, also z.B. die
durchschnittliche Schuhgröße des deutschen Mannes. Auf dieser Basis kann
man dann quantifizieren, ob und wie viel besser man als dieses
Referenzkriterium ist.

\subsection{Mittlere Quadratfehler}\label{mittlere-quadratfehler}

Eine der häufigsten Gütekennzahlen ist der \emph{mittlere quadrierte
Fehler} (engl. ``mean squared error'', MSE), wobei Fehler wieder als
Differenz zwischen Vorhersage (\texttt{pred}) und beobachtete
Wirklichkeit (\texttt{obs}, \texttt{y}) definiert ist. Dieser berechnet
für jede Beobachtung den Fehler, quadriert diesen Fehler und bilden dann
den Mittelwert dieser ``Quadratfehler'', also einen \emph{mittleren
Quadratfehler}. Die englische Abkürzung \emph{MSE} ist auch im Deutschen
gebräuchlich.

\[ MSE = \frac{1}{n} \sum{(pred - obs)^2} \]

Konzeptionell ist dieses Maß an die Varianz angelehnt. Zieht man aus
diesem Maß die Wurzel, so erhält man den sog. \emph{root mean square
error} (RMSE), welchen man sich als die Standardabweichung der
Vorhersagefehler vorstellen kann. In Pseudo-R-Syntax:

\begin{verbatim}
RMSE <- sqrt(mean((df$pred - df$obs)^2))
\end{verbatim}

Der RMSE hat die selben Einheiten wie die zu schätzende Variable, also
z.B. Schuhgrößen-Nummern.

\subsection{\texorpdfstring{R-Quadrat
(\(R^2\))}{R-Quadrat (R\^{}2)}}\label{r-quadrat-r2}

\(R^2\), auch \emph{Bestimmtheitsmaß}\index{Bestimmtheitsmaß} oder
\emph{Determinationskoeffizient}\index{Determinationskoeffizient}
genannt, setzt die Höhe unseres
Vorhersagefehlers\index{Vorhersagefehler} im Verhältnis zum
Vorhersagefehler eines ``Nullmodell''. Das Nullmodell hier würde sagen,
wenn es sprechen könnte: ``Keine Ahnung, was ich schätzen soll, mich
interessieren auch keine Prädiktoren, ich schätzen einfach immer den
Mittelwert der Grundgesamtheit!''.

Analog zum Nullmodell-Fehler spricht auch von der Gesamtvarianz oder
\(SS_T\) (sum of squares total); beim Vorhersagefehler des eigentlichen
Modells spricht man auch von \(SS_M\) (sum of squares model).

Damit gibt \(R^2\) an, wie gut unsere Vorhersagen im Verhältnis zu den
Vorhersagen des Nullmodells sind. Ein \(R^2\) von 25\% (0.25) hieße,
dass unser Vorhersagefehler 25\% \emph{kleiner} ist als der der
Nullmodells. Ein \(R^2\) von 100\% (1) heißt also, dass wir den
kompletten Fehler reduziert haben (Null Fehler übrig) - eine perfekte
Vorhersage. Etwas formaler, kann man \(R^2\) so definieren:

\[ R^2 = 1 - \left( \frac{SS_T - SS_M}{SS_T} \right)\]

Präziser, in R-Syntax:

\begin{verbatim}
R2 <- 1 - sum((df$pred - df$obs)^2) / sum((mean(df$obs) - df$obs)^2)
\end{verbatim}

Praktischerweise gibt es einige R-Pakete, z.B. \texttt{caret}, die diese
Berechnung für uns besorgen:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{postResample}\NormalTok{(}\DataTypeTok{obs =}\NormalTok{ obs, }\DataTypeTok{pred =}\NormalTok{ pred)}
\end{Highlighting}
\end{Shaded}

Hier steht \texttt{obs} für beobachtete Werte und \texttt{pred} für die
vorhergesagten Werte (beides numerische Vektoren). Dieser Befehl gibt
sowohl RMSE als auch \(R^2\) wieder. Wir betrachten gleich ein Beispiel
an echten Daten.

\BeginKnitrBlock{rmdcaution}
Verwendet man die Korrelation (r) oder \(R^2\) als Gütekriterium, so
sollte man sich über folgenden Punkt klar sein. Bei Skalierung der
Variablen ändert sich die Korrelation nicht; das gilt auch für \(R^2\).
Beide Koeffizienten ziehen allein auf das \emph{Muster} der
Zusammenhänge ab - nicht die Größe der Abstände. Aber häufig ist die
Größe der Abstände zwischen beobachteten und vorhergesagten Werten das,
was uns interessiert. In dem Fall wäre der MSE vorzuziehen.
\EndKnitrBlock{rmdcaution}

\section{Die Regression an einem Beispiel
erläutert}\label{die-regression-an-einem-beispiel-erlautert}

Schauen wir uns den Datensatz zur Statistikklausur noch einmal an.
Welchen Einfluss hat die Lernzeit auf den Klausurerfolg? Wie viel bringt
es also zu lernen? Wenn das Lernen keinen Einfluss auf den Klausurerfolg
hat, dann kann man es ja gleich sein lassen\ldots{} Aber umgekehrt, wenn
es viel bringt, ok gut, dann könnte man sich die Sache (vielleicht) noch
mal überlegen. Aber was heißt ``viel bringen'' eigentlich?

\begin{quote}
Wenn für jede Stunde Lernen viele zusätzliche Punkte herausspringen,
dann bringt Lernen viel. Allgemeiner: Je größer der Zuwachs im Kriterium
ist pro zusätzliche Einheit des Prädiktors, desto größer ist der
Einfluss des Prädiktors.
\end{quote}

Natürlich könnte jetzt jemand argumentieren, dass die ersten paar
Stunden lernen viel bringen, aber dann flacht der Nutzen ab, weil es ja
schnell einfach und trivial wird. Aber wir argumentieren (erstmal) so
nicht. Wir gehen davon aus, dass jede Stunde Lernen gleich viel (oder
wenig) Nutzen bringt.

\begin{quote}
Geht man davon aus, dass jede Einheit des Prädiktors gleich viel Zuwachs
bringt, unabhängig von dem Wert des Prädiktors, so geht man von einem
linearen Einfluss aus.
\end{quote}

Versuchen wir im ersten Schritt die Stärke des Einfluss an einem
Streudiagramm abzuschätzen (s. Abb. \ref{fig:bsp-regression}).

Hey R - berechne uns die ``Trendlinie''! Dazu nimmt man den Befehl
\texttt{lm}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mein_lm <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{study_time, }\DataTypeTok{data =}\NormalTok{ stats_test)}
\KeywordTok{tidy}\NormalTok{(mein_lm)}
\CommentTok{#>          term estimate std.error statistic  p.value}
\CommentTok{#> 1 (Intercept)    23.98     0.934     25.67 2.94e-70}
\CommentTok{#> 2  study_time     2.26     0.300      7.54 1.02e-12}
\end{Highlighting}
\end{Shaded}

\texttt{lm} steht für `lineares Modell', eben weil eine \emph{Linie} als
Modell in die Daten gelegt wird. Aha. Die Steigung der Geraden beträgt
2.3 - das ist der Einfluss des Prädiktors Lernzeit auf das Kriterium
Klausurerfolg! Man könnte sagen: Der ``Wechselkurs'' von Lernzeit auf
Klausurpunkte. Für jede Stunde Lernzeit bekommt man offenbar 2.3
Klausurpunkte (natürlich viel zu leicht). Wenn man nichts lernt
(\texttt{study\_time\ ==\ 0}) hat man 24 Punkte.

\begin{quote}
Der Einfluss des Prädiktors steht unter `estimate'. Der Kriteriumswert
wenn der Prädiktor Null ist steht unter `(Intercept)'.
\end{quote}

Malen wir diese Gerade in unser Streudiagramm (Abbildung
\ref{fig:stats-test-scatter2}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ stats_test) }\OperatorTok{+}
\StringTok{  }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ score, }\DataTypeTok{x =}\NormalTok{ study_time) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_jitter}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_abline}\NormalTok{(}\DataTypeTok{slope =} \FloatTok{2.3}\NormalTok{, }\DataTypeTok{intercept =} \DecValTok{24}\NormalTok{, }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{071_Regression_files/figure-latex/stats-test-scatter2-1} 

}

\caption{Streudiagramm von Lernzeit und Klausurerfolg}\label{fig:stats-test-scatter2}
\end{figure}

Jetzt kennen wir die Stärke (und Richtung) des Einflusses der Lernzeit.
Ob das viel oder wenig ist, ist am besten im Verhältnis zu einem
Referenzwert zu sagen.

Die Gerade wird übrigens so in die Punktewolke gelegt, dass die
(quadrierten) Abstände der Punkte zur Geraden minimal sind. Dies wird
auch als \emph{Kriterium der Kleinsten
Quadrate}\index{Kriterium der Kleinsten Quadrate} (\emph{Ordinary Least
Squares}, \emph{OLS})\index{Ordinary Least Squares} bezeichnet.

Jetzt können wir auch einfach Vorhersagen machen. Sagt uns jemand, ich
habe ``viel'' gelernt (Lernzeit = 4), so können wir den Klausurerfolg
grob im Diagramm ablesen.

Genauer geht es natürlich mit dieser Rechnung:

\(y = 4*2.3 + 24\)

Oder mit diesem R-Befehl:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{predict}\NormalTok{(mein_lm, }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{study_time =} \DecValTok{4}\NormalTok{))}
\CommentTok{#>  1 }
\CommentTok{#> 33}
\end{Highlighting}
\end{Shaded}

Berechnen wir noch die Vorversagegüte des Modells. Dazu kann man den
Befehl \texttt{summary} nehmen, oder auch \texttt{broom::glance}.
\texttt{glance} gibt Informationen zur Modellgüte zurück und das in Form
eines Dateframes. Summary liefert eine Menge Informationen mit einem
infomrationen Ausdruck, aber nicht in Form eines Dataframes (sondern in
Form einer Liste).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{glance}\NormalTok{(mein_lm)}
\CommentTok{#>   r.squared adj.r.squared sigma statistic  p.value df logLik  AIC  BIC}
\CommentTok{#> 1     0.194         0.191  5.15      56.8 1.02e-12  2   -727 1459 1470}
\CommentTok{#>   deviance df.residual}
\CommentTok{#> 1     6255         236}
\end{Highlighting}
\end{Shaded}

Das Bestimmtheitsmaß \(R^2\) ist mit 0.19 ``ok'': 19-\% der Varianz des
Klausurerfolg wird im Modell `erklärt'. `Erklärt' meint hier, dass wenn
die Lernzeit konstant wäre, würde die Varianz von Klausurerfolg um
diesen Prozentwert sinken.

\section{Überprüfung der Annahmen der linearen
Regression}\label{uberprufung-der-annahmen-der-linearen-regression}

Aber wie sieht es mit den Annahmen aus?

\begin{itemize}
\item
  Die \emph{Linearität des Zusammenhangs} haben wir zu Beginn mit Hilfe
  des Scatterplots überprüft. Es schien einigermaßen zu passen.
\item
  Zur Überprüfung der \emph{Normalverteilung der Residuen} zeichnen wir
  ein Histogramm (s. Abbildung \ref{fig:resid-distrib}). Die
  \emph{Residuen}\index{Residuen} können über den Befehl
  \texttt{add\_residuals} (Paket \texttt{modelr}) zum Datensatz
  hinzugefügt werden. Dann wird eine Spalte mit dem Namen \texttt{resid}
  zum Datensatz hinzugefügt.
\end{itemize}

Hier scheint es zu passen:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{add_residuals}\NormalTok{(mein_lm) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{ggplot }\OperatorTok{+}
\StringTok{  }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ resid) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{071_Regression_files/figure-latex/resid-distrib-1} 

}

\caption{Die Residuen verteilen sich hinreichend normal.}\label{fig:resid-distrib}
\end{figure}

Sieht passabel aus. Übrigens kann man das Paket \texttt{modelr} auch
nutzen, um sich komfortabel die vorhergesagten Werte zum Datensatz
hinzufügen zu lassen (Spalte \texttt{pred}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{add_predictions}\NormalTok{(mein_lm) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(pred) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{head}
\CommentTok{#>   pred}
\CommentTok{#> 1 35.3}
\CommentTok{#> 2 30.8}
\CommentTok{#> 3 35.3}
\CommentTok{#> 4 28.5}
\CommentTok{#> 5 33.0}
\CommentTok{#> 6   NA}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \emph{Konstante Varianz}: Dies kann z. B. mit einem Scatterplot der
  Residuen auf der Y-Achse und den vorhergesagten Werten auf der X-Achse
  überprüft werden. Bei jedem X-Wert sollte die Varianz der Y-Werte
  (etwa) gleich sein (s. Abbildung \ref{fig:tips-preds-resid}).
\end{itemize}

Die geschätzten (angepassten) Werte kann man über den Befehl
\texttt{add\_predictions()} aus dem Paket \texttt{modelr} bekommen. Die
Fehlerwerte entsprechend mit dem Befehl \texttt{add\_residuals}.

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{stats_test }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{add_predictions}\NormalTok{(mein_lm) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{add_residuals}\NormalTok{(mein_lm) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ resid, }\DataTypeTok{x =}\NormalTok{ pred) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{071_Regression_files/figure-latex/tips-preds-resid-1} 

}

\caption{Vorhergesagte Werte vs. Residualwerte im Datensatz tips}\label{fig:tips-preds-resid}
\end{figure}

Die Annahme der konstanten Varianz scheint verletzt zu sein: Die sehr
großen vorhersagten Werte können recht genau geschätzt werden; aber die
mittleren Werte nur ungenau. Die Verletzung dieser Annahme beeinflusst
\emph{nicht} die Schätzung der Steigung, sondern die Schätzung des
Standardfehlers, also des p-Wertes der Einflusswerte.

\begin{itemize}
\item
  \emph{Extreme Ausreißer}: Extreme Ausreißer scheint es nicht zu geben.
\item
  \emph{Unabhängigkeit der Beobachtungen}: Wenn die Studenten in
  Lerngruppen lernen, kann es sein, dass die Beobachtungen nicht
  unabhängig voneinander sind: Wenn ein Mitglied der Lerngruppe gute
  Noten hat, ist die Wahrscheinlichkeit für ebenfalls gute Noten bei den
  anderen Mitgliedern der Lerngruppe erhöht. Böse Zungen behaupten, dass
  `Abschreiben' eine Gefahr für die Unabhängigkeit der Beobachtungen
  sei.
\end{itemize}

\BeginKnitrBlock{rmdexercises}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Wie groß ist der Einfluss des Interesss?
\item
  Für wie aussagekräftig halten Sie Ihr Ergebnis aus 1.?
\item
  Welcher Einflussfaktor (in unseren Daten) ist am stärksten?
\end{enumerate}
\EndKnitrBlock{rmdexercises}

\section{Regression mit kategorialen
Prädiktoren}\label{regression-mit-kategorialen-pradiktoren}

Vergleichen wir interessierte und nicht interessierte Studenten. Dazu
teilen wir die Variable \texttt{interest} in zwei Gruppen (1-3 vs.~4-6)
auf:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test}\OperatorTok{$}\NormalTok{interessiert <-}\StringTok{ }\NormalTok{stats_test}\OperatorTok{$}\NormalTok{interest }\OperatorTok{>}\StringTok{ }\DecValTok{3}
\end{Highlighting}
\end{Shaded}

Vergleichen wir die Mittelwerte des Klausurerfolgs zwischen den
Interessierten und Nicht-Interessierten:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(interessiert) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{score =} \KeywordTok{mean}\NormalTok{(score)) ->}\StringTok{ }\NormalTok{score_interesse}

\NormalTok{score_interesse}
\CommentTok{#> # A tibble: 3 x 2}
\CommentTok{#>   interessiert score}
\CommentTok{#>          <lgl> <dbl>}
\CommentTok{#> 1        FALSE  29.9}
\CommentTok{#> 2         TRUE  31.5}
\CommentTok{#> 3           NA  33.1}
\end{Highlighting}
\end{Shaded}

Aha, die Interessierten haben im Schnitt mehr Punkte; aber nicht viel.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{na.omit }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ interessiert, }\DataTypeTok{y =}\NormalTok{ score) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_jitter}\NormalTok{(}\DataTypeTok{width =}\NormalTok{ .}\DecValTok{1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ score_interesse, }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{5}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ score_interesse, }\DataTypeTok{group =} \DecValTok{1}\NormalTok{, }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{071_Regression_files/figure-latex/unnamed-chunk-11-1} \end{center}

Mit \texttt{group=1} bekommt man eine Linie, die alle Punkte verbindet
(im Datensatz \texttt{score\_interesse} sind es dieser zwei). Wir haben
in dem Fall nur zwei Punkte, die entsprechend verbunden werden.

\subsection{Aufgaben}\label{aufgaben-12}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Visualisieren Sie den Gruppenunterschied auch mit einem Boxplot!
\item
  Berechnen Sie ein lineares Modell dazu!
\end{enumerate}

\emph{Lösung:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Boxplot:
  \texttt{qplot(x\ =\ interessiert,\ y\ =\ score,\ data\ =\ stats\_test,\ geom\ =\ "boxplot")}
\item
  Lineares Modell:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{interessiert, }\DataTypeTok{data =}\NormalTok{ stats_test)}
\KeywordTok{summary}\NormalTok{(lm2)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> lm(formula = score ~ interessiert, data = stats_test)}
\CommentTok{#> }
\CommentTok{#> Residuals:}
\CommentTok{#>     Min      1Q  Median      3Q     Max }
\CommentTok{#> -13.537  -4.380  -0.537   4.463  10.091 }
\CommentTok{#> }
\CommentTok{#> Coefficients:}
\CommentTok{#>                  Estimate Std. Error t value Pr(>|t|)    }
\CommentTok{#> (Intercept)        29.909      0.475   62.99   <2e-16 ***}
\CommentTok{#> interessiertTRUE    1.628      0.752    2.17    0.031 *  }
\CommentTok{#> ---}
\CommentTok{#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1}
\CommentTok{#> }
\CommentTok{#> Residual standard error: 5.68 on 236 degrees of freedom}
\CommentTok{#>   (68 observations deleted due to missingness)}
\CommentTok{#> Multiple R-squared:  0.0195, Adjusted R-squared:  0.0153 }
\CommentTok{#> F-statistic: 4.69 on 1 and 236 DF,  p-value: 0.0313}
\end{Highlighting}
\end{Shaded}

Der Einfluss von \texttt{interessiert} ist statistisch signifikant (p =
.03). Der Stärke des Einflusses ist im Schnitt 1.6 Klausurpunkte
(zugunsten \texttt{interessiertTRUE}). Das ist genau, was wir oben
herausgefunden haben.

\BeginKnitrBlock{rmdexercises}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  Vie ist der Einfluss von \texttt{study\_time}, auch in zwei Gruppen
  geteilt?
\item
  Wie viel \% der Variation des Klausurerfolgs können Sie durch das
  Interesse modellieren?
\end{enumerate}
\EndKnitrBlock{rmdexercises}

\section{Multiple Regression}\label{multiple-regression}

Aber wie wirken sich mehrere Einflussgrößen \emph{zusammen} auf den
Klausurerfolg aus?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm3 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{study_time }\OperatorTok{+}\StringTok{ }\NormalTok{interessiert, }\DataTypeTok{data =}\NormalTok{ stats_test)}
\KeywordTok{summary}\NormalTok{(lm3)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> lm(formula = score ~ study_time + interessiert, data = stats_test)}
\CommentTok{#> }
\CommentTok{#> Residuals:}
\CommentTok{#>     Min      1Q  Median      3Q     Max }
\CommentTok{#> -13.896  -3.577   0.418   3.805  13.065 }
\CommentTok{#> }
\CommentTok{#> Coefficients:}
\CommentTok{#>                  Estimate Std. Error t value Pr(>|t|)    }
\CommentTok{#> (Intercept)        23.955      0.938   25.55  < 2e-16 ***}
\CommentTok{#> study_time          2.314      0.323    7.15  1.1e-11 ***}
\CommentTok{#> interessiertTRUE   -0.333      0.736   -0.45     0.65    }
\CommentTok{#> ---}
\CommentTok{#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1}
\CommentTok{#> }
\CommentTok{#> Residual standard error: 5.16 on 235 degrees of freedom}
\CommentTok{#>   (68 observations deleted due to missingness)}
\CommentTok{#> Multiple R-squared:  0.195,  Adjusted R-squared:  0.188 }
\CommentTok{#> F-statistic: 28.4 on 2 and 235 DF,  p-value: 8.81e-12}
\end{Highlighting}
\end{Shaded}

Interessant ist das \emph{negative} Vorzeichen vor dem Einfluss von
\texttt{interessiertTRUE}! Die multiple Regression untersucht den
`Nettoeinfluss' jedes Prädiktors. Den Einfluss also, wenn der andere
Prädiktor \emph{konstant} gehalten wird. Anders gesagt: Betrachten wir
jeden Wert von \texttt{study\_time} separat, so haben die Interessierten
jeweils im Schnitt etwas \emph{weniger} Punkte (jesses). Allerdings ist
dieser Unterschied nicht statistisch signifikant.

\begin{quote}
Die multiple Regression zeigt den `Nettoeinfluss' jedes Prädiktor: Den
Einfluss dieses Prädiktor, wenn der andere Prädiktor oder die anderne
Prädiktoren konstant gehalten werden.
\end{quote}

Hier haben wir übrigens dem Modell aufgezwungen, dass der Einfluss von
Lernzeit auf Klausurerfolg bei den beiden Gruppen gleich groß sein soll
(d.h. bei Interessierten und Nicht-Interessierten ist die Steigung der
Regressionsgeraden gleich). Das illustriert sich am einfachsten in einem
Diagramm (s. Abbildung \ref{fig:no-interakt}).

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{071_Regression_files/figure-latex/no-interakt-1} 

}

\caption{Eine multivariate Analyse fördert Einsichten zu Tage, die bei einfacheren Analysen verborgen bleiben}\label{fig:no-interakt}
\end{figure}

Diese \emph{multivariate}\index{multivariat} Analyse (mehr als 2
Variablen sind beteiligt) zeigt uns, dass die Regressionsgerade nicht
gleich ist in den beiden Gruppen (Interessierte vs.~Nicht-Interessierte;
s. Abbildung \ref{fig:no-interakt}): Im Teildiagramm A sind die Geraden
(leicht) versetzt. Analog zeigt Teildiagramm B, dass die Interessierten
(\texttt{interessiert\ ==\ TRUE}) geringe Punktewerte haben als die
Nicht-Interessierten, wenn man die Werte von \texttt{study\_time}
getrennt betrachtet.

\begin{quote}
Die multivariate Analyse zeigt ein anderes Bild, ein genaueres Bild als
die einfachere Analyse. Ein Sachverhalt, der für den ganzen Datensatz
gilt, kann in Subgruppen anders sein.
\end{quote}

Ohne multivariate Analyse hätten wir dies nicht entdeckt. Daher sind
multivariate Analysen sinnvoll und sollten gegenüber einfacheren
Analysen bevorzugt werden.

Man könnte sich jetzt noch fragen, ob die Regressionssgerade in
Abbildung \ref{fig:no-interakt} parallel sein müssen. Gerade hat unser
R-Befehl sie noch gezwungen, parallel zu sein. Gleich lassen wir hier
die Zügel locker. Wenn die Regressionsgerade nicht mehr parallel sind,
spricht man von \emph{Interaktionseffekten}.

Das Ergebnis des zugrunde-liegenden F-Tests (vgl. Varianzanalyse) wird
in der letzten Zeile angegeben (\texttt{F-Statistic}). Hier wird \(H_0\)
also verworfen.

\section{Interaktionen}\label{interaktionen}

Es könnte ja sein, dass die Stärke des Einflusses von Lernzeit auf
Klausurerfolg in der Gruppe der Interessierten anders ist als in der
Gruppe der Nicht-Interessierten. Wenn man nicht interessiert ist, so
könnte man argumentieren, dann bringt eine Stunden Lernen weniger als
wenn man interessiert ist. Darum müssten die Steigungen der
Regressionsgeraden in den beiden Gruppen unterschiedlich sein. Schauen
wir uns es an. Um R dazu zu bringen, die Regressionsgeraden frei
variieren zu lassen, so dass sie nicht mehr parallel sind, nutzen wir
das Symbol \texttt{*}, dass wir zwischen die betreffenden Prädiktoren
schreiben:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm4 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{interessiert}\OperatorTok{*}\NormalTok{study_time, }\DataTypeTok{data =}\NormalTok{ stats_test)}
\KeywordTok{summary}\NormalTok{(lm4)}
\CommentTok{#> }
\CommentTok{#> Call:}
\CommentTok{#> lm(formula = score ~ interessiert * study_time, data = stats_test)}
\CommentTok{#> }
\CommentTok{#> Residuals:}
\CommentTok{#>     Min      1Q  Median      3Q     Max }
\CommentTok{#> -13.950  -3.614   0.356   4.020  12.598 }
\CommentTok{#> }
\CommentTok{#> Coefficients:}
\CommentTok{#>                             Estimate Std. Error t value Pr(>|t|)    }
\CommentTok{#> (Intercept)                   23.627      1.158   20.40  < 2e-16 ***}
\CommentTok{#> interessiertTRUE               0.655      2.170    0.30     0.76    }
\CommentTok{#> study_time                     2.441      0.418    5.85  1.7e-08 ***}
\CommentTok{#> interessiertTRUE:study_time   -0.321      0.662   -0.48     0.63    }
\CommentTok{#> ---}
\CommentTok{#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1}
\CommentTok{#> }
\CommentTok{#> Residual standard error: 5.17 on 234 degrees of freedom}
\CommentTok{#>   (68 observations deleted due to missingness)}
\CommentTok{#> Multiple R-squared:  0.196,  Adjusted R-squared:  0.185 }
\CommentTok{#> F-statistic:   19 on 3 and 234 DF,  p-value: 4.81e-11}
\end{Highlighting}
\end{Shaded}

Interessanterweise zeigen die Interessierten nun wiederum - betrachtet
man jede Stufe von \texttt{study\_time} einzeln - bessere
Klausurergebnisse als die Nicht-Interessierten. Ansonsten ist noch die
Zeile \texttt{interessiertTRUE:study\_time} neu. Diese Zeile zeigt die
Höhe des \emph{Interaktionseffekts}\index{Interaktionseffekt}. Bei den
Interessierten ist die Steigung der Geraden um 0.32 Punkte geringer als
bei den Interessierten. Der Effekt ist klein und nicht statistisch
signifikant, so dass wir wahrscheinlich Zufallsrauschen
überinterpretieren. Aber die reine Zahl sagt, dass bei den
Interessierten jede Lernstunde weniger Klausurerfolg bringt als bei den
Nicht-Interessierten. Auch hier ist eine Visualisierung wieder
hilfreich.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{071_Regression_files/figure-latex/interakt-stats-test-1} 

}

\caption{Eine Regressionsanalyse mit Interaktionseffekten}\label{fig:interakt-stats-test}
\end{figure}

Wir wir in Abbildung \ref{fig:interakt-stats-test} sehen, ist der
Einfluss von
\texttt{study\_time\textquotesingle{}\ je\ nach\ Gruppe\ (Wert\ von}interessiert\texttt{)\ unterschiedlich\ (Teildiagramm\ A).\ Analog\ ist\ der\ Einfluss\ des\ Interesses\ (leicht)\ unterschiedlich,\ wenn\ man\ die\ fünf\ Stufen\ von}study\_time`
getrennt betrachtet.

\begin{quote}
Sind die Regressionsgerade nicht parallel, so liegt ein
Interaktionseffekt vor. Andernfalls nicht.
\end{quote}

\section{Fallstudie zu Overfitting}\label{overfitting-casestudy}

Vergleichen wir im ersten Schritt eine Regression, die die Modellgüte
anhand der \emph{Trainingsstichprobe} schätzt mit einer Regression, bei
der die Modellgüte in einer \emph{Test-Stichprobe} überprüft wird.

Betrachten wir nochmal die einfache Regression von oben. Wie lautet das
\(R^2\)?

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{lm1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{study_time, }\DataTypeTok{data =}\NormalTok{ stats_test)}
\end{Highlighting}
\end{Shaded}

Es lautet \texttt{round(summary(lm1)\$r.squared,\ 2)}.

Im zweiten Schritt teilen wir die Stichprobe in eine Trainings- und eine
Test-Stichprobe auf. Wir ``trainieren'' das Modell anhand der Daten aus
der Trainings-Stichprobe:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train <-}\StringTok{ }\NormalTok{stats_test }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{sample_frac}\NormalTok{(.}\DecValTok{8}\NormalTok{, }\DataTypeTok{replace =} \OtherTok{FALSE}\NormalTok{)  }\CommentTok{# Stichprobe von 80%, ohne Zurücklegen}

\NormalTok{test <-}\StringTok{ }\NormalTok{stats_test }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{anti_join}\NormalTok{(train)  }\CommentTok{# Alle Zeilen von "df", die nicht in "train" vorkommen}

\NormalTok{lm_train <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(score }\OperatorTok{~}\StringTok{ }\NormalTok{study_time, }\DataTypeTok{data =}\NormalTok{ train)}
\end{Highlighting}
\end{Shaded}

Dann testen wir (die Modellgüte) anhand der \emph{Test}-Stichprobe. Also
los, \texttt{lm\_train}, mach Deine Vorhersage:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm2_predict <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(lm_train, }\DataTypeTok{newdata =}\NormalTok{ test)}
\end{Highlighting}
\end{Shaded}

Diese Syntax sagt:

\BeginKnitrBlock{rmdpseudocode}
Speichere unter dem Namen ``lm2\_predict'' das Ergebnis folgender
Berechnung:\\
Mache eine Vorhersage (``to predict'') anhand des Modells ``lm2'',\\
wobei frische Daten (``newdata = test'') verwendet werden sollen.
\EndKnitrBlock{rmdpseudocode}

Als Ergebnis bekommen wir einen Vektor, der für jede Beobachtung des
Test-Samples den geschätzten (vorhergesagten) Klausurpunktewert
speichert.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{caret}\OperatorTok{::}\KeywordTok{postResample}\NormalTok{(}\DataTypeTok{pred =}\NormalTok{ lm2_predict, }\DataTypeTok{obs =}\NormalTok{ test}\OperatorTok{$}\NormalTok{score)}
\CommentTok{#>     RMSE Rsquared }
\CommentTok{#>    4.331    0.345}
\end{Highlighting}
\end{Shaded}

Die Funktion \texttt{postResample} aus dem Paket \texttt{caret} liefert
uns zentrale Gütekennzahlen unser Modell. Wir sehen, dass die Modellgüte
im Test-Sample deutlich \emph{schlechter} ist als im Trainings-Sample.
Ein typischer Fall, der uns warnt, nicht vorschnell optimistisch zu
sein!

\begin{quote}
Die Modellgüte im in der Test-Stichprobe ist meist schlechter als in der
Trainings-Stichprobe. Das warnt uns vor Befunden, die naiv nur die Werte
aus der Trainings-Stichprobe berichten.
\end{quote}

\section[Aufgaben]{\texorpdfstring{Aufgaben\footnote{F, R, R, F, F, F,
  F, F, R}}{Aufgaben}}\label{aufgaben-13}

\BeginKnitrBlock{rmdexercises}
Richtig oder Falsch!?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  X-Wert: Kriterium; Y-Wert: Prädiktor.
\item
  Der Y-Wert in der einfachen Regression wird berechnet als
  Achsenabschnitt plus \emph{x} mal die Geradensteigung.
\item
  \(R^2\) liefert einen \emph{relativen} Vorhersagefehler und MSE einen
  \emph{absoluten} (relativ im Sinne eines Anteils).
\item
  Unter `Ordinary Least Squares' versteht man eine abschätzige Haltung
  gegenüber Statistik.
\item
  Zu den Annahmen der Regression gehört Normalverteilung der
  \emph{Kriteriumswerte}.
\item
  Die Regression darf nicht bei kategorialen Prädiktoren verwendet
  werden.
\item
  Mehrere bivariate Regressionsanalysen (1 Prädiktor, 1 Kriterium) sind
  einer multivariaten Regression i.d.R. vorzuziehen.
\item
  Interaktionen erkennt man daran, dass die Regressionsgeraden
  \emph{nicht} parallel sind.
\end{enumerate}
\EndKnitrBlock{rmdexercises}

\section{Befehlsübersicht}\label{befehlsubersicht-6}

Tabelle \ref{tab:befehle-regression} stellt die Befehle dieses Kapitels
dar.

\begin{table}

\caption{\label{tab:befehle-regression}Befehle des Kapitels 'Regression'}
\centering
\begin{tabular}[t]{l|l}
\hline
Paket..Funktion & Beschreibung\\
\hline
lm & Berechnet eine Regression\\
\hline
sqrt & Zieht die Quadratwurzel\\
\hline
caret::postResample & Berechnet Gütekriterien für das Testsample\\
\hline
summary & Fasst zentrale Informationen zu einem Objekt zusammen\\
\hline
modelr::add\_residuals & Erstellt eine Spalte mit Residuen\\
\hline
modelr::add\_predictions & Erstellt eine Spalte mit den vorhergesagten Werten\\
\hline
levels & Zeigt oder ändert die Stufen eines Faktors\\
\hline
factor & Erstellt einen Faktor (nominalskalierte Variable)\\
\hline
coef & Zeigt die Koeffizienten eines Objekts an.\\
\hline
step & Führt eine Schrittweise-Rückwärtsselektion durch\\
\hline
sample\_frac & Sampelt einen Prozentsatz aus einem Datensatz\\
\hline
anti\_join & Fügt Vereint nicht-gleiche Zeilen zweier Datensätze\\
\hline
\end{tabular}
\end{table}

\chapter{Klassifizierende Regression}\label{klassifizierende-regression}

\begin{center}\includegraphics[width=0.3\linewidth]{images/FOM} \end{center}

\begin{center}\includegraphics[width=0.1\linewidth]{images/licence} \end{center}

\BeginKnitrBlock{rmdcaution}
Lernziele:

\begin{itemize}
\tightlist
\item
  Die Idee der logistischen Regression verstehen.
\item
  Die Koeffizienten der logistischen Regression interpretieren können.
\item
  Die Modellgüte einer logistischten Regression einschätzen können.
\item
  Klassifikatorische Kennzahlen kennen und beurteilen können.
\end{itemize}
\EndKnitrBlock{rmdcaution}

Für dieses Kapitel benötigen Sie folgende Pakete:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(SDMTools)  }\CommentTok{# Güte von Klassifikationsmodellen}
\KeywordTok{library}\NormalTok{(pROC)  }\CommentTok{# für ROC- und AUC-Berechnung}
\KeywordTok{library}\NormalTok{(tidyverse)  }\CommentTok{# Datenjudo}
\KeywordTok{library}\NormalTok{(BaylorEdPsych)  }\CommentTok{# Pseudo-R-Quadrat}
\KeywordTok{library}\NormalTok{(broom)  }\CommentTok{# lm-Ergebnisse aufräumen}
\end{Highlighting}
\end{Shaded}

Hilft Lernen, eine Statistkklausur zu bestehen? Kommmt es auf Interesse
an? Versuchen wir vorherzusagen, wer eine Statistikklausur besteht.
Etwas genauer gesagt, sagen wir ein \emph{binäres} (dichotomes) Ereignis
- Bestehen der Klausur - vorher anhand von einer mehr Variablen mit
beliebigen Skalenniveau.

Laden wir die Klausurdaten.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"data/stats_test.csv"}\NormalTok{)}
\NormalTok{stats_test <-}\StringTok{ }\KeywordTok{na.omit}\NormalTok{(stats_test)}
\end{Highlighting}
\end{Shaded}

Um uns das Leben leichter zu machen, haben wir fehlende Werte
(\texttt{NA}s) mit \texttt{na.omit} gelöscht.

\section{Normale Regression für ein binäres
Kriterium}\label{normale-regression-fur-ein-binares-kriterium}

In gewohnter Manier nutzen wir die normale Regression um das Kriterium
`Bestehen' anhand der Vorbreitungszeit vorherzusagen. Mit einem kleinen
Trick könenn wir die binäre Variable \texttt{bestanden} in eine Art
metrische Variable umwandeln, damit sie wieder in unser
Regressions-Handwerk passt: Wenn \texttt{bestanden=="ja"} dann sei
\texttt{bestanden\_num\ =\ 1}; ansonsten \texttt{bestanden\_num\ =\ 0}.
Dieses `wenn-dann' leistet der Befehl \texttt{if\_else}:
\texttt{if\_else(bedingung,\ wenn\_erfüllt,\ ansonsten)}. In unserem
Fall sieht das so aus:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{bestanden_num =} \KeywordTok{if_else}\NormalTok{(bestanden }\OperatorTok{==}\StringTok{ "ja"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)) ->}\StringTok{ }\NormalTok{stats_test}
\end{Highlighting}
\end{Shaded}

Rechnen wir jetzt unsere Regression:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(bestanden_num }\OperatorTok{~}\StringTok{ }\NormalTok{study_time, }\DataTypeTok{data =}\NormalTok{ stats_test)}
\KeywordTok{tidy}\NormalTok{(lm1)}
\CommentTok{#>          term estimate std.error statistic  p.value}
\CommentTok{#> 1 (Intercept)   0.6465    0.0654      9.89 1.61e-19}
\CommentTok{#> 2  study_time   0.0666    0.0210      3.17 1.70e-03}
\end{Highlighting}
\end{Shaded}

Hm. Stellen wir das Ergebnis grafisch dar (vgl. Abbildung
\ref{fig:fig-logist-regr2}).

\begin{Shaded}
\begin{Highlighting}[]

\KeywordTok{ggplot}\NormalTok{(stats_test) }\OperatorTok{+}
\StringTok{  }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ study_time, }\DataTypeTok{y =}\NormalTok{ bestanden_num) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_jitter}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_abline}\NormalTok{(}\DataTypeTok{slope =}\NormalTok{ .}\DecValTok{07}\NormalTok{, }\DataTypeTok{intercept =}\NormalTok{ .}\DecValTok{65}\NormalTok{, }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{072_klassifizierende_Regression_files/figure-latex/fig-logist-regr2-1} 

}

\caption{Regressionsgerade für das Bestehen-Modell}\label{fig:fig-logist-regr2}
\end{figure}

Ah! Mehr Lernen hilft offenbar: Die Regressionsgerade steigt.

Betrachten Sie diese praktische Eigenschaft der Regression: Obwohl die
Kriteriumsvariable (Y-Achse) nur zwei Ausprägungen aufweißt (0 und 1),
sagt sich die Regression: ``Hey, 0 und 1 sind normale reelle Zahlen und
zwischen jedem solcher Zahlenpaare gibt es Zahlen dazwischen. Also kann
ich meine Regressiongerade ohne abzusetzen durchmalen''. Damit können
wir die Werte zwischen 0 und 1 wie Wahrscheinlichkeiten interpretieren:
Sagt die Regressionsgerade für bestimmte Prädiktorwerte hohe
Kriteriumswerte voraus, so können wir sagen, die Wahrscheinlichkeit, die
Klausur zu bestehen, ist hoch.

Soweit, so gut. Aber Moment. Was bedeutet es, wenn die
Wahrscheinlichkeit größer 1 ist? Dass der Professor vorher einen
eidesstattliche Erklärung für Bestehen geschickt hat? Von so etwas hat
man noch nicht gehört\ldots{} Kurz gesagt: Wahrscheinlichkeiten größer 1
und kleiner 0 sind Quatsch. Wahrscheinlichkeiten müssen zwischen 0 und 1
liegen.

\section{Die logistische Funktion}\label{die-logistische-funktion}

Daher brauchen wir eine Funktion, die das Ergebnis einer linearen
Regression in einen Bereich von 0 bis 1 ``umbiegt'' (die sogenannte
\emph{Linkfunktion}). Eine häufig dafür verwendete Funktion ist die
\emph{logistische Funktion}\index{logistische Funktion}. Im einfachsten
Fall:

\[p(y=1)=\frac{e^x}{1+e^x} = \frac{e^x}{e^x(\frac{1}{e^x}+1)}=\frac{1}{\frac{1}{e^x}+1}=\frac{1}{e^{-x}+1}\]

Exemplarisch können wir die logistische Funktion für einen Bereich von
\(x=-10\) bis \(x=+10\) darstellen (vgl. \ref{fig:logist-curve}). Der
Graph der logistischen Funktion ähnelt einem langgestreckten S
(``Ogive'' genannt).

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{072_klassifizierende_Regression_files/figure-latex/logist-curve-1} 

}

\caption{Die logistische Regression beschreibt eine 's-förmige' Kurve}\label{fig:logist-curve}
\end{figure}

\section{Die Idee der logistischen
Regression}\label{die-idee-der-logistischen-regression}

Die logistische Regression ist eine Anwendung des \emph{Allgemeinen
Linearen Modells}\index{Allgemeines Lineares Modells} (general linear
model, GLM). Die Modellgleichung lautet:

\(p(y_i=1)=L\bigl(\beta_0+\beta_1\cdot x_{i1}+\dots+\beta_K\cdot x_{ik}\bigr)+\epsilon_i\)

\begin{itemize}
\tightlist
\item
  \(L\) ist die Linkfunktion, in unserer Anwendung die logistische
  Funktion.\\
\item
  \(x_{ik}\) sind die beobachten Werte der unabhängigen Variablen
  \(X_k\).\\
\item
  \(k\) sind die unabhängigen Variablen \(1\) bis \(K\).
\end{itemize}

Die Funktion \texttt{glm} führt die logistische Regression durch.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{glm1 <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(bestanden_num }\OperatorTok{~}\StringTok{ }\NormalTok{study_time, }
            \DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{,}
            \DataTypeTok{data =}\NormalTok{ stats_test)}
\end{Highlighting}
\end{Shaded}

Wir schauen uns zunächst den Plot an (Abb. \ref{fig:aktien-plot}.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{072_klassifizierende_Regression_files/figure-latex/aktien-plot-1} 

}

\caption{Modelldiagramm mit logistischer Regression}\label{fig:aktien-plot}
\end{figure}

\begin{quote}
Es werden ein Streudiagramm der beobachten Werte sowie die
\emph{Regressionslinie} ausgegeben. Wir können so z. B. ablesen, dass
mit einer Lernzeit von 5 die Wahrscheinlichkeit für Bestehen bei knapp
100\% liegt; viel zu einfach\ldots{}
\end{quote}

Die Zusammenfassung des Modells zeigt folgendes:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tidy}\NormalTok{(glm1)}
\CommentTok{#>          term estimate std.error statistic p.value}
\CommentTok{#> 1 (Intercept)    0.276     0.458     0.602 0.54698}
\CommentTok{#> 2  study_time    0.513     0.168     3.049 0.00229}
\end{Highlighting}
\end{Shaded}

Die p-Werte der Koeffizienten können in der Spalte
\texttt{Pr(\textgreater{}\textbar{}z\textbar{})} abgelesen werden. Der
Achsenabschnitt (\texttt{intercept}) wird mit 0.28 geschätzt, die
Steigung in Richtung \texttt{study\_time} mit 0.51. Allerdings sind die
hier dargestellten Werte sogenannte \emph{Logits}\index{Logit}
\(\mathfrak{L}\)\footnote{ein schnödes L wie in Ludwig}:

\(\mathfrak{L} = ln\left( \frac{p}{1-p} \right)\)

Zugeben, dass klingt erstmal opaque. Das Praktische ist, dass wir die
Koeffizienten in Logitform in gewohnter Manier verrechnen dürfen. Wollen
wir zum Beispiel wissen, wie wahrscheinlich das Ereignis `Bestehen' für
eine Person mit einer Lernzeit von 3 ist, können wir einfach rechnen:

\texttt{y\ =\ intercept\ +\ 3*study\_time}, also

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(y <-}\StringTok{ }\NormalTok{.}\DecValTok{27} \OperatorTok{+}\StringTok{ }\DecValTok{3} \OperatorTok{*}\StringTok{ }\FloatTok{0.51}\NormalTok{)}
\CommentTok{#> [1] 1.8}
\end{Highlighting}
\end{Shaded}

Einfach, oder? Genau wie bei der normalen Regression. Aber beachten Sie,
dass das Ergebnis in \emph{Logits}\index{Logit} angegeben ist. Was ein
\emph{Logit} ist? Naja, das ist der \emph{Logarithmus der Chancen};
unter `Chancen'\index{Chancen} versteht man den Quotienten von
Wahrscheinlichkeit \(p\) zur Gegenwahrscheinlichkeit, \(1-p\); die
Chancen werden auch \emph{Odds}\index{Odds} oder \emph{Wettquotient}
genant.

Um zur `normalen' Wahrscheinlichkeit zu kommen, muss man also erst
`delogarithmieren'. Delogarithmieren bedeutet, die e-Funktion
anzuwenden, \texttt{exp} auf Errisch:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{exp}\NormalTok{(y)}
\CommentTok{#> [1] 6.05}
\end{Highlighting}
\end{Shaded}

Jetzt haben wir wir also Chancen. Wie rechnet man Chancen in
Wahrscheinlichkeiten um? Ein Beispiel zur Illustration. Bei
Prof.~Schnaggeldi fallen von 10 Studenten 9 durch. Die
Durchfall\emph{chance} ist also 9:1 oder 9. Die
Durchfall\emph{wahrscheinlichkeit} 9/10 oder .9. Also kann man so
umrechnen:

\texttt{wskt\ =\ 9\ /\ (9+1)\ =\ 9/10\ =\ .9}.

In unserem Fall sind die Chancen etwa 6:1; also lautet die Umrechnung:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(wskt <-}\StringTok{ }\DecValTok{6} \OperatorTok{/}\StringTok{ }\NormalTok{(}\DecValTok{6}\OperatorTok{+}\DecValTok{1}\NormalTok{))}
\CommentTok{#> [1] 0.857}
\end{Highlighting}
\end{Shaded}

Diesen Ritt kann man sich merklich kommoder bereiten, wenn man diesen
Befehl kennt:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{predict}\NormalTok{(glm1, }\DataTypeTok{newdata =} \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{study_time =} \DecValTok{3}\NormalTok{), }\DataTypeTok{type =} \StringTok{"response"}\NormalTok{)}
\CommentTok{#>    1 }
\CommentTok{#> 0.86}
\end{Highlighting}
\end{Shaded}

\section{\texorpdfstring{Kein \(R^2\), dafür
AIC}{Kein R\^{}2, dafür AIC}}\label{kein-r2-dafur-aic}

Es gibt kein \(R^2\) im Sinne einer erklärten Streuung der \(y\)-Werte,
da die beobachteten \(y\)-Werte nur \(0\) oder \(1\) annehmen können.
Das Gütemaß bei der logistischen Regression ist das \emph{Akaike
Information Criterion} (\emph{AIC}). Hier gilt allerdings: je
\emph{kleiner}, desto \emph{besser}. (Anmerkung: es kann ein
Pseudo-\(R^2\) berechnet werden -- kommt später.) Richtlinien, was ein
``guter'' AIC-Wert ist, gibt es nicht. Diese Werte helfen nur beim
Vergleichen von Modellen.

\section{Interpretation der
Koeffizienten}\label{interpretation-der-koeffizienten}

Ist ein Logit \(\mathfrak{L}\) größer als \(0\), so ist die zugehörige
Wahrscheinlichkeit größer als 50\% (und umgekehrt.)

\subsection{\texorpdfstring{y-Achsenabschnitt (\texttt{Intercept})
\(\beta_0\)}{y-Achsenabschnitt (Intercept) \textbackslash{}beta\_0}}\label{y-achsenabschnitt-intercept-beta_0}

Für \(\beta_0>0\) gilt, dass selbst wenn alle anderen unabhängigen
Variablen \(0\) sind, es eine Wahrscheinlichkeit von mehr als 50\% gibt,
dass das modellierte Ereignis eintritt. Für \(\beta_0<0\) gilt
entsprechend das Umgekehrte.

\subsection{\texorpdfstring{Steigung \(\beta_i\) mit
\(i=1,2,...,K\)}{Steigung \textbackslash{}beta\_i mit i=1,2,...,K}}\label{steigung-beta_i-mit-i12...k}

Für \(\beta_i>0\) gilt, dass mit zunehmenden \(x_i\) die
Wahrscheinlichkeit für das modellierte Ereignis steigt. Bei
\(\beta_i<0\) nimmt die Wahrscheinlichkeit entsprechend ab.

\subsection{Aufgabe}\label{aufgabe}

Berechnen Sie den Zuwachs an Wahrscheinlichkeit für unser
Beispielmodell, wenn sich die \texttt{study\_time} von 1 auf 2 erhöht.
Vergleichen Sie das Ergebnis mit der Punktprognose für
\texttt{study\_time}\(=7\) im Vergleich zu \texttt{study\_time}\(=8\).

Lösung:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# aus Koeffizient abgeschätzt}
\NormalTok{wskt1 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(glm1, }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{study_time =} \DecValTok{1}\NormalTok{), }\DataTypeTok{type =} \StringTok{"response"}\NormalTok{)}

\NormalTok{wskt2 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(glm1, }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{study_time =} \DecValTok{2}\NormalTok{), }\DataTypeTok{type =} \StringTok{"response"}\NormalTok{)}

\NormalTok{wskt2 }\OperatorTok{-}\StringTok{ }\NormalTok{wskt1}
\CommentTok{#>      1 }
\CommentTok{#> 0.0985}
\end{Highlighting}
\end{Shaded}

Anders gesagt: ``Mit jedem Punkt mehr `study\_time' steigt der Logit
(die logarithmierten Chancen) für Bestehen um 0.513''.

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{# mit dem vollständigen Modell berechnet}
\KeywordTok{predict}\NormalTok{(glm1, }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{study_time =} \DecValTok{1}\NormalTok{), }
        \DataTypeTok{type =} \StringTok{"response"}\NormalTok{)}
\CommentTok{#>     1 }
\CommentTok{#> 0.688}

\KeywordTok{predict}\NormalTok{(glm1, }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{study_time =} \DecValTok{8}\NormalTok{), }
        \DataTypeTok{type =} \StringTok{"response"}\NormalTok{)}
\CommentTok{#>     1 }
\CommentTok{#> 0.988}
\end{Highlighting}
\end{Shaded}

Bei einer study\_time von 4 beträgt die Wahrscheinlichkeit für \(y=1\),
d.h. für das Ereignis `Bestehen', 0.91. Bei einer study\_time von 58
liegt diese Wahrscheinlichkeit bei 0.94.

\section{Kategoriale Prädiktoren}\label{kategoriale-pradiktoren}

Wie in der linearen Regression können auch in der logistischen
Regression kategoriale Variablen als unabhängige Variablen genutzt
werden.

Betrachten wir als Beispiel die Frage, ob die kategoriale Variable
``Interessiert'' (genauer: dichotome Variable) einen Einfluss auf das
Bestehen in der Klausur hat, also die Wahrscheinlichkeit für Bestehen
erhöht.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stats_test}\OperatorTok{$}\NormalTok{interessiert <-}\StringTok{ }\NormalTok{stats_test}\OperatorTok{$}\NormalTok{interest }\OperatorTok{>}\StringTok{ }\DecValTok{3}
\end{Highlighting}
\end{Shaded}

Erstellen Sie zum Aufwärmen ein passendes Diagramm!

Los geht's, probieren wir die logistische Regression aus:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{glm2 <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(bestanden_num }\OperatorTok{~}\StringTok{ }\NormalTok{interessiert, }
               \DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{,}
               \DataTypeTok{data =}\NormalTok{ stats_test)}
\KeywordTok{tidy}\NormalTok{(glm2)}
\CommentTok{#>               term estimate std.error statistic  p.value}
\CommentTok{#> 1      (Intercept)     1.50     0.217      6.94 4.00e-12}
\CommentTok{#> 2 interessiertTRUE     0.43     0.377      1.14 2.55e-01}
\end{Highlighting}
\end{Shaded}

Der Einflusswert (die Steigung) von \texttt{interessiert} ist positiv:
Wenn man interessiert ist, steigt die Wahrscheinlichkeit zu bestehen.
Gut. Aber wie groß ist die Wahrscheinlichkeit für jede Gruppe? Am
einfachsten lässt man sich das von R ausrechnen:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{predict}\NormalTok{(glm2, }\DataTypeTok{newdata =} \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{interessiert =} \OtherTok{FALSE}\NormalTok{), }
        \DataTypeTok{type =} \StringTok{"response"}\NormalTok{)}
\CommentTok{#>     1 }
\CommentTok{#> 0.818}
\KeywordTok{predict}\NormalTok{(glm2, }\DataTypeTok{newdata =} \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{interessiert =} \OtherTok{TRUE}\NormalTok{), }
        \DataTypeTok{type =} \StringTok{"response"}\NormalTok{)}
\CommentTok{#>     1 }
\CommentTok{#> 0.874}
\end{Highlighting}
\end{Shaded}

Also 82\% bzw. 87\%; kein gewaltig großer Unterschied, aber
immerhin\ldots{}

\section{Multiple logistische
Regression}\label{multiple-logistische-regression}

Können wir unser Model \texttt{glm1} mit nur einer erklärenden Variable
verbessern, indem weiterer Prädiktoren hinzugefügt werden? Verbessern
heißt hier: Können wir die Präzision der Vorhersage verbessern durch
Hinzunahme weiterer Prädiktoren?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{glm3 <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(bestanden_num }\OperatorTok{~}\StringTok{ }\NormalTok{study_time }\OperatorTok{+}\StringTok{ }\NormalTok{interest }\OperatorTok{+}\StringTok{ }\NormalTok{self_eval, }
            \DataTypeTok{family =}\NormalTok{ binomial, }
            \DataTypeTok{data =}\NormalTok{ stats_test)}

\KeywordTok{tidy}\NormalTok{(glm3)}
\CommentTok{#>          term estimate std.error statistic p.value}
\CommentTok{#> 1 (Intercept)   -0.202     0.550    -0.367 0.71359}
\CommentTok{#> 2  study_time    0.115     0.218     0.529 0.59665}
\CommentTok{#> 3    interest   -0.155     0.155    -0.998 0.31820}
\CommentTok{#> 4   self_eval    0.447     0.107     4.173 0.00003}
\end{Highlighting}
\end{Shaded}

Hm, die Interessierten schneiden jetzt - unter Konstanthalten anderer
Einflussfaktoren - \emph{schlechter} ab als die Nicht-Interessierten.
Als Stastistik-Dozent bin ich der Meinung, dieses Ergebnis sollte in der
Schubladen verschwinden (wie es geläufige Praxis ist in vielen
Laboren\ldots{}).

\section{Modellgüte}\label{Modellguete}

Aber wie gut ist das Modell? Und welches Modell von beiden ist besser? R
hat uns kein \(R^2\) ausgegeben. R hat uns deswegen kein \(R^2\)
ausgegeben, weil die Regressionsfunktion nicht über Abweichungsquadrate
bestimmt wird. Stattdessen wird das Maximum Likelihood-Verfahren
eingesetzt. Man kann also kein \(R^2\) ausrechnen, zumindest nicht ohne
Tricks. Einige findige Statistiker haben sich aber Umrechungswege
einfallen lassen, wie man auch ohne Abweichungsquadrate ein \(R^2\)
berechnen kann; weil es kein `echtes' \(R^2\) ist, nennt man es auch
\emph{Pseudo-}\(R^2\). Es gibt ein paar Varianten, wir bleiben bei der
Variante von Herrn McFadden (s. Ausgabe).

Eine Reihe von R-Paketen bieten die Berechnung a:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(BaylorEdPsych)}
\KeywordTok{PseudoR2}\NormalTok{(glm1)}
\KeywordTok{PseudoR2}\NormalTok{(glm2)}
\KeywordTok{PseudoR2}\NormalTok{(glm3)}
\end{Highlighting}
\end{Shaded}

Die Ausgabe zeigt uns, dass das erste Modell schon schlecht ist, dass
zweite praktisch keinen Erkärungwert und das dritte einen zumindest
kleinen bis mittleren Erklärungswert bietet:
\(f^2 = \frac{R^2}{1-R^2}\approx \frac{.1}{.9} = .11\).

\section{Klassifikationskennzahlen}\label{klassifikationskennzahlen}

\subsection{Vier Arten von Ergebnissen einer
Klassifikation}\label{vier-arten-von-ergebnissen-einer-klassifikation}

Logistische Regressionsmodelle werden häufig zur
\emph{Klassifikation}\index{Klassifikation} verwendet. Das heißt man
versucht, Beobachtungen richtig zu zu Klassen zuzuordnen:

\begin{itemize}
\tightlist
\item
  Ein medizinischer Test soll Kranke als krank und Gesunde als gesund
  klassifizieren.
\item
  Ein statistischer Test sollte wahre Hypothesen als wahr und falsche
  Hypothesen als falsch klassifizieren.
\item
  Ein Personaler sollte geeignete Bewerber als geeignet und nicht
  geeignete Bewerber als nicht geeignet einstufen.
\end{itemize}

Diese beiden Arten von Klassifikationen können unterschiedlich gut sein.
Im Extremfall könnte ein Test alle Menschen als krank (`positiv')
einstufen. Mit Sicherheit wurden dann alle Kranken korrekt als krank
diagnostiziert. Dummerweise würde der Test `auf der anderen Seite' viele
Fehler machen: Gesunde als gesund (`negativ') zu klassifizieren.

\begin{quote}
Ein Test, der alle positiven Fälle korrekt als positiv klassifiziert
muss deshalb noch lange nicht alle negativen Fälle als negativ
klassifizieren. Die beiden Werte können unterschiedlich sein.
\end{quote}

Etwas genauer kann man folgende vier Arten von Ergebnisse aus einem Test
erwarten (s. Tabelle \ref{tab:class-stats}, vgl. James, Witten, Hastie,
und Tibshirani
(\protect\hyperlink{ref-introstatlearning}{2013}\protect\hyperlink{ref-introstatlearning}{b})).

\begin{table}

\caption{\label{tab:class-stats}Vier Arten von Ergebnisse von Klassfikationen}
\centering
\begin{tabular}[t]{l|l|l|l}
\hline
Wahrheit & Als negativ (-) vorhergesagt & Als positiv (+) vorhergesagt & Summe\\
\hline
In Wahrheit negativ (-) & Richtig negativ (RN) & Falsch positiv (FP) & N\\
\hline
In Wahrheit positiv (+) & Falsch negativ (FN) & Richtig positiv (RN) & P\\
\hline
Summe & N* & P* & N+P\\
\hline
\end{tabular}
\end{table}

Die logistische Regression gibt uns für jeden Fall eine
Wahrscheinlichkeit zurück, dass der Fall zum Ereignis \(1\) gehört. Wir
müssen dann einen Schwellenwert (threshold) auswählen. Einen Wert also,
der bestimmt, ob der Fall zum Ereignis \(1\) gehört. Häufig nimmt man
\(0.5\). Liegt die Wahrscheinlichkeit unter dem Schwellenwert, so ordnet
man den Fall dem Ereignis \(0\) zu.

Beispiel: Alois' Wahrscheinlichkeit, die Klausur zu bestehen, wird vom
Regressionsmodell auf 51\% geschätzt. Unser Schwellenwert sei 50\%; wir
ordnen Alois der Klasse ``bestehen'' zu. Alois freut sich. Das Modell
sagt also ``bestehen'' (\(1\)) für Alois voraus. Man sagt auch, der
`geschätzte Wert' (\emph{fitted value}) von Alois sei \(1\).

Die aus dem Modell ermittelten Wahrscheinlichkeiten werden dann in einer
sogenannten Konfusionsmatrix\index{Konfusionsmatrix} (\emph{confusion
matrix}) mit den beobachteten Häufigkeiten verglichen:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(cm <-}\StringTok{ }\KeywordTok{confusion.matrix}\NormalTok{(stats_test}\OperatorTok{$}\NormalTok{bestanden_num, glm3}\OperatorTok{$}\NormalTok{fitted.values)) }
\CommentTok{#>     obs}
\CommentTok{#> pred  0   1}
\CommentTok{#>    0  1   1}
\CommentTok{#>    1 37 199}
\CommentTok{#> attr(,"class")}
\CommentTok{#> [1] "confusion.matrix"}
\end{Highlighting}
\end{Shaded}

Dabei stehen \texttt{obs} (observed) für die wahren, also tatsächlich
beobachteten Werte und \texttt{pred} (predicted) für die geschätzten
(vorhergesagten) Werte.

Wie häufig hat unser Modell richtig geschätzt? Genauer: Wie viele echte
\(1\) hat unser Modell als \(1\) vorausgesagt und wie viele echte \(0\)
hat unser Modell als \(0\) vorausgesagt?

\subsection{Klassifikationsgütekennzahlen}\label{klassifikationsgutekennzahlen}

In der Literatur und Praxis herrscht eine recht wilde Vielfalt an
Begriffen dazu, deswegen stellt Tabelle \ref{tab:class-stats} einen
Überblick vor.

\begin{table}

\caption{\label{tab:diag-stats}Geläufige Kennwerte der Klassifikation}
\centering
\begin{tabular}[t]{l|l|l}
\hline
Name & Definition & Synonyme\\
\hline
Falsch-Positiv-Rate (FP-Rate) & FP/N & Alphafehler, Typ-1-Fehler, 1-Spezifität, Fehlalarm\\
\hline
Richtig-Positiv-Rate (RP-Rate) & RP/N & Power, Sensitivität, 1-Betafehler, Recall\\
\hline
Falsch-Negativ-Rate (FN-Rate) & FN/N & Fehlender Alarm, Befafehler\\
\hline
Richtig-Negativ-Rate (RN-Rate) & RN/N & Spezifität, 1-Alphafehler\\
\hline
Positiver Vorhersagewert & RP/P* & Präzision, Relevanz\\
\hline
Negativer Vorhersagewert & RN/N* & Segreganz\\
\hline
Gesamtgenauigkeitsrate & (RP+RN) / (N+P) & Richtigkeit, Korrektklassifikationsrate\\
\hline
\end{tabular}
\end{table}

Zu beachten ist, dass die Gesamtgenauigkeit einer Klassifikation an sich
wenig aussagekräftig ist: Ist eine Krankheit sehr selten, werde ich
durch die einfache Strategie ``diagnostiziere alle als gesund''
insgesamt kaum Fehler machen. Meine Gesamtgenauigkeit wird beeindruckend
genau sein - trotzdem lassen Sie sich davon wohl kaum beeindrucken.
Besser ist, die Richtig-Positiv- und die Richtig-Negativ-Raten getrennt
zu beurteilen. Aus dieser Kombination leitet sich der
\emph{Youden-Index} ab.\index{Youden-Index}. Er berechnet sich als:
\texttt{RP-Rate\ +\ RN-Rate\ -\ 1}.

Sie können die Konfusionsmatrix mit dem Paket
\texttt{confusion.matrix()} aus dem Paket \texttt{SDMTools} berechnen.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sensitivity}\NormalTok{(cm)}
\CommentTok{#> [1] 0.995}
\KeywordTok{specificity}\NormalTok{(cm)}
\CommentTok{#> [1] 0.0263}
\end{Highlighting}
\end{Shaded}

Unser Modell hat es sich recht leicht gemacht: Es hat immer auf
`bestanden' getippt: Damit wurden alle `Besteher' korrekt identifiziert
(Sensitivität = 1); allerdings wurden auch alle `Nicht-Besteher'
übersehen (Spezifität = 0).

Wir könnten jetzt sagen, dass wir im Zweifel lieber eine Person als
Nicht-Besteher einschätzen (um die Lernschwachen noch unterstützen zu
können). Dazu würden wir den Schwellenwert (threshold) von 50\% auf z.B.
80\%\$ heraufsetzen. Erst bei Erreichen des Schwellenwerts
klassifizieren wir die Beobachtung als `bestanden' (1):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(cm <-}\StringTok{ }\KeywordTok{confusion.matrix}\NormalTok{(stats_test}\OperatorTok{$}\NormalTok{bestanden_num, glm3}\OperatorTok{$}\NormalTok{fitted.values, }\DataTypeTok{threshold =}\NormalTok{ .}\DecValTok{8}\NormalTok{))}
\CommentTok{#>     obs}
\CommentTok{#> pred  0   1}
\CommentTok{#>    0 24  47}
\CommentTok{#>    1 14 153}
\CommentTok{#> attr(,"class")}
\CommentTok{#> [1] "confusion.matrix"}
\KeywordTok{sensitivity}\NormalTok{(cm)}
\CommentTok{#> [1] 0.765}
\KeywordTok{specificity}\NormalTok{(cm)}
\CommentTok{#> [1] 0.632}
\end{Highlighting}
\end{Shaded}

\subsection{ROC-Kurven}\label{roc-kurven}

Siehe da! Die Spezifität ist gestiegen, wir haben mehr Nicht-Lerner als
solche identifiziert. Unsere liberalere Strategie hat aber mehr
Falsch-Negative Fälle produziert (geringere Sensitivität). So können wir
jetzt viele verschiedene Schwellenwerte vergleichen.

\begin{quote}
Ein Test ist dann gut, wenn wir für alle möglichen Schwellenwert
ingesamt wenig Fehler produziert.
\end{quote}

Hierzu wird der Cutpoint zwischen 0 und 1 variiert und die
Richtig-Positiv-Rate (Sensitivität) gegen die Falsch-Positiv-Rate
(\(1-\)Spezifität) abgetragen. Das Paket \texttt{pROC} hilft uns hier
weiter. Zuerst berechnen wir für viele verschiedene Schwellenwerte
jeweils die beiden Fehler (Falsch-Positiv-Rate und Falsch-Negativ-Rate).
Trägt man diese in ein Diagramm ab, so bekommt man Abbildung
\ref{fig:roc-stats}, eine sog. \emph{ROC-Kurve}\index{ROC}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lets_roc <-}\StringTok{ }\KeywordTok{roc}\NormalTok{(stats_test}\OperatorTok{$}\NormalTok{bestanden_num, glm3}\OperatorTok{$}\NormalTok{fitted.values)}
\end{Highlighting}
\end{Shaded}

Da die Sensitivität determiniert ist, wenn die Falsch-Positiv-Rate
bekannt ist (1 - FP-Rate), kann man statt Sensitivität auch die FP-Rate
abbilden. Für die Spezifität und die Falsch-Negativ-Rate gilt das
gleiche. In Abbildung \ref{fig:roc-stats} steht auf der X-Achse
Spezifität, aber die Achse ist `rückwärts' (absteigend) skaliert, so
dass die X-Achse identisch ist mit FP-Rate (normal skaliert; d.h.
aufsteigend).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(lets_roc)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{072_klassifizierende_Regression_files/figure-latex/roc-stats-1} 

}

\caption{Eine ROC-Kurve}\label{fig:roc-stats}
\end{figure}

Die `Fläche unter der Kurve' (area under curve, AUC) ist damit ein Maß
für die Güte des Tests. Abbildung \ref{fig:example-rocs} stellt drei
Beispiele von Klassifikationsgüten dar: sehr gute (A), gute (B) und
schlechte (C). Ein hohe Klassifikationsgüte zeigt sich daran, dass eine
hohe Richtig-Positiv-Rate mit einer kleinen Fehlalarmquote einher geht:
Wir finden alle Kranken, aber nur die Kranken. Die AUC-Kurve ``hängt
oben links an der Decke''. Ein schlechter Klassifikator trifft so gut
wie ein Münzwurf: Ist das Ereignis selten, hat er eine hohe
Falsch-Positiv-Rate und eine geringe Falsch-Negativ-Rate. Ist das
Ereignis hingegen häufig, liegen die Fehlerhöhen genau umgekehrt: Eine
hohe Richtig-Positiv-Rate wird mit einer hoher Falsch-Positiv-Rate
einher.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{072_klassifizierende_Regression_files/figure-latex/example-rocs-1} 

}

\caption{Beispiel für eine sehr gute (A), gute (B) und schlechte (C) Klassifikation}\label{fig:example-rocs}
\end{figure}

Fragt sich noch, wie man den besten Schwellenwert herausfindet. Den
besten Schwellenwert kann man als besten Youden-Index-Wert verstehen. Im
Paket \texttt{pROC} gibt es dafür den Befehl \texttt{coords}, der uns im
ROC-Diagramm die Koordinaten des besten Schwellenwert und den Wert
dieses besten Schwellenwerts liefert:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{coords}\NormalTok{(lets_roc, }\StringTok{"best"}\NormalTok{)}
\CommentTok{#>   threshold specificity sensitivity }
\CommentTok{#>       0.874       0.868       0.595}
\end{Highlighting}
\end{Shaded}

\section[Aufgaben]{\texorpdfstring{Aufgaben\footnote{R, R, R, R, F, R, R}}{Aufgaben}}\label{aufgaben-14}

\BeginKnitrBlock{rmdexercises}
Richtig oder Falsch!?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Die logistische Regression ist eine Regression für dichotome
  Kriterien.
\item
  Unter einer \sout{Olive}Ogive versteht man eine eine ``s-förmige''
  Kurve.
\item
  Berechnet man eine ``normale'' (OLS-)Regression bei einem dichotomen
  Kriterium, so kann man Wahrscheinlichkeiten \textless{} 0 oder
  \textgreater{} 1 erhalten, was keinen Sinn macht.
\item
  Ein Logit ist definiert als der Einfluss eines Prädiktors in der
  logistischen Regression. Der Koeffizient berechnet sich als
  Logarithmus des Wettquotienten.
\item
  Das AIC ein Gütemaß, welches man bei der logistischten Regression
  generell vermeidet.
\item
  Eine Klassifikation kann 4 Arten von Ergebnissen bringen - gemessen an
  der Richtigkeit des Ergebnisses.
\item
  Der `positive Vorhersagewert' ist definiert als der Anteil aller
  richtig-positiven Klassifikationen an allen als positiv
  klassifizierten Objekten.
\end{enumerate}
\EndKnitrBlock{rmdexercises}

\section{Befehlsübersicht}\label{befehlsubersicht-7}

Tabelle \ref{tab:befehle-logist-regression} stellt die Befehle dieses
Kapitels dar.

\begin{table}

\caption{\label{tab:befehle-logist-regression}Befehle des Kapitels 'Logistische Regression'}
\centering
\begin{tabular}[t]{l|l}
\hline
Paket..Funktion & Beschreibung\\
\hline
ggplot2::geom\_abline & Fügt das Geom "abline" (normale Gerade) hinzu\\
\hline
glm & Berechnet eine logistische Regression\\
\hline
exp & Berechnet die e-Funktion\\
\hline
SDMTools::confusion.matrix & Berechnet eine Konfusionsmatrix\\
\hline
SDMTools::sensitivity & Berechnet die Sensitivität eines Klassifikationsmodells\\
\hline
SDMTools::specificity & Berechnet die Spezifität eines Klassifikationsmodells\\
\hline
ROCR::performance & Erstellt Objekte mit Gütekennzahlen von Klassifikationsmodellen\\
\hline
BaylorEdPsych::PseudoR2 & Berechnet Pseudo-R-Quadrat-Werte\\
\hline
\end{tabular}
\end{table}

\chapter{Fallstudien zum geleiteten
Modellieren}\label{fallstudien-zum-geleiteten-modellieren}

In diesem Kapitel werden folgende Pakete benötigt.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)  }\CommentTok{# Datenjudo}
\KeywordTok{library}\NormalTok{(psych)  }\CommentTok{# Befehl 'describe'}
\KeywordTok{library}\NormalTok{(broom)  }\CommentTok{# lm-Ergebnisse aufpolieren}
\KeywordTok{library}\NormalTok{(corrplot)  }\CommentTok{# Korrelationstabellen visualisieren}
\KeywordTok{library}\NormalTok{(titanic)  }\CommentTok{# Für Datensatz 'titanic'}
\KeywordTok{library}\NormalTok{(compute.es)  }\CommentTok{# Effektstärken berechnen}
\end{Highlighting}
\end{Shaded}

\section{Überleben auf der Titanic}\label{uberleben-auf-der-titanic}

In dieser YACSDA\footnote{Yet-another-case-study-on-data-analysis} geht
es um die beispielhafte Analyse nominaler Daten anhand des
``klassischen'' Falls zum Untergang der Titanic. Eine Frage, die sich
hier aufdrängt, lautet: Kann (konnte) man sich vom Tod freikaufen, etwas
polemisch formuliert. Oder neutraler: Hängt die Überlebensquote von der
Klasse, in der der Passagiers reist, ab?

\subsection{Daten laden}\label{daten-laden}

Mit dem Befehl \texttt{data} kann man Daten aus Paketen laden; lässt man
den Paramter \texttt{package} weg, so werden alle geladenen Pakete nach
diesem Datensatz durchsucht. Benennt man den Parameter, so kann man auch
\emph{nicht} geladene Pakete damit ansteuern.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(titanic_train, }\DataTypeTok{package =} \StringTok{"titanic"}\NormalTok{)}
\NormalTok{titanic_train <-}\StringTok{ }\KeywordTok{na.omit}\NormalTok{(titanic_train)}
\end{Highlighting}
\end{Shaded}

\subsection{Erster Blick}\label{erster-blick}

Werfen Sie einen ersten Blick in die Daten mit
\texttt{glimpse(titanic\_train)}. Lassen Sie sich dann einige
deskriptive Statistiken ausgeben\footnote{z.B. mit
  \texttt{titanic\_train\ \%\textgreater{}\%\ count(Survived)} oder
  \texttt{titanic\_train\ \%\textgreater{}\%\ summarise(Ticketpreis\ =\ mean(Fare,\ na.rm\ =\ TRUE))}}

\subsection{Welche Variablen sind
interessant?}\label{welche-variablen-sind-interessant}

Von 12 Variablen des Datensatzes interessieren uns offenbar
\texttt{Pclass} und \texttt{Survived}; Hilfe zum Datensatz kann man
übrigens mit \texttt{help(titanic\_train)} bekommen. Diese beiden
Variablen sind kategorial (nicht-metrisch), wobei sie in der Tabelle mit
Zahlen kodiert sind. Natürlich ändert die Art der Codierung (hier als
Zahl) nichts am eigentlichen Skalenniveau. Genauso könnte man ``Mann''
mit \texttt{1} und ``Frau'' mit \texttt{2} kodieren; ein Mittelwert
bliebe genauso (wenig) aussagekräftig. Zu beachten ist hier nur, dass
sich manche R-Befehle verunsichern lassen, wenn nominale Variablen mit
Zahlen kodiert sind. Daher ist es oft besser, nominale Variablen mit
Text-Werten zu benennen (wie ``survived'' vs. ``drowned'' etc.). Wir
kommen später auf diesen Punkt zurück.

\subsection{Univariate Häufigkeiten}\label{univariate-haufigkeiten}

Bevor wir uns in kompliziertere Fragestellungen stürzen, halten wir
fest: Wir untersuchen zwei nominale Variablen. Sprich: wir werden
Häufigkeiten auszählen. Häufigkeiten (und relative Häufigkeiten, also
Anteile oder Quoten) sind das, was uns hier beschäftigt.

Zählen wir zuerst die univariaten Häufigkeiten aus: Wie viele Passagiere
gab es pro Klasse? Wie viele Passagiere gab es pro Wert von
\texttt{Survived} (also die überlebten bzw. nicht überlebten)?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{c1 <-}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{count}\NormalTok{(titanic_train, Pclass)}
\NormalTok{c1}
\CommentTok{#> # A tibble: 3 x 2}
\CommentTok{#>   Pclass     n}
\CommentTok{#>    <int> <int>}
\CommentTok{#> 1      1   186}
\CommentTok{#> 2      2   173}
\CommentTok{#> 3      3   355}
\end{Highlighting}
\end{Shaded}

\BeginKnitrBlock{rmdcaution}
Achtung - Namenskollision! Sowohl im Paket \texttt{mosaic} als auch im
Paket \texttt{dplyr} gibt es einen Befehl \texttt{count}. Für
\texttt{select} gilt Ähnliches - und für eine Reihe anderer Befehle
auch. Das arme R weiß nicht, welchen von beiden wir meinen und
entscheidet sich im Zweifel für den falschen. Da hilft, zu sagen, aus
welchem Paket wir den Befehl beziehen wollen. Das macht der Operator
\texttt{::}. Probieren Sie die Funktion \texttt{find\_funs} aus Kapitel
\ref{funs-pckgs}, um herauszufinden, welche Pakete z.B. den Befehl
\texttt{count} beherbergen.
\EndKnitrBlock{rmdcaution}

Aha. Zur besseren Anschaulichkeit können Sie das auch plotten (ein
Diagramm dazu malen). Wie?\footnote{\texttt{qplot(x\ =\ Pclass,\ y\ =\ n,\ data\ =\ c1)}}

Der Befehl \texttt{qplot} zeichnet automatisch Punkte, wenn auf beiden
Achsen ``Zahlen-Variablen'' stehen (also Variablen, die keinen ``Text'',
sondern nur Zahlen beinhalten. In R sind das Variablen vom Typ
\texttt{int} (integer), also Ganze Zahlen oder vom Typ \texttt{num}
(numeric), also reelle Zahlen).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{c2 <-}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{count}\NormalTok{(titanic_train, Survived)}
\NormalTok{c2}
\CommentTok{#> # A tibble: 2 x 2}
\CommentTok{#>   Survived     n}
\CommentTok{#>      <int> <int>}
\CommentTok{#> 1        0   424}
\CommentTok{#> 2        1   290}
\end{Highlighting}
\end{Shaded}

Man beachte, dass der Befehl \texttt{count} stehts eine Tabelle
(data.frame bzw. \texttt{tibble}) verlangt und zurückliefert.

\subsection{Bivariate Häufigkeiten}\label{bivariate-haufigkeiten}

OK, gut. Jetzt wissen wir die Häufigkeiten pro Wert von
\texttt{Survived} (dasselbe gilt für \texttt{Pclass}). Eigentlich
interessiert uns aber die Frage, ob sich die relativen Häufigkeiten der
Stufen von \texttt{Pclass} innerhalb der Stufen von \texttt{Survived}
unterscheiden. Einfacher gesagt: Ist der Anteil der Überlebenden in der
1. Klasse größer als in der 3. Klasse?

Zählen wir zuerst die Häufigkeiten für alle Kombinationen von
\texttt{Survived} und \texttt{Pclass}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{c3 <-}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{count}\NormalTok{(titanic_train, Survived, Pclass)}
\NormalTok{c3}
\CommentTok{#> # A tibble: 6 x 3}
\CommentTok{#>   Survived Pclass     n}
\CommentTok{#>      <int>  <int> <int>}
\CommentTok{#> 1        0      1    64}
\CommentTok{#> 2        0      2    90}
\CommentTok{#> 3        0      3   270}
\CommentTok{#> 4        1      1   122}
\CommentTok{#> 5        1      2    83}
\CommentTok{#> 6        1      3    85}
\end{Highlighting}
\end{Shaded}

Da \texttt{Pclass} 3 Stufen hat (1., 2. und 3. Klasse) und innerhalb
jeder dieser 3 Klassen es die Gruppe der Überlebenden und der
Nicht-Überlebenden gibt, haben wir insgesamt 3*2=6 Gruppen.

Es ist hilfreich, sich diese Häufigkeiten wiederum zu plotten; probieren
Sie \texttt{qplot(x\ =\ Pclass,\ y\ =\ n,\ data\ =\ c3)}.

Hm, nicht so hilfreich. Schöner wäre, wenn wir (farblich) erkennen
könnten, welcher Punkt für ``Überlebt'' und welcher Punkt für
``Nicht-Überlebt'' steht. Mit \texttt{qplot} geht das recht einfach: Wir
sagen der Funktion \texttt{qplot}, dass die Farbe (\texttt{color}) der
Punkte den Stufen von \texttt{Survived} zugeordnet werden sollen:
\texttt{qplot(x\ =\ Pclass,\ y\ =\ n,\ color\ =\ Survived,\ data\ =\ c3)}.

Viel besser. Was noch stört, ist, dass \texttt{Survived} als metrische
Variable verstanden wird. Das Farbschema lässt Nuancen, feine
Farbschattierungen, zu. Für nominale Variablen macht das keinen Sinn; es
gibt da keine Zwischentöne. Tot ist tot, lebendig ist lebendig. Wir
sollten daher der Funktion sagen, dass es sich um nominale Variablen
handelt (s. Abbildung \ref{fig:titanic1}).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{factor}\NormalTok{(Pclass), }\DataTypeTok{y =}\NormalTok{ n, }\DataTypeTok{color =} \KeywordTok{factor}\NormalTok{(Survived), }\DataTypeTok{data =}\NormalTok{ c3)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{075_Fallstudien_Titanic_Affairs_files/figure-latex/titanic1-1} 

}

\caption{Überlebensraten auf der Titanic, in Abhängigkeit von der Passagierklasse}\label{fig:titanic1}
\end{figure}

Viel besser. Jetzt fügen Sie noch noch ein bisschen Schnickschnack
hinzu:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{factor}\NormalTok{(Pclass), }\DataTypeTok{y =}\NormalTok{ n, }\DataTypeTok{color =} \KeywordTok{factor}\NormalTok{(Survived), }\DataTypeTok{data =}\NormalTok{ c3) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Klasse"}\NormalTok{, }
       \DataTypeTok{title =} \StringTok{"Überleben auf der Titanic"}\NormalTok{,}
       \DataTypeTok{colour =} \StringTok{"Überlebt?"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Signifikanztest}\label{signifikanztest}

Manche Leute mögen Signifikanztests. Ich persönlich stehe ihnen kritisch
gegenüber, da ein p-Wert eine Funktion der Stichprobengröße ist und
außerdem zumeist missverstanden wird (und er gibt \emph{nicht} die
Wahrscheinlichkeit der getesteten Hypothese an, was die Frage aufwirft,
warum er mich dann interessieren sollte). Aber seisdrum, berechnen wir
mal einen p-Wert. Es gibt mehrere statistische Tests, die sich hier
potenziell anböten und unterschiedliche Ergebnisse liefern können
(Briggs
\protect\hyperlink{ref-breaking}{2008}\protect\hyperlink{ref-breaking}{a})
(was die Frage nach der Objektivität von statistischen Tests in ein
ungünstiges Licht rückt). Nehmen wir den \(\chi^2\)-Test.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{chisq.test}\NormalTok{(titanic_train}\OperatorTok{$}\NormalTok{Survived, titanic_train}\OperatorTok{$}\NormalTok{Pclass)}
\CommentTok{#> }
\CommentTok{#>  Pearson's Chi-squared test}
\CommentTok{#> }
\CommentTok{#> data:  titanic_train$Survived and titanic_train$Pclass}
\CommentTok{#> X-squared = 90, df = 2, p-value <2e-16}
\end{Highlighting}
\end{Shaded}

Der p-Wert ist kleiner als 5\%, daher entscheiden wir uns, entsprechend
der üblichen Gepflogenheit, gegen die H0 und für die H1: ``Es gibt einen
Zusammenhang von Überlebensrate und Passagierklasse''.

\subsection{Effektstärke}\label{effektstarke-1}

Abgesehen von der Signifikanz, und interessanter, ist die Frage, wie
sehr die Variablen zusammenhängen. Für Häufigkeitsanalysen mit
2*2-Feldern bietet sich das ``Odds Ratio'' (OR), das Chancenverhältnis
an. Das Chancen-Verhältnis beantwortet die Frage: ``Um welchen Faktor
ist die Überlebenschance in der einen Klasse größer als in der anderen
Klasse?''. Eine interessante Frage, als schauen wir es uns an.

Das OR ist nur definiert für 2*2-Häufigkeitstabellen, daher müssen wir
die Anzahl der Passagierklassen von 3 auf 2 verringern. Nehmen wir nur
1. und 3. Klasse, um den vermuteten Effekt deutlich herauszuschälen:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t2 <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(titanic_train, Pclass }\OperatorTok{!=}\StringTok{ }\DecValTok{2}\NormalTok{)  }\CommentTok{# "!=" heißt "nicht"}
\end{Highlighting}
\end{Shaded}

Alternativ (synonym) könnten wir auch schreiben:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t2 <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(titanic_train, Pclass }\OperatorTok{==}\StringTok{ }\DecValTok{1} \OperatorTok{|}\StringTok{ }\NormalTok{Pclass }\OperatorTok{==}\StringTok{ }\DecValTok{3}\NormalTok{)  }\CommentTok{# "|" heißt "oder"}
\end{Highlighting}
\end{Shaded}

Und dann zählen wir wieder die Häufigkeiten aus pro Gruppe:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{c4 <-}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{count}\NormalTok{(t2, Pclass)}
\NormalTok{c4}
\CommentTok{#> # A tibble: 2 x 2}
\CommentTok{#>   Pclass     n}
\CommentTok{#>    <int> <int>}
\CommentTok{#> 1      1   186}
\CommentTok{#> 2      3   355}
\end{Highlighting}
\end{Shaded}

Schauen wir nochmal den p-Wert an, da wir jetzt ja mit einer veränderten
Datentabelle operieren:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{chisq.test}\NormalTok{(t2}\OperatorTok{$}\NormalTok{Survived, t2}\OperatorTok{$}\NormalTok{Pclass)}
\CommentTok{#> }
\CommentTok{#>  Pearson's Chi-squared test with Yates' continuity correction}
\CommentTok{#> }
\CommentTok{#> data:  t2$Survived and t2$Pclass}
\CommentTok{#> X-squared = 90, df = 1, p-value <2e-16}
\end{Highlighting}
\end{Shaded}

Ein \(\chi^2\)-Wert von \textasciitilde{}96 bei einem \emph{n} von 707.

Dann berechnen wir die Effektstärke (OR) mit dem Paket
\texttt{compute.es} (muss ebenfalls installiert sein)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{compute.es}\OperatorTok{::}\KeywordTok{chies}\NormalTok{(}\DataTypeTok{chi.sq =} \DecValTok{96}\NormalTok{, }\DataTypeTok{n =} \DecValTok{707}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

Das OR beträgt also etwa 4.21. Die Chance zu überleben ist also in der
1. Klasse mehr als 4 mal so hoch wie in der 3. Klasse. Es scheint: Money
buys you life\ldots{}

\subsection{Logististische Regression}\label{logististische-regression}

Berechnen wir noch das Odds Ratio mit Hilfe der logistischen Regression.
Zum Einstieg: Ignorieren Sie die folgende Syntax und schauen Sie sich
das Diagramm an. Hier sehen wir die (geschätzten)
Überlebens-Wahrscheinlichkeiten für Passagiere der 1. Klasse
vs.~Passagiere der 2. vs.~der 3. Klasse.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{glm1 <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ titanic_train, }
            \DataTypeTok{formula =}\NormalTok{ Survived }\OperatorTok{~}\StringTok{ }\NormalTok{Pclass, }
            \DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{)}

\KeywordTok{exp}\NormalTok{(}\KeywordTok{coef}\NormalTok{(glm1))}
\CommentTok{#> (Intercept)      Pclass }
\CommentTok{#>       5.056       0.402}

\NormalTok{titanic_train}\OperatorTok{$}\NormalTok{pred_prob <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(glm1, }\DataTypeTok{type =} \StringTok{"response"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{075_Fallstudien_Titanic_Affairs_files/figure-latex/fig-titanic-1} 

}

\caption{Logistische Regression zur Überlebensrate nach Passagierklasse}\label{fig:fig-titanic}
\end{figure}

Wir sehen, dass die Überlebens-Wahrscheinlichkeit in der 1. Klasse höher
ist als in der 3. Klasse. Optisch grob geschätzt, \textasciitilde{}60\%
in der 1. Klasse und \textasciitilde{}25\% in der 3. Klasse.

Schauen wir uns die logistische Regression an: Zuerst haben wir den
Datensatz auf die Zeilen beschränkt, in denen Personen aus der 1. und 3.
Klasse vermerkt sind (zwecks Vergleichbarkeit zu oben). Dann haben wir
mit \texttt{glm} und \texttt{family\ =\ "binomial"} eine
\emph{logistische} Regression angefordert. Man beachte, dass der Befehl
sehr ähnlich zur normalen Regression (\texttt{lm(...)}) ist.

Da die Koeffizienten in der Logit-Form zurückgegeben werden, haben wir
sie mit der Exponential-Funktion in die ``normale'' Odds-Form gebracht
(delogarithmiert, boa) mithilfe von \texttt{exp(coef)}. Wir sehen, dass
sich die Überlebens-\emph{Chance} (Odds; nicht Wahrscheinlichkeit) um
den Faktor .4 verringert pro zusätzlicher Stufe der Passagierklase.
Würde jemand in der ``nullten'' Klasse fahren, wäre seine
Überlebenschance ca. 5:1 (5/6, gut 80\%). Die Überlebenschance sind der
1. Klasse sind demnach etwa: \texttt{5*\ 0.4}, also 2:1, etwa 67\%.

Komfortabler können wir uns die Überlebens-\emph{Wahrscheinlichkeiten}
mit der Funktion \texttt{predict} ausgeben lassen.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{predict}\NormalTok{(glm1, }\DataTypeTok{newdata =} \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{Pclass =} \DecValTok{1}\NormalTok{), }\DataTypeTok{type =} \StringTok{"response"}\NormalTok{)}
\CommentTok{#>    1 }
\CommentTok{#> 0.67}
\KeywordTok{predict}\NormalTok{(glm1, }\DataTypeTok{newdata =} \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{Pclass =} \DecValTok{2}\NormalTok{), }\DataTypeTok{type =} \StringTok{"response"}\NormalTok{)}
\CommentTok{#>     1 }
\CommentTok{#> 0.449}
\KeywordTok{predict}\NormalTok{(glm1, }\DataTypeTok{newdata =} \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{Pclass =} \DecValTok{3}\NormalTok{), }\DataTypeTok{type =} \StringTok{"response"}\NormalTok{)}
\CommentTok{#>     1 }
\CommentTok{#> 0.247}
\end{Highlighting}
\end{Shaded}

Alternativ kann man die tatsächlichen (beobachteten) Häufigkeiten auch
noch ``per Hand'' bestimmen:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{titanic_train }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(Pclass }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(Survived, Pclass) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(Pclass, Survived) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{n =} \KeywordTok{n}\NormalTok{() ) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Anteil =}\NormalTok{ n }\OperatorTok{/}\StringTok{ }\KeywordTok{sum}\NormalTok{(n))}
\CommentTok{#> # A tibble: 4 x 4}
\CommentTok{#> # Groups:   Pclass [2]}
\CommentTok{#>   Pclass Survived     n Anteil}
\CommentTok{#>    <int>    <int> <int>  <dbl>}
\CommentTok{#> 1      1        0    64  0.344}
\CommentTok{#> 2      1        1   122  0.656}
\CommentTok{#> 3      3        0   270  0.761}
\CommentTok{#> 4      3        1    85  0.239}
\end{Highlighting}
\end{Shaded}

Übersetzen wir dies Syntax auf Deutsch:

\BeginKnitrBlock{rmdpseudocode}
Nehme den Datensatz ``titanic\_train'' UND DANN\\
Filtere nur die 1. und die 3. Klasse heraus UND DANN\\
wähle nur die Spalten ``Survived'' und ``Pclass'' UND DANN\\
gruppiere nach ``Pclass'' und ``Survived'' UND DANN\\
zähle die Häufigkeiten für jede dieser Gruppen aus UND DANN\\
berechne den Anteil an Überlebenden bzw. Nicht-Überlebenden\\
für jede der beiden Passagierklassen. FERTIG.
\EndKnitrBlock{rmdpseudocode}

\subsection{Effektstärken visualieren}\label{effektstarken-visualieren}

Zum Abschluss schauen wir uns die Stärke des Zusammenhangs noch einmal
graphisch an. Wir berechnen dafür die relativen Häufigkeiten pro Gruppe
(im Datensatz ohne 2. Klasse, der Einfachheit halber).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{c5 <-}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{count}\NormalTok{(t2, Pclass, Survived)}
\NormalTok{c5}\OperatorTok{$}\NormalTok{prop <-}\StringTok{ }\NormalTok{c5}\OperatorTok{$}\NormalTok{n }\OperatorTok{/}\StringTok{ }\DecValTok{707}
\NormalTok{c5}
\CommentTok{#> # A tibble: 4 x 4}
\CommentTok{#>   Pclass Survived     n   prop}
\CommentTok{#>    <int>    <int> <int>  <dbl>}
\CommentTok{#> 1      1        0    64 0.0905}
\CommentTok{#> 2      1        1   122 0.1726}
\CommentTok{#> 3      3        0   270 0.3819}
\CommentTok{#> 4      3        1    85 0.1202}
\end{Highlighting}
\end{Shaded}

Genauer gesagt haben die Häufigkeiten pro Gruppe in Bezug auf die
Gesamtzahl aller Passagiere berechnet; die vier Anteile addieren sich
also zu 1 auf. Das visualisieren wir wieder, s. Abbildung
\ref{fig:titanic2}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{factor}\NormalTok{(Pclass), }
      \DataTypeTok{y =}\NormalTok{ prop, }
      \DataTypeTok{fill =} \KeywordTok{factor}\NormalTok{(Survived), }
      \DataTypeTok{data =}\NormalTok{ c5, }
      \DataTypeTok{geom =} \StringTok{"col"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{075_Fallstudien_Titanic_Affairs_files/figure-latex/titanic2-1} 

}

\caption{Absolute Überlebenshäufigkeiten}\label{fig:titanic2}
\end{figure}

Das \texttt{geom\ =\ "col"} heißt, dass als ``geometrisches Objekt''
dieses Mal keine Punkte, sondern Säulen (columns) verwendet werden
sollen.

Ganz nett, aber die Häufigkeitsunterscheide von \texttt{Survived}
zwischen den beiden Werten von \texttt{Pclass} stechen noch nicht so ins
Auge. Wir sollten es anders darstellen. Hier kommt der Punkt, wo wir von
\texttt{qplot} auf seinen großen Bruder, \texttt{ggplot} wechseln
sollten. \texttt{qplot} ist in Wirklichkeit nur eine vereinfachte Form
von \texttt{ggplot}; die Einfachheit wird mit geringeren Möglichkeiten
bezahlt. Satteln wir zum Schluss dieser Fallstudie also um, s. Abbildung
\ref{fig:titanic3}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ c5) }\OperatorTok{+}
\StringTok{  }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{factor}\NormalTok{(Pclass), }\DataTypeTok{y =}\NormalTok{ n, }\DataTypeTok{fill =} \KeywordTok{factor}\NormalTok{(Survived)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_col}\NormalTok{(}\DataTypeTok{position =} \StringTok{"fill"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Passagierklasse"}\NormalTok{, }
       \DataTypeTok{fill =} \StringTok{"Überlebt?"}\NormalTok{, }
       \DataTypeTok{caption =} \StringTok{"Nur Passagiere, keine Besatzung"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{075_Fallstudien_Titanic_Affairs_files/figure-latex/titanic3-1} 

}

\caption{Relative Überlebenshäufigkeiten}\label{fig:titanic3}
\end{figure}

Jeden sehen wir die Häufigkeiten des Überlebens bedingt auf die
Passagierklasse besser. Wir sehen auf den ersten Blick, dass sich die
Überlebensraten deutlich unterscheiden: Im linken Balken überleben die
meisten; im rechten Balken ertrinken die meisten. Mit \texttt{labs}
haben wir noch die X-Achse (\texttt{x}), die Bezeichnung der Füllfarbe
(\texttt{fill}) sowie die Legende des Diagramms beschrieben. Diese
letzte Analyse zeigt schön die Kraft von (Daten-)Visualisierungen auf.
Der zu untersuchende Effekt tritt hier am stärken zu Tage; außerdem ist
die Analyse relativ einfach.

Eine alternative Darstellung zeigt Abbildung \ref{fig:titanic4}. Hier
werden die vier ``Fliesen'' gleich groß dargestellt; die Fallzahl wird
durch die Füllfarbe besorgt.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{c5 }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{ggplot }\OperatorTok{+}
\StringTok{  }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{factor}\NormalTok{(Pclass), }\DataTypeTok{y =} \KeywordTok{factor}\NormalTok{(Survived), }\DataTypeTok{fill =}\NormalTok{ n) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_tile}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{075_Fallstudien_Titanic_Affairs_files/figure-latex/titanic4-1} 

}

\caption{Überlebenshäufigkeiten anhand eines Fliesendiagramms dargestellt}\label{fig:titanic4}
\end{figure}

\subsection{Fazit}\label{fazit-1}

In der Datenanalyse (mit R) kommt man mit wenigen Befehlen schon sehr
weit; \texttt{dplyr} und \texttt{ggplot2} zählen (zu Recht) zu den am
häufigsten verwendeten Paketen. Beide sind flexibel, konsistent und
spielen gerne miteinander. Die besten Einblicke haben wir aus
deskriptiver bzw. explorativer Analyse (Diagramme) gewonnen.
Signifikanztests oder komplizierte Modelle waren nicht zentral. In
vielen Studien/Projekten der Datenanalyse gilt ähnliches: Daten umformen
und verstehen bzw. ``veranschaulichen'' sind zentrale Punkte, die häufig
viel Zeit und Wissen fordern. Bei der Analyse von nominalskalierten sind
Häufigkeitsauswertungen ideal.

\section{Außereheliche Affären}\label{auereheliche-affaren}

Für diese Fallstudie benötigen wir folgende Pakete:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(AER)  }\CommentTok{# Datensatz 'Affairs'}
\KeywordTok{library}\NormalTok{(psych)  }\CommentTok{# Befehl 'describe'}
\KeywordTok{library}\NormalTok{(tidyverse)  }\CommentTok{# Datenjudo}
\KeywordTok{library}\NormalTok{(broom)  }\CommentTok{# Befehl 'tidy`'}
\end{Highlighting}
\end{Shaded}

Wovon ist die Häufigkeit von Affären (Seitensprüngen) in Ehen abhängig?
Diese Frage soll anhand des Datensatzes \texttt{Affairs} untersucht
werden. Laden wir als erstes den Datensatz in R.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(Affairs, }\DataTypeTok{package =} \StringTok{"AER"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Verschaffen Sie sich zum Einstieg einen Überblick über die Daten.
\ldots{} OK, scheint zu passen. Was jetzt?

\subsection{Zentrale Statistiken}\label{zentrale-statistiken}

Geben Sie zentrale deskriptive Statistiken an für Affärenhäufigkeit und
Ehezufriedenheit!

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# nicht robust:}
\KeywordTok{mean}\NormalTok{(Affairs}\OperatorTok{$}\NormalTok{affairs, }\DataTypeTok{na.rm =}\NormalTok{ T)}
\CommentTok{#> [1] 1.46}
\KeywordTok{sd}\NormalTok{(Affairs}\OperatorTok{$}\NormalTok{affairs, }\DataTypeTok{na.rm =}\NormalTok{ T)}
\CommentTok{#> [1] 3.3}
\CommentTok{# robust:}
\KeywordTok{median}\NormalTok{(Affairs}\OperatorTok{$}\NormalTok{Affairs, }\DataTypeTok{na.rm =}\NormalTok{ T)}
\CommentTok{#> NULL}
\KeywordTok{IQR}\NormalTok{(Affairs}\OperatorTok{$}\NormalTok{Affairs, }\DataTypeTok{na.rm =}\NormalTok{ T)}
\CommentTok{#> [1] NA}
\end{Highlighting}
\end{Shaded}

Es scheint, die meisten Leute haben keine Affären:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{count}\NormalTok{(Affairs, affairs)}
\CommentTok{#> # A tibble: 6 x 2}
\CommentTok{#>   affairs     n}
\CommentTok{#>     <dbl> <int>}
\CommentTok{#> 1       0   451}
\CommentTok{#> 2       1    34}
\CommentTok{#> 3       2    17}
\CommentTok{#> 4       3    19}
\CommentTok{#> 5       7    42}
\CommentTok{#> 6      12    38}
\end{Highlighting}
\end{Shaded}

Man kann sich viele Statistiken mit dem Befehl \texttt{describe} aus
\texttt{psych} ausgeben lassen, das ist etwas praktischer:

\begin{Shaded}
\begin{Highlighting}[]

\KeywordTok{describe}\NormalTok{(Affairs}\OperatorTok{$}\NormalTok{affairs)}
\CommentTok{#>    vars   n mean  sd median trimmed mad min max range skew kurtosis   se}
\CommentTok{#> X1    1 601 1.46 3.3      0    0.55   0   0  12    12 2.34     4.19 0.13}
\KeywordTok{describe}\NormalTok{(Affairs}\OperatorTok{$}\NormalTok{rating)}
\CommentTok{#>    vars   n mean  sd median trimmed  mad min max range  skew kurtosis   se}
\CommentTok{#> X1    1 601 3.93 1.1      4    4.07 1.48   1   5     4 -0.83    -0.22 0.04}
\end{Highlighting}
\end{Shaded}

Dazu muss das Paket \texttt{psych} natürlich vorher installiert sein.
Beachten Sie, dass man ein Paket nur \emph{einmal} installieren muss ,
aber jedes Mal, wenn Sie R starten, auch starten muss (mit
\texttt{library}; vgl. Kapitel \ref{Rahmen}).

\subsection{Visualisieren}\label{visualisieren}

Visualisieren Sie zentrale Variablen!

Sicherlich sind Diagramme auch hilfreich. Dies geht wiederum mit dem
R-Commander oder z.B. mit folgenden Befehlen:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ affairs, }\DataTypeTok{data =}\NormalTok{ Affairs)}
\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ rating, }\DataTypeTok{data =}\NormalTok{ Affairs)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{075_Fallstudien_Titanic_Affairs_files/figure-latex/plot-affairs1-1} \includegraphics[width=0.7\linewidth]{075_Fallstudien_Titanic_Affairs_files/figure-latex/plot-affairs1-2} \end{center}

Die meisten Menschen (dieser Stichprobe) scheinen mit Ihrer Beziehung
sehr zufrieden zu sein.

\subsection{Wer ist zufriedener mit der Partnerschaft: Personen mit
Kindern oder
ohne?}\label{wer-ist-zufriedener-mit-der-partnerschaft-personen-mit-kindern-oder-ohne}

Nehmen wir dazu mal ein paar \texttt{dplyr}-Befehle:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}
\NormalTok{Affairs }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(children) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{rating_children =} 
              \KeywordTok{mean}\NormalTok{(rating, }\DataTypeTok{na.rm =}\NormalTok{ T))}
\CommentTok{#> # A tibble: 2 x 2}
\CommentTok{#>   children rating_children}
\CommentTok{#>     <fctr>           <dbl>}
\CommentTok{#> 1       no            4.27}
\CommentTok{#> 2      yes            3.80}
\end{Highlighting}
\end{Shaded}

Ah! Kinder sind also ein Risikofaktor für eine Partnerschaft! Gut, dass
wir das geklärt haben.

\subsection{Vertiefung: Wie viele fehlende Werte gibt
es?}\label{vertiefung-wie-viele-fehlende-werte-gibt-es}

Was machen wir am besten damit?

Diesen Befehl könnten wir für jede Spalte ausführen:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(Affairs}\OperatorTok{$}\NormalTok{affairs))}
\CommentTok{#> [1] 0}
\end{Highlighting}
\end{Shaded}

Oder lieber alle auf einmal:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Affairs }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise_all}\NormalTok{(}\KeywordTok{funs}\NormalTok{(}\KeywordTok{sum}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(.))))}
\CommentTok{#>   affairs gender age yearsmarried children religiousness education}
\CommentTok{#> 1       0      0   0            0        0             0         0}
\CommentTok{#>   occupation rating}
\CommentTok{#> 1          0      0}
\end{Highlighting}
\end{Shaded}

Übrigens gibt es ein gutes
\href{https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf}{Cheat
Sheet} für \texttt{dplyr}.

Ah, gut, keine fehlenden Werte. Das macht uns das Leben leichter.

\subsection{Wer ist glücklicher: Männer oder
Frauen?}\label{wer-ist-glucklicher-manner-oder-frauen}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Affairs }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(gender) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{rating_gender =} \KeywordTok{mean}\NormalTok{(rating))}
\CommentTok{#> # A tibble: 2 x 2}
\CommentTok{#>   gender rating_gender}
\CommentTok{#>   <fctr>         <dbl>}
\CommentTok{#> 1 female          3.94}
\CommentTok{#> 2   male          3.92}
\end{Highlighting}
\end{Shaded}

Praktisch kein Unterschied. Heißt das auch, es gibt keinen Unterschied
in der Häufigkeit der Affären?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Affairs }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(gender) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{affairs_gender =} \KeywordTok{mean}\NormalTok{(affairs))}
\CommentTok{#> # A tibble: 2 x 2}
\CommentTok{#>   gender affairs_gender}
\CommentTok{#>   <fctr>          <dbl>}
\CommentTok{#> 1 female           1.42}
\CommentTok{#> 2   male           1.50}
\end{Highlighting}
\end{Shaded}

Scheint auch kein Unterschied zu sein\ldots{}

Und zum Abschluss noch mal etwas genauer: Teilen wir mal nach Geschlecht
und nach Kinderstatus auf, also in 4 Gruppen. Theoretisch dürfte es hier
auch keine Unterschiede/Zusammenhänge geben. Zumindest fällt mir kein
sinnvoller Grund ein; zumal die vorherige eindimensionale Analyse keine
Unterschiede zu Tage gefördert hat.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Affairs }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(gender, children) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{affairs_mean =} \KeywordTok{mean}\NormalTok{(affairs),}
            \DataTypeTok{rating_mean =} \KeywordTok{mean}\NormalTok{(rating))}
\CommentTok{#> # A tibble: 4 x 4}
\CommentTok{#> # Groups:   gender [?]}
\CommentTok{#>   gender children affairs_mean rating_mean}
\CommentTok{#>   <fctr>   <fctr>        <dbl>       <dbl>}
\CommentTok{#> 1 female       no        0.838        4.40}
\CommentTok{#> 2 female      yes        1.685        3.73}
\CommentTok{#> 3   male       no        1.014        4.10}
\CommentTok{#> 4   male      yes        1.659        3.86}

\NormalTok{Affairs }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(children, gender) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{affairs_mean =} \KeywordTok{mean}\NormalTok{(affairs),}
            \DataTypeTok{rating_mean =} \KeywordTok{mean}\NormalTok{(rating))}
\CommentTok{#> # A tibble: 4 x 4}
\CommentTok{#> # Groups:   children [?]}
\CommentTok{#>   children gender affairs_mean rating_mean}
\CommentTok{#>     <fctr> <fctr>        <dbl>       <dbl>}
\CommentTok{#> 1       no female        0.838        4.40}
\CommentTok{#> 2       no   male        1.014        4.10}
\CommentTok{#> 3      yes female        1.685        3.73}
\CommentTok{#> 4      yes   male        1.659        3.86}
\end{Highlighting}
\end{Shaded}

\subsection{Effektstärken}\label{effektstarken}

Berichten Sie eine relevante Effektstärke!

Hm, auch keine gewaltigen Unterschiede. Höchstens für die Zufriedenheit
mit der Partnerschaft bei kinderlosen Personen scheinen sich Männer und
Frauen etwas zu unterscheiden. Hier stellt sich die Frage nach der Größe
des Effekts, z.B. anhand Cohen's d. Dafür müssen wir noch die SD pro
Gruppe wissen:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Affairs }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(children, gender) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{rating_mean =} \KeywordTok{mean}\NormalTok{(rating),}
            \DataTypeTok{rating_sd =} \KeywordTok{sd}\NormalTok{(rating))}
\CommentTok{#> # A tibble: 4 x 4}
\CommentTok{#> # Groups:   children [?]}
\CommentTok{#>   children gender rating_mean rating_sd}
\CommentTok{#>     <fctr> <fctr>       <dbl>     <dbl>}
\CommentTok{#> 1       no female        4.40     0.914}
\CommentTok{#> 2       no   male        4.10     1.064}
\CommentTok{#> 3      yes female        3.73     1.183}
\CommentTok{#> 4      yes   male        3.86     1.046}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d <-}\StringTok{ }\NormalTok{(}\FloatTok{4.4} \OperatorTok{-}\StringTok{ }\FloatTok{4.1}\NormalTok{)}\OperatorTok{/}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Die Effektstärke beträgt etwa 0.3.

\subsection{Korrelationen}\label{korrelationen}

Berechnen und visualisieren Sie zentrale Korrelationen!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Affairs }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select_if}\NormalTok{(is.numeric) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{cor ->}\StringTok{ }\NormalTok{cor_tab}

\KeywordTok{corrplot}\NormalTok{(cor_tab)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{075_Fallstudien_Titanic_Affairs_files/figure-latex/affairs-cortab-1} \end{center}

\subsection{Ehejahre und Affären}\label{ehejahre-und-affaren}

Wie groß ist der Einfluss (das Einflussgewicht) der Ehejahre bzw.
Ehezufriedenheit auf die Anzahl der Affären?

Dazu sagen wir R: ``Hey R, rechne mal ein lineares Modell'', also eine
normale (lineare) Regression. Dazu können wir entweder das entsprechende
Menü im R-Commander auswählen, oder folgende R-Befehle ausführen:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(affairs }\OperatorTok{~}\StringTok{ }\NormalTok{yearsmarried, }\DataTypeTok{data =}\NormalTok{ Affairs)}
\KeywordTok{tidy}\NormalTok{(lm1)  }\CommentTok{# Ergebnisse der Regression zeigen}
\CommentTok{#>           term estimate std.error statistic  p.value}
\CommentTok{#> 1  (Intercept)    0.551    0.2351      2.34 0.019378}
\CommentTok{#> 2 yearsmarried    0.111    0.0238      4.65 0.000004}
\KeywordTok{glance}\NormalTok{(lm1)}
\CommentTok{#>   r.squared adj.r.squared sigma statistic p.value df logLik  AIC  BIC}
\CommentTok{#> 1    0.0349        0.0333  3.24      21.7   4e-06  2  -1559 3124 3137}
\CommentTok{#>   deviance df.residual}
\CommentTok{#> 1     6301         599}
\NormalTok{lm2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(affairs }\OperatorTok{~}\StringTok{ }\NormalTok{rating, }\DataTypeTok{data =}\NormalTok{ Affairs)}
\KeywordTok{tidy}\NormalTok{(lm2)}
\CommentTok{#>          term estimate std.error statistic  p.value}
\CommentTok{#> 1 (Intercept)    4.742     0.479      9.90 1.68e-21}
\CommentTok{#> 2      rating   -0.836     0.117     -7.12 3.00e-12}
\KeywordTok{glance}\NormalTok{(lm2)}
\CommentTok{#>   r.squared adj.r.squared sigma statistic p.value df logLik  AIC  BIC}
\CommentTok{#> 1    0.0781        0.0766  3.17      50.8   3e-12  2  -1545 3096 3110}
\CommentTok{#>   deviance df.residual}
\CommentTok{#> 1     6019         599}
\end{Highlighting}
\end{Shaded}

Also: \texttt{yearsmarried} und \texttt{rating} sind beide statistisch
signifikante Prädiktoren für die Häufigkeit von Affären. Das adjustierte
\(R^2\) ist allerdings in beiden Fällen nicht so groß.

\subsection{Ehezufriedenheit als
Prädiktor}\label{ehezufriedenheit-als-pradiktor}

Um wie viel erhöht sich die erklärte Varianz (R-Quadrat) von
Affärenhäufigkeit wenn man den Prädiktor Ehezufriedenheit zum Prädiktor
Ehejahre hinzufügt? (Wie) verändern sich die Einflussgewichte
(b)?\footnote{Output im Folgenden nicht abgedruckt.}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm3 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(affairs }\OperatorTok{~}\StringTok{ }\NormalTok{rating }\OperatorTok{+}\StringTok{ }\NormalTok{yearsmarried, }\DataTypeTok{data =}\NormalTok{ Affairs)}
\NormalTok{lm4 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(affairs }\OperatorTok{~}\StringTok{ }\NormalTok{yearsmarried }\OperatorTok{+}\StringTok{ }\NormalTok{rating, }\DataTypeTok{data =}\NormalTok{ Affairs)}
\KeywordTok{summary}\NormalTok{(lm3)}
\KeywordTok{summary}\NormalTok{(lm4)}
\end{Highlighting}
\end{Shaded}

Ok. Macht eigentlich die Reihenfolge der Prädiktoren in der Regression
einen Unterschied? Der Vergleich von Modell 3 vs.~Modell 4 beantwortet
diese Frage.

Wir sehen, dass beim 1. Regressionsmodell das R\^{}2 0.03 war; beim 2.
Modell 0.08 und beim 3. Modell liegt R\^{}2 bei 0.09. Die Differenz
zwischen Modell 1 und 3 liegt bei (gerundet) 0.06; wenig.

\subsection{Weitere Prädiktoren der
Affärenhäufigkeit}\label{weitere-pradiktoren-der-affarenhaufigkeit}

Welche Prädiktoren würden Sie noch in die Regressionsanalyse aufnehmen?

Hm, diese Frage klingt nicht so, als ob der Dozent die Antwort selber
wüsste\ldots{} Naja, welche Variablen gibt es denn alles:

\begin{verbatim}
#> [1] "affairs"       "gender"        "age"           "yearsmarried" 
#> [5] "children"      "religiousness" "education"     "occupation"   
#> [9] "rating"
\end{verbatim}

Z.B. wäre doch interessant, ob Ehen mit Kinder mehr oder weniger
Seitensprüngen aufweisen. Und ob die ``Kinderfrage'' die anderen
Zusammenhänge/Einflussgewichte in der Regression verändert. Probieren
wir es auch. Wir können wiederum im R-Commander ein Regressionsmodell
anfordern oder es mit der Syntax probieren:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm5 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(affairs}\OperatorTok{~}\StringTok{ }\NormalTok{rating }\OperatorTok{+}\StringTok{ }\NormalTok{yearsmarried }\OperatorTok{+}\StringTok{ }\NormalTok{children, }\DataTypeTok{data =}\NormalTok{ Affairs)}
\KeywordTok{summary}\NormalTok{(lm5)}
\NormalTok{r2_lm5 <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(lm5)}\OperatorTok{$}\NormalTok{r.squared}
\end{Highlighting}
\end{Shaded}

Das Regressionsgewicht von \texttt{childrenyes} ist negativ. Das
bedeutet, dass Ehen mit Kindern weniger Affären verbuchen (aber geringe
Zufriedenheit, wie wir oben gesehen haben! Hrks!). Allerdings ist der
p-Wert nicht signifikant, was wir als Zeichen der Unbedeutsamkeit dieses
Prädiktors verstehen können. \(R^2\) lungert immer noch bei mickrigen
0.094 herum. Wir haben bisher kaum verstanden, wie es zu Affären kommt.
Oder unsere Daten bergen diese Informationen einfach nicht.

Wir könnten auch einfach mal Prädiktoren, die wir haben, ins Feld
schicken. Mal sehen, was dann passiert:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm6 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(affairs }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ Affairs)}
\KeywordTok{summary}\NormalTok{(lm6)}
\end{Highlighting}
\end{Shaded}

Der ``.'' im Befehl \texttt{affairs\ \textasciitilde{}\ .} oben soll
sagen: nimm ``alle Variablen, die noch in der Datenmatrix übrig sind''.

Insgesamt bleibt die erklärte Varianz in sehr bescheidenem Rahmen: 0.13.
Das zeigt uns, dass es immer noch nur schlecht verstanden ist -- im
Rahmen dieser Analyse -- welche Faktoren die Affärenhäufigkeit erklärt.

\subsection{Unterschied zwischen den
Geschlechtern}\label{unterschied-zwischen-den-geschlechtern}

Unterscheiden sich die Geschlechter statistisch signifikant? Wie groß
ist der Unterschied? Sollte hier lieber das d-Maß oder Rohwerte als
Effektmaß angegeben werden?

Hier bietet sich ein t-Test für unabhängige Gruppen an. Die Frage lässt
auf eine ungerichtete Hypothese schließen (\(\alpha\) sei .05). Mit dem
entsprechenden Menüpunkt im R-Commander oder mit folgender Syntax lässt
sich diese Analyse angehen:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{t.test}\NormalTok{(affairs }\OperatorTok{~}\StringTok{ }\NormalTok{gender, }\DataTypeTok{data =}\NormalTok{ Affairs) ->}\StringTok{ }\NormalTok{t1}

\NormalTok{t1 }\OperatorTok{%>%}\StringTok{ }\NormalTok{tidy}
\CommentTok{#>   estimate estimate1 estimate2 statistic p.value parameter conf.low}
\CommentTok{#> 1  -0.0775      1.42       1.5    -0.287   0.774       594   -0.607}
\CommentTok{#>   conf.high                  method alternative}
\CommentTok{#> 1     0.452 Welch Two Sample t-test   two.sided}
\end{Highlighting}
\end{Shaded}

Der p-Wert ist größer als \(\alpha\). Daher wird die \(H_0\)
beibehalten. Auf Basis der Stichprobendaten entscheiden wir uns für die
\(H_0\). Entsprechend umschließt das 95\%-KI die Null.

Da die Differenz nicht signifikant ist, kann argumentiert werden, dass
wir \texttt{d} auf 0 schätzen müssen. Man kann sich den d-Wert auch z.B.
von \{MBESS\} schätzen lassen.

Dafür brauchen wir die Anzahl an Männer und Frauen: 315, 286.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(MBESS)}
\KeywordTok{ci.smd}\NormalTok{(}\DataTypeTok{ncp =}\NormalTok{ t1}\OperatorTok{$}\NormalTok{statistic,}
    \DataTypeTok{n.1 =} \DecValTok{315}\NormalTok{,}
    \DataTypeTok{n.2 =} \DecValTok{286}\NormalTok{)}
\CommentTok{#> $Lower.Conf.Limit.smd}
\CommentTok{#> [1] -0.184}
\CommentTok{#> }
\CommentTok{#> $smd}
\CommentTok{#>       t }
\CommentTok{#> -0.0235 }
\CommentTok{#> }
\CommentTok{#> $Upper.Conf.Limit.smd}
\CommentTok{#> [1] 0.137}
\end{Highlighting}
\end{Shaded}

Das Konfidenzintervall ist zwar relativ klein (die Schätzung also
aufgrund der recht großen Stichprobe relativ präzise), aber der
Schätzwert für d \texttt{smd} liegt sehr nahe bei Null. Das stärkt
unsere Entscheidung, von einer Gleichheit der Populationen (Männer
vs.~Frauen) auszugehen.

\subsection{Kinderlose Ehe vs.~Ehen mit
Kindern}\label{kinderlose-ehe-vs.ehen-mit-kindern}

Rechnen Sie die Regressionsanalyse getrennt für kinderlose Ehe und Ehen
mit Kindern!

Hier geht es im ersten Schritt darum, die entsprechenden Teil-Mengen der
Datenmatrix zu erstellen. Das kann man natürlich mit Excel o.ä. tun.
Alternativ könnte man es in R z.B. so machen:

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{Affair4 <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(Affairs, children }\OperatorTok{==}\StringTok{ "yes"}\NormalTok{)}
\KeywordTok{head}\NormalTok{(Affair4)}
\end{Highlighting}
\end{Shaded}

\subsection{Halodries}\label{halodries}

Rechnen Sie die Regression nur für ``Halodries''; d.h. für Menschen mit
Seitensprüngen. Dafür müssen Sie alle Menschen ohne Affären aus den
Datensatz entfernen.

Also, rechnen wir nochmal die Standardregression (\texttt{lm1}).
Probieren wir den Befehl \texttt{filter} dazu nochmal aus:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Affair5 <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(Affairs, affairs }\OperatorTok{!=}\StringTok{ }\DecValTok{0}\NormalTok{)}
\NormalTok{lm9 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(affairs }\OperatorTok{~}\StringTok{ }\NormalTok{rating, }\DataTypeTok{data =}\NormalTok{ Affair5)}
\KeywordTok{summary}\NormalTok{(lm9)}
\end{Highlighting}
\end{Shaded}

\subsection{logistische Regression}\label{logistische-regression-1}

Berechnen Sie für eine logistische Regression mit ``Affäre ja vs.~nein''
als Kriterium, wie stark der Einfluss von Geschlecht, Kinderstatus,
Ehezufriedenheit und Ehedauer ist!

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{Affairs }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{affairs_dichotom =}\NormalTok{ affairs }\OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{glm}\NormalTok{(affairs_dichotom }\OperatorTok{~}\StringTok{ }\NormalTok{gender }\OperatorTok{+}\StringTok{ }\NormalTok{children }\OperatorTok{+}\StringTok{ }\NormalTok{rating }\OperatorTok{+}\StringTok{ }\NormalTok{yearsmarried,}
      \DataTypeTok{data =}\NormalTok{ ., }
      \DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{) ->}\StringTok{ }\NormalTok{lm10}

\KeywordTok{tidy}\NormalTok{(lm10)}
\end{Highlighting}
\end{Shaded}

Wenn \texttt{if\_else} unbekannt ist, lohnt sich ein Blick in die Hilfe
mit \texttt{?if\_else} (\texttt{dplyr} muss vorher geladen sein).

Aha, signifikant ist die Ehezufriedenheit: Je größer \texttt{rating}
desto geringer die Wahrscheinlichkeit für \texttt{affairs\_dichotom}.
Macht Sinn!

Übrigens, die Funktionen \texttt{lm}, \texttt{glm} und \texttt{summary}
spucken leider keine brave Tabelle in Normalform aus, was aber schön
wäre. Aber man leicht eine Tabelle (data.frame) bekommen mit dem Befehl
\texttt{tidy} aus \texttt{broom}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tidy}\NormalTok{(lm10) }
\CommentTok{#>           term estimate std.error statistic  p.value}
\CommentTok{#> 1  (Intercept)  -0.0537    0.4299    -0.125 9.01e-01}
\CommentTok{#> 2   gendermale  -0.2416    0.1966    -1.229 2.19e-01}
\CommentTok{#> 3  childrenyes  -0.3935    0.2831    -1.390 1.64e-01}
\CommentTok{#> 4       rating   0.4654    0.0874     5.327 9.97e-08}
\CommentTok{#> 5 yearsmarried  -0.0221    0.0212    -1.040 2.99e-01}
\end{Highlighting}
\end{Shaded}

\subsection{Zum Abschluss}\label{zum-abschluss}

Visualisieren wir mal was! Ok, wie wäre es mit einem Jitter-Diagramm
(vgl. Abbildungen \ref{fig:affairs-jitter} und
\ref{fig:affairs-smooth}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Affairs }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(affairs, gender, children, rating) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ affairs, }\DataTypeTok{y =}\NormalTok{ rating)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_jitter}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{color =}\NormalTok{ gender, }\DataTypeTok{shape =}\NormalTok{ children)) }
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{075_Fallstudien_Titanic_Affairs_files/figure-latex/affairs-jitter-1} 

}

\caption{Affären, mit Jitter}\label{fig:affairs-jitter}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Affairs }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{rating_dichotom =} \KeywordTok{ntile}\NormalTok{(rating, }\DecValTok{2}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ yearsmarried, }\DataTypeTok{y =}\NormalTok{ affairs)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_jitter}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{color =}\NormalTok{ gender)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{075_Fallstudien_Titanic_Affairs_files/figure-latex/affairs-smooth-1} 

}

\caption{Affären, mit Smooth}\label{fig:affairs-smooth}
\end{figure}

Puh. Geschafft!

\section{Befehlsübersicht}\label{befehlsubersicht-8}

Tabelle \ref{tab:befehle-fallstudien} fasst die R-Funktionen dieses
Kapitels zusammen.

\begin{table}

\caption{\label{tab:befehle-fallstudien}Befehle des Kapitels 'Fallstudien titanic und affairs'}
\centering
\begin{tabular}[t]{l|l}
\hline
Paket..Funktion & Beschreibung\\
\hline
data & Lädt Daten aus einem Datensatz\\
\hline
chisq.test & Rechnet einen Chi-Quadrat-Test\\
\hline
compute.es::chies & Liefert Effektstärkemaße für einen Chi-Quadrat-Test\\
\hline
predict & Macht eine Vorhersage\\
\hline
psych::describe & Liefert eine Reihe zentraler Statistiken\\
\hline
is.na & Zeigt an, ob ein Vektor fehlende Werte beinhaltet\\
\hline
dplyr::summarise\_each & Führt summarise für jede Spalte aus\\
\hline
t.test & Rechnet einen t-Test\\
\hline
MBESS:ci.smd & Berechnet Cohens d\\
\hline
dplyr::ntile & Teilt einen Vektor in n Teile mit jeweils gleich viel Werten\\
\hline
broom::tidy & Wandelt ein Objekt vom Typ 'lm' in einen Dataframe um.\\
\hline
\end{tabular}
\end{table}

\part{Ungeleitetes Modellieren}

\chapter{Vertiefung: Clusteranalyse}\label{cluster}

\begin{center}\includegraphics[width=0.3\linewidth]{images/FOM} \end{center}

\begin{center}\includegraphics[width=0.1\linewidth]{images/licence} \end{center}

Benötigte Pakte:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(cluster)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(broom)}
\end{Highlighting}
\end{Shaded}

\BeginKnitrBlock{rmdcaution}
Lernziele:

\begin{itemize}
\tightlist
\item
  Das Ziel einer Clusteranalyse erläutern können.
\item
  Das Konzept der euklidischen Abstände verstehen.
\item
  Eine k-Means-Clusteranalyse berechnen und interpretieren können.
\end{itemize}
\EndKnitrBlock{rmdcaution}

\section{Grundlagen der
Clusteranalyse}\label{grundlagen-der-clusteranalyse}

Das Ziel einer Clusteranalyse ist es, Gruppen von Beobachtungen (d. h.
\emph{Cluster}) zu finden, die innerhalb der Cluster möglichst homogen,
zwischen den Clustern möglichst heterogen sind. Um die Ähnlichkeit von
Beobachtungen zu bestimmen, können verschiedene Distanzmaße herangezogen
werden. Für metrische Merkmale wird z. B. häufig die euklidische Metrik
verwendet, d. h., Ähnlichkeit und Distanz werden auf Basis des
euklidischen Abstands bestimmt. Aber auch andere Abstände wie
``Manhattan'' oder ``Gower'' sind möglich. Letztere haben den Vorteil,
dass sie nicht nur für metrische Daten sondern auch für gemischte
Variablentypen verwendet werden können. Wir werden uns hier auf den
euklidischen Abstand konzentrieren.

\subsection{Intuitive Darstellung der
Clusteranalayse}\label{intuitive-darstellung-der-clusteranalayse}

Betrachten Sie das folgende Streudiagramm (die Daten sind frei erfunden;
``simuliert'', sagt der Statistiker). Es stellt den Zusammenhang von
Lernzeit (wie viel ein Student für eine Statistikklausur lernt) und dem
Klausurerfolg (wie viele Punkte ein Student in der Klausur erzielt) dar.
Sehen Sie Muster? Lassen sich Gruppen von Studierenden mit bloßem Auge
abgrenzen (Abb. \ref{fig:cluster1})?

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{082_Clusteranalyse_files/figure-latex/cluster1-1} 

}

\caption{Ein Streudiagramm - sehen Sie Gruppen (Cluster) ?}\label{fig:cluster1}
\end{figure}

Färben wir das Diagramm mal ein (Abb. \ref{fig:cluster2}).

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{082_Clusteranalyse_files/figure-latex/cluster2-1} 

}

\caption{Ein Streudiagramm - mit drei Clustern}\label{fig:cluster2}
\end{figure}

Nach dieser ``Färbung'', d.h. nach dieser Aufteilung in drei Gruppen,
scheint es folgende ``Cluster'', ``Gruppen'' oder ``Typen'' von
Studierenden zu geben:

\begin{itemize}
\item
  ``Blaue Gruppe'': Fälle dieser Gruppe lernen wenig und haben wenig
  Erfolg in der Klausur. Tja.
\item
  ``Rote Gruppe'': Fälle dieser Gruppe lernen viel; der Erfolg ist recht
  durchwachsen.
\item
  ``Grüne Gruppe'': Fälle dieser Gruppe lernen mittel viel und erreichen
  einen vergleichsweise großen Erfolg in der Klausur.
\end{itemize}

Drei Gruppen scheinen ganz gut zu passen. Wir hätten theoretisch auch
mehr oder weniger Gruppen unterteilen können. Die Clusteranalyse gibt
keine definitive Anzahl an Gruppen vor; vielmehr gilt es, aus
theoretischen und statistischen Überlegungen heraus die richtige Anzahl
auszuwählen (dazu gleich noch mehr).

Unterteilen wir zur Illustration den Datensatz einmal in bis zu 9
Cluster (Abbildung \ref{fig:cluster3}).

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{082_Clusteranalyse_files/figure-latex/cluster3-1} 

}

\caption{Unterschiedliche Anzahlen von Clustern im Vergleich}\label{fig:cluster3}
\end{figure}

Das ``X'' soll den ``Mittelpunkt'' des Clusters zeigen. Der Mittelpunkt
ist so gewählt, dass die Distanz von jedem Punkt zum Mittelpunkt
möglichst kurz ist. Dieser Abstand wird auch ``Varianz innerhalb des
Clusters'' oder kurz ``Varianz within'' bezeichnet. Natürlich wird diese
Varianz within immer kleiner, je größer die Anzahl der Cluster wird.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{082_Clusteranalyse_files/figure-latex/cluster4-1} 

}

\caption{Die Summe der Varianz within in Abhängigkeit von der Anzahl von Clustern. Ein Screeplot.}\label{fig:cluster4}
\end{figure}

Die vertikale gestrichelte Linie zeigt an, wo die Einsparung an Varianz
auf einmal ``sprunghaft'' weniger wird - just an jedem Knick bei x=3;
dieser ``Knick'' wird auch ``Ellbogen'' genannt (da sage einer,
Statistiker haben keine Phantasie). Man kann jetzt sagen, dass 3 Cluster
eine gute Lösung seien, weil mehr Cluster die Varianz innerhalb der
Cluster nur noch wenig verringern. Diese Art von Diagramm wird als
``Screeplot'' bezeichnet. Fertig!

\subsection{Euklidische Distanz}\label{euklidische-distanz}

Aber wie weit liegen zwei Punkte entfernt? Betrachten wir ein Beispiel.
Anna und Berta sind zwei Studentinnen, die eine Statistikklausur
\sout{geschrieben haben}schreiben mussten (bedauernswert). Die beiden
unterscheiden sich sowohl in Lernzeit als auch in Klausurerfolg. Aber
wie sehr unterscheiden sie sich? Wie groß ist der ``Abstand'' zwischen
Anna und Berta (vgl. Abb. \ref{fig:distanz})?

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/cluster/distanz_crop} 

}

\caption{Distanz zwischen zwei Punkten in der Ebene}\label{fig:distanz}
\end{figure}

Eine Möglichkeit, die Distanz zwischen zwei Punkten in der Ebene (2D) zu
bestimmen, ist der \emph{Satz des Pythagoras} (leise Trompetenfanfare).
Generationen von Schülern haben diese Gleichung ähmm\ldots{} geliebt:

\[c^2 = a^2 + b^2\]

In unserem Beispiel heißt das \(c^2 = 3^2+4^2 = 25\). Folglich ist
\(\sqrt{c^2}=\sqrt{25}=5\). Der Abstand oder der Unterschied zwischen
Anna und Berta beträgt also 5 - diese Art von ``Abstand'' nennt man den
\emph{euklidischen Abstand}\index{euklidischen Abstand}.

Aber kann man den euklidischen Abstand auch in 3D (Raum) verwenden? Oder
gar in Räumen mehr mehr Dimensionen??? Betrachten wir den Versuch, zwei
Dreiecke in 3D zu zeichnen. Stellen wir uns vor, zusätzlich zu Lernzeit
und Klausurerfolg hätten wir als 3. Merkmal der Studentinnen noch
``Statistikliebe'' erfasst (Bertas Statistikliebe ist um 2 Punkte höher
als Annas).

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/cluster/pythagoras2_crop} 

}

\caption{Pythagoras in 3D}\label{fig:pythagoras2}
\end{figure}

Sie können sich Punkt \(A\) als Ecke eines Zimmers vorstellen; Punkt
\(B\) schwebt dann in der Luft, in einiger Entfernung zu \(A\).

Wieder suchen wir den Abstand zwischen den Punkten \(A\) und \(B\). Wenn
wir die Länge \(e\) wüssten, dann hätten wir die Lösung; \(e\) ist der
Abstand zwischen \(A\) und \(B\). Im orangenen Dreieck gilt wiederum der
Satz von Pythagoras: \(c^2+d^2=e^2\). Wenn wir also \(c\) und \(d\)
wüssten, so könnten wir \(e\) berechnen\ldots{} \(c\) haben wir ja
gerade berechnet (5) und \(d\) ist einfach der Unterschied in
Statistikliebe zwischen Anna und Berta (2)! Also

\[e^2 = c^2 + d^2\] \[e^2 = 5^2 + 2^2\] \[e^2 = 25 + 4\]

\[e = \sqrt{29} \approx 5.4\]

Ah! Der Unterschied zwischen den beiden Studentinnen beträgt also
\textasciitilde{}5.4!

Intuitiv gesprochen, ``schalten wir mehrere Pythagoras-Sätze
hintereinander''.

\begin{quote}
Der euklidische Abstand berechnet sich mit Pythagoras' Satz!
\end{quote}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/cluster/pythagoras_crop} 

}

\caption{Pythagoras in Reihe geschaltet}\label{fig:pythagoras}
\end{figure}

Das geht nicht nur für ``zwei Dreiecke hintereinander'', sondern der
Algebra ist es wurscht, wie viele Dreiecke das sind.

\begin{quote}
Um den Abstand zweier Objekte mit \emph{k} Merkmalen zu bestimmen, kann
der euklidische Abstand berechnet werden mit. Bei k=3 Merkmalen lautet
die Formel dann \(e^2 = a^2 + b^2 + d^2\). Bei mehr als 3 Merkmalen
erweitert sich die Formel entsprechend.
\end{quote}

Dieser Gedanken ist mächtig! Wir können von allen möglichen Objekten den
Unterschied bzw. die (euklidische) Distanz ausrechnen! Betrachten wir
drei Professoren, die einschätzen sollten, wir sehr sie bestimmte Filme
mögen (1: gar nicht; 10: sehr). Die Filme waren: ``Die Sendung mit der
Maus'', ``Bugs Bunny'', ``Rambo Teil 1'', ``Vom Winde verweht'' und
``MacGyver''.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{profs <-}\StringTok{ }\KeywordTok{data_frame}\NormalTok{(}
  \DataTypeTok{film1 =} \KeywordTok{c}\NormalTok{(}\DecValTok{9}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{8}\NormalTok{),}
  \DataTypeTok{film2 =} \KeywordTok{c}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{7}\NormalTok{),}
  \DataTypeTok{film3 =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{3}\NormalTok{),}
  \DataTypeTok{film4 =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{),}
  \DataTypeTok{film5 =} \KeywordTok{c}\NormalTok{(}\DecValTok{7}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{6}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Betrachten Sie die Film-Vorlieben der drei Professoren. Gibt es ähnliche
Professoren hinsichtlich der Vorlieben? Welche Professoren haben einen
größeren ``Abstand'' in ihren Vorlieben?

Wir könnten einen ``fünffachen Pythagoras'' zu Rate ziehen.
Praktischerweise gibt es aber eine R-Funktion, die uns die Rechnerei
abnimmt:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dist}\NormalTok{(profs)}
\CommentTok{#>       1     2}
\CommentTok{#> 2 13.23      }
\CommentTok{#> 3  2.65 10.77}
\end{Highlighting}
\end{Shaded}

Offenbar ist der (euklidische) Abstand zwischen Prof.~1 und 2 groß
(13.2); zwischen Prof 2 und 3 auch recht groß (10.8). Aber der Abstand
zwischen Prof.~1 und 3 ist relativ klein! Endlich hätten wir diese Frage
auch geklärt. Sprechen Sie Ihre Professoren auf deren Filmvorlieben
an\ldots{}

\subsection{k-Means Clusteranalyse}\label{k-means-clusteranalyse}

Beim k-Means Clusterverfahren handelt es sich um eine bestimmte Form von
Clusteranalysen; zahlreiche Alternativen existieren, aber die k-Means
Clusteranalyse ist recht verbreitet. Im Gegensatz zur z.B. der
hierarchischen Clusteranalyse um ein partitionierendes Verfahren. Die
Daten werde in \(k\) Cluster aufgeteilt -- dabei muss die Anzahl der
Cluster im vorhinein feststehen. Ziel ist es, dass die Quadratsumme der
Abweichungen der Beobachtungen im Cluster zum Clusterzentrum minimiert
wird.

Der Ablauf des Verfahrens ist wie folgt:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Zufällige Beobachtungen als Clusterzentrum
\item
  Zuordnung der Beobachtungen zum nächsten Clusterzentrum (Ähnlichkeit,
  z. B. über die euklidische Distanz)
\item
  Neuberechnung der Clusterzentren als Mittelwert der dem Cluster
  zugeordneten Beobachtungen
\end{enumerate}

Dabei werden die Schritte 2. und 3. solange wiederholt, bis sich keine
Änderung der Zuordnung mehr ergibt -- oder eine maximale Anzahl an
Iterationen erreicht wurde. Aufgrund von (1.) hängt das Ergebnis einer
k-Means Clusteranalyse vom Zufall ab. Aus Gründen der Reproduzierbarkeit
sollte daher der Zufallszahlengenerator gesetzt werden (mit
\texttt{set.seed}). Außerdem bietet es sich an verschiedene
Startkonfigurationen zu versuchen. In der Funktion \texttt{kmeans()}
erfolgt dies durch die Option \texttt{nstart\ =}.

\section{Beispiel für eine einfache
Clusteranalyse}\label{beispiel-fur-eine-einfache-clusteranalyse}

Nehmen wir uns noch einmal den Extraversionsdatensatz vor. Kann man die
Personen clustern anhand von Ähnlichkeiten wie Facebook-Freunde,
Partyfrequenz und Katerhäufigkeit? Probieren wir es aus!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{extra <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"data/extra.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Verschaffen Sie sich einen Überblick mit der Funktion \texttt{glimpse}.

\subsection{Distanzmaße berechnen}\label{distanzmae-berechnen}

Auf Basis der drei metrischen Merkmale (d. h. \texttt{Alter},
\texttt{Einkommen} und \texttt{Kinder}), die wir hier aufs Geratewohl
auswählen, ergeben sich für die ersten sechs Beobachtungen folgende
Abstände:

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{extra }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(n_facebook_friends, n_hangover, extra_single_item) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{head }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{dist}\NormalTok{(.)}
\CommentTok{#>        1      2      3      4      5}
\CommentTok{#> 2 144.01                            }
\CommentTok{#> 3  35.01 109.00                     }
\CommentTok{#> 4  51.93  95.19  21.24              }
\CommentTok{#> 5 150.00   6.08 115.00 101.12       }
\CommentTok{#> 6 126.00 270.00 161.00 176.56 276.00}
\end{Highlighting}
\end{Shaded}

Sie können erkennen, dass die Beobachtungen \texttt{1} und \texttt{3}
den kleinsten Abstand haben, während \texttt{1} und \texttt{5} den
größten haben.

Allerdings hängen die Abstände von der Skalierung der Variablen ab
(\texttt{n\_facebook\_friends} streut stärker als
\texttt{extra\_single\_item}). Daher sollten wir die Variablen vor der
Analyse zu standardisieren (z. B. über \texttt{scale()}).

Mit der Funktion \texttt{daisy()} aus dem Paket \texttt{cluster} kann
man sich auch den Abstand zwischen den Objekten ausgeben lassen. Die
Funktion errechnet auch Abstandsmaße, wenn die Objekte aus Variablen mit
unterschiedlichen Skalenniveaus bestehen. Allerdings mag \texttt{daisy}
Variablen vom Typ \texttt{chr} nicht, daher sollten wir \texttt{sex}
zuerst in eine Faktorvariable umwandeln.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{extra }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(n_facebook_friends, sex, extra_single_item) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{sex =} \KeywordTok{factor}\NormalTok{(sex)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{head }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{cluster}\OperatorTok{::}\KeywordTok{daisy}\NormalTok{(.)}
\end{Highlighting}
\end{Shaded}

\subsection{kmeans für den
Extraversionsdatensatz}\label{kmeans-fur-den-extraversionsdatensatz}

Versuchen wir, einige Variablen mit \texttt{centers\ =\ 4} Clustern
mithilfe einer kmeans-Clusteranalyse zu clustern.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1896}\NormalTok{)}

\NormalTok{extra }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Frau =}\NormalTok{ sex }\OperatorTok{==}\StringTok{ "Frau"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(n_facebook_friends, Frau, extra_single_item) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{na.omit }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{scale ->}\StringTok{ }\NormalTok{extra_cluster}

\NormalTok{kmeans_extra_}\DecValTok{4}\NormalTok{ <-}\StringTok{ }\KeywordTok{kmeans}\NormalTok{(extra_cluster, }\DataTypeTok{centers =} \DecValTok{4}\NormalTok{, }\DataTypeTok{nstart =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Lassen Sie sich das Objekt \texttt{extra\_cluster} ausgeben und
betrachten Sie die Ausgabe; auch \texttt{str(kmeans\_extra\_4)} ist
interessant. Neben der Anzahl Beobachtungen pro Cluster (z. B. 337 in
Cluster 2) werden auch die Clusterzentren ausgegeben. Diese können dann
direkt verglichen werden. Schauen wir mal, in welchem Cluster die Anzahl
der Facebookfreunde im Schnitt am kleinsten ist:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kmeans_extra_}\DecValTok{4}\OperatorTok{$}\NormalTok{centers}
\CommentTok{#>   n_facebook_friends   Frau extra_single_item}
\CommentTok{#> 1            -0.0237  0.712            1.3792}
\CommentTok{#> 2            -0.0445  0.712           -0.3909}
\CommentTok{#> 3            -0.0368 -1.402           -0.0587}
\CommentTok{#> 4            25.6707 -1.402            1.3792}
\end{Highlighting}
\end{Shaded}

Betrachten Sie auch die Mittelwerte der anderen Variablen, die in die
Clusteranalyse eingegangen sind. Wie `gut' ist diese Clusterlösung?
Vielleicht wäre ja eine andere Anzahl von Clustern besser? Eine Antwort
darauf liefert die Varianz (Streung) innerhalb der Cluster: Sind die
Summen der quadrierten Abweichungen vom Clusterzentrum gering, so ist
die Varianz `innerhalb' der Cluster gering; die Cluster sind homogen und
die Clusterlösung ist `gut' (vgl. Abbildung \ref{fig:cluster-bsp}).

\begin{quote}
Je größer die Varianz innerhalb der Cluster, um schlechter ist die
Clusterlösung.
\end{quote}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/cluster/cluster_bsp-crop} 

}

\caption{Schematische Darstellung zweier einfacher Clusterlösungen; links: geringe Varianz innerhalb der Cluster; rechts: hohe Varianz innerhalb der Cluster}\label{fig:cluster-bsp}
\end{figure}

In zwei Dimensionen kann man Cluster gut visualisieren (Abbildung
\ref{fig:cluster3}); in drei Dimensionen wird es schon unübersichtlich.
Mehr Dimensionen sind schwierig. Daher ist es oft sinnvoll, die Anzahl
der Dimensionen durch Verfahren der Dimensionsreduktion zu verringern.
Die Hauptkomponentenanalyse oder die Faktorenanalyse bieten sich dafür
an.

Vergleichen wir ein paar verschiedene Lösungen, um zu sehen, welche
Lösung am besten zu sein scheint.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kmeans_extra_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{kmeans}\NormalTok{(extra_cluster, }\DataTypeTok{centers =} \DecValTok{2}\NormalTok{, }\DataTypeTok{nstart =} \DecValTok{10}\NormalTok{)}
\NormalTok{kmeans_extra_}\DecValTok{3}\NormalTok{ <-}\StringTok{ }\KeywordTok{kmeans}\NormalTok{(extra_cluster, }\DataTypeTok{centers =} \DecValTok{3}\NormalTok{, }\DataTypeTok{nstart =} \DecValTok{10}\NormalTok{)}
\NormalTok{kmeans_extra_}\DecValTok{5}\NormalTok{ <-}\StringTok{ }\KeywordTok{kmeans}\NormalTok{(extra_cluster, }\DataTypeTok{centers =} \DecValTok{5}\NormalTok{, }\DataTypeTok{nstart =} \DecValTok{10}\NormalTok{)}
\NormalTok{kmeans_extra_}\DecValTok{6}\NormalTok{ <-}\StringTok{ }\KeywordTok{kmeans}\NormalTok{(extra_cluster, }\DataTypeTok{centers =} \DecValTok{6}\NormalTok{, }\DataTypeTok{nstart =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Dann nehmen wir die Gesamtstreuung jeder Lösung und erstellen daraus
erst eine Liste und dann einen Dataframe:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{streuung_innerhalb <-}\StringTok{ }\KeywordTok{c}\NormalTok{(kmeans_extra_}\DecValTok{2}\OperatorTok{$}\NormalTok{tot.withinss,}
\NormalTok{                        kmeans_extra_}\DecValTok{3}\OperatorTok{$}\NormalTok{tot.withinss,}
\NormalTok{                        kmeans_extra_}\DecValTok{4}\OperatorTok{$}\NormalTok{tot.withinss,}
\NormalTok{                        kmeans_extra_}\DecValTok{5}\OperatorTok{$}\NormalTok{tot.withinss,}
\NormalTok{                        kmeans_extra_}\DecValTok{6}\OperatorTok{$}\NormalTok{tot.withinss)}

\NormalTok{streuung_df <-}\StringTok{ }\KeywordTok{data_frame}\NormalTok{(}
\NormalTok{  streuung_innerhalb,}
  \DataTypeTok{anzahl_cluster =} \DecValTok{2}\OperatorTok{:}\DecValTok{6}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Jetzt plotten wir die Höhe der Streuung pro Clusteranalyse um einen
Hinweis zu bekommen, welche Lösung am besten passen könnte.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(streuung_df) }\OperatorTok{+}
\StringTok{  }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ anzahl_cluster,}
      \DataTypeTok{y =}\NormalTok{ streuung_innerhalb) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_col}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{082_Clusteranalyse_files/figure-latex/unnamed-chunk-13-1} \end{center}

Nach der Lösung mit 4 Clustern kann man (vage) einen Knick ausmachen:
Noch mehr Cluster verbessern die Streuung innerhalb der Cluster (und
damit ihre Homogenität) nur noch unwesentlich oder zumindest deutlich
weniger. Daher entscheiden wir uns für eine Lösung mit 4 Clustern.

\section[Aufgaben]{\texorpdfstring{Aufgaben\footnote{R, R, F, F, R}}{Aufgaben}}\label{aufgaben-15}

\BeginKnitrBlock{rmdexercises}
Richtig oder Falsch!?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Die Clusteranalyse wird gemeinhin dazu verwenden, Objekte nach
  Ähnlichkeit zu Gruppen zuammenzufassen.
\item
  Die Varianz innerhalb eines Clusters kann als Maß für die Anzahl der
  zu extrahierenden Cluster herangezogen werden.
\item
  Unter euklidischer Distanz versteht jedes Maß, welches den Abstand
  zwischen Punkten in der Ebene misst.
\item
  Bei der k-means-Clusteranalyse darf man die Anzahl der zu
  extrahierenden Clustern nicht vorab festlegen.
\item
  Cluster einer k-means-Clusteranalyse werden so bestimmt, dass die
  Cluster möglichst homogen sind, d.h. möglichst wenig Streuung
  aufweisen (m.a.W. möglichst nah am Cluster-Zentrum sind).
\end{enumerate}
\EndKnitrBlock{rmdexercises}

Laden Sie den Datensatz \texttt{extra} zur Extraversion.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Unter Berücksichtigung der 10 Extraversionsitems: Lassen sich die
  Teilnehmer der Umfrage in eine Gruppe oder in mehrere Gruppen
  einteilen? Wenn in mehrere Gruppen, wie viele Gruppen passen am
  besten?
\item
  Berücksichtigen Sie den Extraversionsmittelwert und einige andere
  Variablen aus dem Datensatz (aber nicht die Items). Welche Gruppen
  ergeben sich? Versuchen Sie die Gruppen zu interpretieren!
\item
  Suchen Sie sich zwei Variablen aus dem Datensatz und führen Sie auf
  dieser Basis eine Clusteranalyse durch. Visualisieren Sie das Ergebnis
  anhand eines Streudiagrammes!
\end{enumerate}

\section{Befehlsübersicht}\label{befehlsubersicht-9}

Tabelle \ref{tab:befehle-cluster} fasst die R-Funktionen dieses Kapitels
zusammen.

\begin{table}

\caption{\label{tab:befehle-cluster}Befehle des Kapitels 'Clusteranalyse'}
\centering
\begin{tabular}[t]{l|l}
\hline
Paket::Funktion & Beschreibung\\
\hline
dist & Berechnet den euklidischen Abstand zwischen Vektoren\\
\hline
dplyr::glimpse & Stellt einen Dataframe im Überblick dar\\
\hline
cluster::daisy & Berechnet verschiedene Abstandsmaße\\
\hline
set.seed & Zufallsgenerator auf bestimmte Zahlen festlegen\\
\hline
cluster::clusplot & Visualisiert eine Clusteranalyse\\
\hline
\end{tabular}
\end{table}

\section{Verweise}\label{verweise-5}

\begin{itemize}
\item
  Diese Übung orientiert sich am Beispiel aus Kapitel 11.3 aus Chapman
  und Feit (\protect\hyperlink{ref-Chapman2015}{2015}) und steht unter
  der Lizenz
  \href{http://creativecommons.org/licenses/by-sa/3.0}{Creative Commons
  Attribution-ShareAlike 3.0 Unported}. Der Code steht unter der
  \href{http://www.apache.org/licenses/LICENSE-2.0}{Apache Lizenz 2.0}
\item
  Der erste Teil dieser Übung basiert auf diesem Skript:
  \url{https://cran.r-project.org/web/packages/broom/vignettes/kmeans.html}
\item
  Eine weiterführende, aber gut verständliche Einführung findet sich bei
  James, Witten, Hastie, und Tibshirani
  (\protect\hyperlink{ref-james2013introduction}{2013}\protect\hyperlink{ref-james2013introduction}{c}).
\item
  Die Intuition zum euklidischen Abstand mit Pythagoras' Satz kann hier
  im Detail nachgelesen werden:
  \url{https://betterexplained.com/articles/measure-any-distance-with-the-pythagorean-theorem/}.
\end{itemize}

\chapter{Vertiefung:
Dimensionsreduktion}\label{vertiefung-dimensionsreduktion}

\begin{center}\includegraphics[width=0.3\linewidth]{images/FOM} \end{center}

\begin{center}\includegraphics[width=0.1\linewidth]{images/licence} \end{center}

\BeginKnitrBlock{rmdcaution}
Lernziele:

\begin{itemize}
\tightlist
\item
  Den Unterschied zwischen einer Hauptkomponentenanalyse und einer
  Exploratorischen Faktorenanalyse kennen
\item
  Methoden kennen, um die Anzahl von Dimensionen zu bestimmen
\item
  Methoden der Visualisierung anwenden können
\item
  Umsetzungsmethoden in R anwenden können
\item
  Ergebnisse interpretieren können.
\end{itemize}
\EndKnitrBlock{rmdcaution}

In diesem Kapitel werden folgende Pakete benötigt:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(corrplot)  }\CommentTok{# für `corrplot`}
\KeywordTok{library}\NormalTok{(gplots)  }\CommentTok{# für `heatmap.2`}
\KeywordTok{library}\NormalTok{(nFactors)  }\CommentTok{# PCA und EFA}
\KeywordTok{library}\NormalTok{(tidyverse)  }\CommentTok{# Datenjudo}
\KeywordTok{library}\NormalTok{(psych)  }\CommentTok{# für z.B. 'alpha'}
\end{Highlighting}
\end{Shaded}

\section{Einführung}\label{einfuhrung}

Häufig möchte man in den Sozialwissenschaften \emph{latente
Variablen}\index{latente Konstrukte} messen - z.B. Arbeitszufriedenheit,
Extraversion, Schmerz oder Intelligenz. Solche Variablen nennt man
\emph{latent}, da man sie nicht direkt messen kann\footnote{Halt, da
  vorn läuft ein IQ-Punkt. Schnell, fangt ihn!}.

\begin{quote}
Konstrukte bezeichnen gedankliche bzw. theoretische Sachverhalt dar, die
nicht direkt beobachtbar und damit nicht direkt messbar sind.
\end{quote}

Komplementär zu latenten Konstrukten spricht man von manifesten
Variablen, wie Schuhgröße oder Körpergewicht; Dinge also, die man in
gewohntem Sinne beobachtbar messen kann. Messung von manifesten
Variablen bezeichnet man auch als \emph{extensives Messen} (Michell
\protect\hyperlink{ref-Michell2000}{2000}).

Was ist eigentlich \emph{Messen}\index{Messen}? Sagen wir, ich finde den
Urmeter auf der Straße (Details tun nichts zur Sache). Dann betrachte
ich intensiv den Weg von meinem Carport bis zu meiner Haustür. Alsdann
schaue ich, wie oft ich den Urmeter hintereinander legen muss, um den
Weg von Haustür zu Carport zurückzulegen. Voilà! Die Länge des Weges ist
\emph{gemessen}. Allgemein ist Messen - nach diesem Verständnis - also
das Vielfache eines Maßstabes in einer Größe (Michell
\protect\hyperlink{ref-Michell2000}{2000}, aber s. Eid, Gollwitzer, und
Schmitt (\protect\hyperlink{ref-eid2010statistik}{2010}) für eine
andere, verbreitete Definition).

Nach einer anderen Art von Messdefinition ist Messen alles, was aus
manifesten Variablen eine Zahl erzeugt (Michell
\protect\hyperlink{ref-Michell2000}{2000}). Das ist das Verständnis von
Messen der meisten Sozialwissenschaftler (doch, im Ernst). Bei Lichte
betrachtet ``misst'' man in den Sozialwissenschaften gerne so:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Such ein paar Variablen, die mit dem zu messenden Konstrukt zu tun
  haben könnten (z.B. Extraversion)
\item
  Frage ein paar Leute, wie sich selber einschätzen in diesen Variablen
\item
  Die Antwortskala denkst Du Dir nach eigenem Gusto aus (z.B. von
  \texttt{1} bis \texttt{10}).
\item
  Addiere die Punkte aller Variablen auf.
\item
  Verkündige, dass Du Dein Konstrukt gemessen hast. Fertig.
\end{enumerate}

Natürlich ist das \ldots{} nicht ganz richtig. Zumindest kann man nicht
sicher sein, dass man Extraversion gemessen hat, oder ausreichend genau
gemessen hat.

Da komplexe Phänomene wie Extraversion facettenreich sind, nimmt man
häufig mehrere manifeste Variablen und bezeichnet deren Mittelwert dann
als Messung von Extraversion. Handelt es sich um Kreuze in einer
Befragung, so spricht man von \emph{Items (Indikatoren)}\index{Item}.

Eine notwendige (aber nicht hinreichende) Voraussetzung, dass eine Reihe
von Items sagen wir Extraversion messen, ist, dass sie miteinander stark
korrelieren. Wenn sie das tun, so kann man sie auf \emph{eine} Variable
zusammenfassen, welche dann als Extraversion bezeichnet wird.

In diesem Kapitel betrachten wir zwei gängige Methoden solcher
Zusammenfassungsmethoden. Da diese Methoden Variablen zusammenfassen,
spricht man \emph{Dimensionsreduktion}. Wir setzen voraus, dass es sich
um metrische Variablen handelt (wir prüfen das nicht weiter).

\begin{itemize}
\tightlist
\item
  Die \emph{Hauptkomponentenanalyse}\index{Hauptkomponentenanalyse}
  (engl. principal component analysis, PCA) versucht, unkorrelierte
  Linearkombinationen zu finden, die die Gesamtvarianz in den Daten
  erfassen. Die PCA\index{PCA} beinhaltet also das Extrahieren von
  linearen Zusammenhängen der beobachteten Variablen.
\item
  Die \emph{Exploratorische Faktorenanalyse
  (EFA)}\index{Exploratorische Faktorenanalyse} versucht, die Varianz
  auf Basis einer kleinen Anzahl von Dimensionen zu modellieren, während
  sie gleichzeitig versucht, die Dimensionen in Bezug auf die
  ursprünglichen Variablen interpretierbar zu machen. Es wird davon
  ausgegangen, dass die Daten einem Faktoren Modell entsprechen, bei der
  die beobachteten Korrelationen auf \texttt{latente} Faktoren
  zurückführen. Mit der EFA wird \emph{nicht} die gesamte Varianz
  erklärt.
\end{itemize}

Die EFA wird oft als \emph{Common Factor Analysis} oder \emph{principal
axis analysis (Hauptachsenanalyse)}
bezeichnet\index{Hauptachsenanalyse}. Die EFA eröffnet dem Nutzer eine
Menge an analytischen Varianten, so dass das Ergebnis, im Gegensatz zu
PCA, recht unterschiedlich ausfallen kann. Es gibt also \emph{keine
einzige} Lösung bei der EFA. Wichtig ist, genau zu berichten, welche
Details man verwendet hat.

Eine einfache Faustregel für die Entscheidung zwischen diesen beiden
Methoden:

\begin{itemize}
\tightlist
\item
  Führe die PCA durch, wenn die korrelierten beobachteten Variablen
  einfach auf einen kleineren Satz von wichtigen unabhängigen
  zusammengesetzten Variablen reduziert werden soll.
\item
  Führe die EFA durch, wenn ein theoretisches Modell von latenten
  Faktoren zugrunde liegt, dass die beobachtete Variablen verursacht.
\end{itemize}

\section{Warum Datenreduktion wichtig
ist}\label{warum-datenreduktion-wichtig-ist}

\begin{itemize}
\tightlist
\item
  \emph{Dimensionen reduzieren}: Im technischen Sinne der
  Dimensionsreduktion können wir statt Variablen-Sets die Faktor-/
  Komponentenwerte verwenden (z. B. für Mittelwertvergleiche zwischen
  Experimental- und Kontrollgruppe, Regressionsanalyse und
  Clusteranalyse).
\item
  \emph{Unsicherheit verringern}: Wenn wir glauben, dass ein Konstrukt
  nicht eindeutig messbar ist, dann kann mit einem Variablen-Set die
  Unsicherheit reduziert werden.
\item
  \emph{Aufwand verringern}: Wir können den Aufwand bei der
  Datenerfassung vereinfachen, indem wir uns auf Variablen
  konzentrieren, von denen bekannt ist, dass sie einen hohen Beitrag zum
  interessierenden Faktor/ Komponente leisten. Wenn wir feststellen,
  dass einige Variablen für einen Faktor nicht wichtig sind, können wir
  sie aus dem Datensatz eliminieren. Außerdem werden die statistischen
  Modelle einfacher, wenn wir statt vieler Ausgangsvariablen einige
  wenige Komponenten/Faktoren als Eingabevariablen verwenden.
\end{itemize}

\section{Intuition zur
Dimensionsreduktion}\label{intuition-zur-dimensionsreduktion}

Betrachten Sie die die Visualisierung eines Datensatzes mit 3
Dimensionen (Spalten) in Abbildung \ref{fig:fig-scatter3d}). Man braucht
nicht viel Phantasie, um einen Pfeil (Vektor) in der Punktewolke zu
sehen. Um jeden Punkt einigermaßen genau zu bestimmen, reicht es, seine
``Pfeil-Koordinate'' zu wissen. Praktischerweise geben in Abbildung
\ref{fig:fig-scatter3d} die Farben (in etwa) die Koordinaten auf dem
Pfeil an\footnote{genau genommen ist hier die Regressionsgerade
  gezeichnet, es müsste aber der größte Eigenvektor sein. Geschenkt.}.
Damit können wir die Anzahl der Variablen (Dimensionen), die es braucht,
um einen Punkt zu beschreiben von 3 auf 1 reduzieren; 2/3 der
Komplexität eingespart. Wir verlieren etwas Genauigkeit, aber nicht
viel. Dieser Pfeil, der mitten durch den Punkteschwarm geht, nennt man
auch die 1. \emph{Hauptkomponente}\index{Hauptkomponente}.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{083_Dimensionsreduktion_files/figure-latex/fig-scatter3d-1} 

}

\caption{Der Pfeil ist eindimensional; reduziert also die drei Dimensionen auf eine}\label{fig:fig-scatter3d}
\end{figure}

Beachten Sie, dass hoch korrelierte Variablen eng an der
Regressionsgeraden liegen; entsprechend sind in Abbildung
\ref{fig:fig-scatter3d} die drei Variablen stark korreliert. Sehen Sie
auch, dass die Hauptkomponente Varianz erklärt: Jede Variable für sich
genommen, hat recht viel Streuung. Die Streuung der Punkte zur
Hauptkomponente ist aber relativ gering. Daher sagt man, die Streuung
(Varianz) wurde reduziert durch die Hauptkomponente.

\begin{quote}
Der längste Vektor, den man in die Punktewolke legen kann, bezeichnet
man als den 1. Eigenvektor oder die 1. Hauptkomponente.
\end{quote}

In Abbildung \ref{fig:fig-scatter3d} ist dieser als Pfeil
eingezeichnet\footnote{die Hauptkomponente ist hier ähnlich zur
  Regressionslinie, aber nicht identisch}. Weitere Hauptkomponenten kann
man nach dem gleichen Muster bestimmen mit der Auflage, dass sie im
\emph{rechten Winkel} zu bestehenden Hauptkomponenten liegen. Damit kann
man in einer 3D-Raum nicht mehr als 3 Hauptkomponenten bestehen (in
einem \(n\)-dimensionalen Raum also maximal \(n\) Hauptkomponenten).

\begin{quote}
Hauptkomponenten liegen stets im rechten Winkel zueinander
(`orthogonal'). Das bedeutet, dass Werte, die auf verschiedenen
Hauptkomponenten liegen, unkorreliert sind.
\end{quote}

\begin{verbatim}
#>       V1    V2    V3
#> V1 1.000 0.933 0.955
#> V2 0.933 1.000 0.833
#> V3 0.955 0.833 1.000
\end{verbatim}

\begin{quote}
Je stärker die Korrelation zwischen Variablen, desto besser kann man sie
zusammenfassen.
\end{quote}

\section{\texorpdfstring{Datensatz
`Werte'}{Datensatz Werte}}\label{datensatz-werte}

Wir untersuchen die Dimensionalität mittels einer auf 1000 Fälle
reduzierten Zufallsauswahl von 15 Variablen zur Messung der
grundlegenden Wertorientierungen von Menschen. Die Daten wurden im
Sommersemester 2017 von FOM Studierenden im ersten Semester an der FOM
bundesweit erhoben. Die Variablen zu Wertorientierungen wurden
ursprüngliche aus dem 40-Item-Set des Portraits Value Questionnaire»
(PVQ) von Schmidt u.~a. (\protect\hyperlink{ref-Schmidt2007}{2007})
adaptiert und durch Studien an der FOM seit 2014 stufenweise bis auf 15
relevante Variablen reduziert. Alle Variablen wurden auf einer Skala von
1 bis 7 (wobei 1 am wenigsten und 7 am meisten zutrifft) abgefragt.

Laden wir zunächst den Datensatz:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Werte <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"data/Werte.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Wir überprüfen zuerst die Struktur des Datensatzes, die ersten 6 Zeilen
und die Zusammenfassung. Probieren Sie die folgenden Befehle aus:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{glimpse}\NormalTok{(Werte)}
\end{Highlighting}
\end{Shaded}

Wir sehen mit \texttt{glimpse}, dass die Bereiche der Bewertungen für
jede Variable 1-7 sind. Außerdem sehen wir, dass die Bewertungen als
numerisch (Integer, also ganzzahlig) eingelesen wurden. Die Daten sind
somit offenbar richtig formatiert.

\section{Neuskalierung der Daten}\label{neuskalierung-der-daten}

In vielen Fällen ist es sinnvoll, Rohdaten neu zu skalieren - auch bei
der Dimensionsreduktion. Warum ist das nötig?

Dies wird üblicherweise als \emph{Standardisierung}, \emph{Normierung},
oder \emph{Z-Transformation} bezeichnet. Als Ergebnis ist der Mittelwert
aller Variablen über alle Beobachtungen dann 0 und die
Standardabweichung (SD) 1. Da wir hier gleiche Skalenstufen haben, ist
ein Skalieren nicht unbedingt notwendig, wir führen es aber trotzdem
durch.

Ein einfacher Weg, alle Variablen im Datensatz auf einmal zu skalieren
ist der Befehl \texttt{scale()}. Da wir die Rohdaten nie ändern wollen,
weisen wir die Rohwerte zuerst einem neuen Dataframe
\texttt{Werte.skaliert} zu und skalieren anschließend die Daten. Wir
skalieren in unserem Datensatz alle Variablen.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Werte }\OperatorTok{%>%}\StringTok{ }\NormalTok{scale }\OperatorTok{%>%}\StringTok{ }\NormalTok{as_tibble ->}\StringTok{ }\NormalTok{Werte.skaliert}
\KeywordTok{summary}\NormalTok{(Werte.skaliert)}
\end{Highlighting}
\end{Shaded}

\BeginKnitrBlock{rmdpseudocode}
Nimm das Objekt (ein Dataframe) \texttt{Werte} UND DANN\\
z-skaliere das Objekt UND DANN\\
definiere es als Dataframe (genauer: tibble) UND speichere dies unter
dem Namen \texttt{Werte\_skaliert}. FERTIG.

Ach ja, dann zeig noch ein \texttt{summary} von diesem Objekt.
\EndKnitrBlock{rmdpseudocode}

Die Daten wurden richtig skaliert, da der Mittelwert aller Variablen
über alle Beobachtungen 0 und die sd 1 ist.

\section{Zusammenhänge in den Daten}\label{zusammenhange-in-den-daten}

Wir verwenden den Befehl \texttt{corrplot()} für die Erstinspektion von
bivariaten Beziehungen zwischen den Variablen. Das Argument
\texttt{order\ =\ "hclust"} ordnet die Zeilen und Spalten entsprechend
der Ähnlichkeit der Variablen in einer hierarchischen Cluster-Lösung der
Variablen (mehr dazu im Kapitel \ref{cluster}) neu an.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{corrplot}\NormalTok{(}\KeywordTok{cor}\NormalTok{(Werte.skaliert), }\DataTypeTok{order =} \StringTok{"hclust"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{083_Dimensionsreduktion_files/figure-latex/unnamed-chunk-10-1} \end{center}

Die Visualisierung der Korrelation der Variablen scheint fünf Cluster zu
zeigen:

\begin{itemize}
\tightlist
\item
  (``Führung'', ``Entscheidung'')
\item
  (``Aufregung'', ``Spaß'', ``Freude'')
\item
  (``Umweltbewusstsein'', ``Zuhören'', ``Interesse'')
\item
  (``Ordentlichkeit'', ``Gefahrenvermeidung'', ``Sicherheit'')
\item
  (``Respekt'', ``Religiösität'', ``Demut'')
\end{itemize}

\section{Daten mit fehlende Werten}\label{daten-mit-fehlende-werten}

Wenn in den Daten leere Zellen, also fehlende Werte, vorhanden sind,
dann kann es bei bestimmten Rechenoperationen zu Fehlermeldungen kommen.
Dies betrifft zum Beispiel Korrelationen, PCA und EFA. Der Ansatz
besteht deshalb darin, NA-Werte explizit zu entfernen. Dies kann am
einfachsten mit dem Befehl \texttt{na.omit()} geschehen:

Beispiel:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Werte.skaliert <-}\StringTok{ }\KeywordTok{na.omit}\NormalTok{(Werte.skaliert)}
\KeywordTok{corrplot}\NormalTok{(}\KeywordTok{cor}\NormalTok{(Werte.skaliert), }\DataTypeTok{order =} \StringTok{"hclust"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Da wir in unserem Datensatz vollständige Daten verwenden, gibt es auch
keine Leerzellen.

Mit dem Parameter \texttt{order} kann man die Reihenfolge (order) der
Variablen, wie sie im Diagramm dargestellt werden ändern (vgl
\texttt{help(corrplot)}). Hier haben wir die Variablen nach Ähnlichkeit
aufgereiht: Ähnliche Variablen stehen näher beieinander. Damit können
wir gut erkennen, welche Variablen sich ähnlich sind (hoch korreliert
sind) und somit Kandidaten für eine Einsparung (Zusammenfassung zu einer
Hauptkomponente bzw. einem Faktor) sind.

\section{Hauptkomponentenanalyse
(PCA)}\label{hauptkomponentenanalyse-pca}

Die PCA berechnet ein Variablenset (Komponenten) in Form von linearen
Gleichungen, die die linearen Beziehungen in den Daten erfassen. Die
erste Komponente erfasst so viel Streuung (Varianz) wie möglich von
allen Variablen als eine einzige lineare Funktion. Die zweite Komponente
erfasst unkorreliert zur ersten Komponente so viel Streuung wie möglich,
die nach der ersten Komponente verbleibt. Das geht so lange weiter, bis
es so viele Komponenten gibt wie Variablen.

\subsection{Bestimmung der Anzahl der
Hauptkomponenten}\label{bestimmung-der-anzahl-der-hauptkomponenten}

Betrachten wir in einem ersten Schritt die wichtigsten Komponenten für
die Werte. Wir finden die Komponenten mit \texttt{prcomp()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Werte.pc <-}\StringTok{ }\KeywordTok{prcomp}\NormalTok{(Werte.skaliert)  }\CommentTok{# Principal Components berechnen}
\KeywordTok{summary}\NormalTok{(Werte.pc)}
\CommentTok{#> Importance of components%s:}
\CommentTok{#>                          PC1   PC2   PC3    PC4    PC5    PC6    PC7}
\CommentTok{#> Standard deviation     1.691 1.542 1.384 1.1428 1.0797 0.8855 0.8298}
\CommentTok{#> Proportion of Variance 0.191 0.159 0.128 0.0871 0.0777 0.0523 0.0459}
\CommentTok{#> Cumulative Proportion  0.191 0.349 0.477 0.5639 0.6416 0.6939 0.7398}
\CommentTok{#>                           PC8    PC9   PC10   PC11   PC12  PC13   PC14}
\CommentTok{#> Standard deviation     0.8078 0.7882 0.7599 0.7413 0.6884 0.648 0.6449}
\CommentTok{#> Proportion of Variance 0.0435 0.0414 0.0385 0.0366 0.0316 0.028 0.0277}
\CommentTok{#> Cumulative Proportion  0.7833 0.8247 0.8632 0.8999 0.9315 0.959 0.9872}
\CommentTok{#>                          PC15}
\CommentTok{#> Standard deviation     0.4388}
\CommentTok{#> Proportion of Variance 0.0128}
\CommentTok{#> Cumulative Proportion  1.0000}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Berechnung der Gesamtvarianz}
\NormalTok{Gesamtvarianz <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(Werte.pc}\OperatorTok{$}\NormalTok{sdev}\OperatorTok{^}\DecValTok{2}\NormalTok{)}

\CommentTok{# Bei sum(Werte.pc$sdev^2) wird die Summe aller 15 Standardabweichungen berechnet.}

\CommentTok{# Varianzanteil der ersten Hauptkomponente}
\NormalTok{Werte.pc}\OperatorTok{$}\NormalTok{sdev[}\DecValTok{1}\NormalTok{]}\OperatorTok{^}\DecValTok{2} \OperatorTok{/}\StringTok{ }\NormalTok{Gesamtvarianz}
\CommentTok{#> [1] 0.191}
\end{Highlighting}
\end{Shaded}

\subsection{Scree-Plot}\label{scree-plot}

Der Standard-Plot \texttt{plot()} für die PCA ist ein
\emph{Scree-Plot}\footnote{scree: engl. ``Geröll''}, Dieser zeigt uns
die jeweils durch eine Hauptkomponente erfasste Streuung (Varianz). Wir
plotten ein Liniendiagramm mit dem Argument \texttt{type\ =\ "l"}
(\texttt{l} für Linie), s. Abb. \ref{fig:pca-scree}).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(Werte.pc, }\DataTypeTok{type=}\StringTok{"l"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{083_Dimensionsreduktion_files/figure-latex/pca-scree-1} 

}

\caption{Screeplot}\label{fig:pca-scree}
\end{figure}

\begin{quote}
Die Höhe der Varianz entspricht der Länge der Pfeile (Eigenvektoren) in
Abbildung \ref{fig:fig-scatter3d}: Längere Pfeile bedeuten größere
erklärte Varianz. Die Länge der Eigenvektoren bezeichnet man auch als
Eigenwert.
\end{quote}

Wir sehen in Abb. \ref{fig:pca-scree}, dass bei den Werte-Daten der
Anteil der Streuung nach der fünften Komponente nicht mehr wesentlich
abnimmt. Es soll die Stelle gefunden werden, ab der die Varianzen der
Hauptkomponenten deutlich kleiner sind. Je kleiner die Varianzen, desto
weniger Streuung erklärt diese Hauptkomponente.

\subsection{Ellbogen-Kriterium}\label{ellbogen-kriterium}

Nach dem \emph{Ellbogen-Kriterium}\index{Ellbogen-Kriterium} werden alle
Hauptkomponenten berücksichtigt, die links von der Knickstelle im
Scree-Plot liegen. Gibt es mehrere Knicks, dann werden jene
Hauptkomponenten ausgewählt, die links vom rechtesten Knick liegen. Gibt
es keinen Knick, dann hilft der Scree-Plot nicht weiter. Bei den
Werte-Daten tritt der Ellbogen, je nach Betrachtungsweise, entweder bei
vier oder sechs Komponenten auf. Dies deutet darauf hin, dass die ersten
fünf Komponenten die meiste Streuung in den Werte-Daten erklären.

\subsection{Eigenwert-Kriterium}\label{eigenwert-kriterium}

Der \emph{Eigenwert}\index{Eigenwert} ist eine Metrik für den Anteil der
erklärten Varianz pro Hauptkomponente. Die Anzahl Eigenwerte können wir
über den Befehl \texttt{eigen()} ausgeben.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{eigen}\NormalTok{(}\KeywordTok{cor}\NormalTok{(Werte))}
\end{Highlighting}
\end{Shaded}

Der Eigenwert einer Komponente/ eines Faktors sagt aus, wie viel Varianz
dieser Faktor an der Gesamtvarianz aufklärt. Laut dem
Eigenwert-Kriterium sollen nur Faktoren mit einem \emph{Eigenwert größer
1} extrahiert werden. Dies sind bei den Werte-Daten fünf Komponenten/
Faktoren, da fünf Eigenwerte größer 1 sind. Der Grund ist, dass
Komponenten/ Faktoren mit einem Eigenwert kleiner als 1 weniger
Erklärungswert haben als die ursprünglichen Variablen.

Dies kann auch grafisch mit dem \texttt{psych::VSS.Scree}\footnote{das
  Paket \texttt{psych} wird automatisch vom Paket \texttt{nfactors}
  gestartet, sie müssen es nicht extra starten} geplottet werden (s.
Abb. \ref{fig:vss-scree}).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{VSS.scree}\NormalTok{(Werte)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{083_Dimensionsreduktion_files/figure-latex/vss-scree-1} 

}

\caption{VSS-Screeplot}\label{fig:vss-scree}
\end{figure}

\subsection{Biplot}\label{biplot}

Eine gute Möglichkeit die Ergebnisse der PCA zu analysieren, besteht
darin, die ersten Komponenten zuzuordnen, die es uns ermöglichen, die
Daten in einem niedrigdimensionalen Raum zu visualisieren. Eine
gemeinsame Visualisierung ist ein \emph{Biplot}\index{Biplot}. Ein
Biplot zeigt die Ausprägungen der Fälle auf den ersten beiden
Hauptkomponenten. Häufig sind die beiden ersten Hauptkomponenten schon
recht aussagekräftig, vereinen also einen Gutteil der Streuung auf sich.
Dazu verwenden wir \texttt{biplot()} (s. Abbildung
\ref{fig:fig-biplot}).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{biplot}\NormalTok{(Werte.pc)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{083_Dimensionsreduktion_files/figure-latex/fig-biplot-1} 

}

\caption{Ein Biplot für den Werte-Datensatz}\label{fig:fig-biplot}
\end{figure}

Die einzelnen Ausgangsvariablen sind in Abbildung Abbildung
\ref{fig:fig-biplot} durch rote Pfeile (Vektoren) gekennzeichnet.

\begin{quote}
Je paralleler der Vektor einer Ausgangsvariable zur X-Achse (1.
Hauptkomponente) ist, umso identischer sind sich die entsprechende
Variable und die Hauptkomponente (für die Y-Achse gilt entsprechendes).
Das hilft uns, die Hauptkomponente inhatlich zu interpretieren.
Hauptkomponenten (oder Faktoren) sollten stets inhaltlich interpretiert
werden - auch wenn eine subjektive Komponente mitschwingt.
\end{quote}

Die 1. Hauptkomponente wird offenbar stark geprägt durch die
Ausgangsvariablen `Freude' und `Spaß'. Bei der 2. Hauptkomponente analog
durch `Demut' und `Gefahrenvermeidung'.

Zusätzlich erhalten wir einen Einblick in die Bewertungscluster (als
dichte Bereiche von Beobachtungspunkten): Gruppen von Punkten
entsprechen ähnlichen Fällen (ähnlich hinsichtlich ihrer Werte in den
ersten zwei Hauptkomponenten). Der Biplot ist hier durch die große
Anzahl an Beobachtung allerdings recht unübersichtlich.

\subsection{Aufgaben}\label{aufgaben-16}

\BeginKnitrBlock{rmdexercises}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Ziehen Sie eine Zufallsstichprobe aus dem Datensatz, berechnen Sie die
  PCA erneut und betrachten Sie den Biplot. Wie stark ist die Änderung?
\item
  Erstellen Sie mehrere Streudiagramme und überprüfen Sie die bivariaten
  Zusammenhänge (die ja zur Dimensionsreduktion führen) visuell.
\end{enumerate}
\EndKnitrBlock{rmdexercises}

Am einfachsten lassen sich die Komponenten extrahieren mit dem
\texttt{principal}-Befehl aus dem Paket \texttt{psych}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Werte.pca <-}\StringTok{ }\KeywordTok{principal}\NormalTok{(Werte, }\DataTypeTok{nfactors =} \DecValTok{5}\NormalTok{, }\DataTypeTok{rotate =} \StringTok{"none"}\NormalTok{)}
\KeywordTok{print}\NormalTok{(Werte.pca, }\DataTypeTok{cut =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{sort =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{digits =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\texttt{cut\ =\ 0.5} heißt, dass nur Ladungen ab 0.5 angezeigt werden
sollen. Mit \texttt{rotate\ =\ \textquotesingle{}none\textquotesingle{}}
sagen wir, dass wir keine Rotation wünschen. Eine Rotation ist

\subsection{Interpretation der Ergebnisse der
PCA}\label{interpretation-der-ergebnisse-der-pca}

Das Ergebnis sieht sehr gut aus. Es laden immer mehrere Items
(Ausgangsvariablen) (mindestens 2) hoch (\textgreater{} 0,5) auf einer
Komponente (die mit RC1 bis RC5 bezeichnet werden, \emph{RC} steht für
\emph{Rotated Component}). Mit ``laden'' ist die Parallelität der
Ausgangsvariable zur Hauptkomponente gemeint. Vereinfacht gesprochen ist
die Ladung die Korrelation der Items mit der jeweiligen Komponente.

Innerhalb einer PCA kann die Interpretierbarkeit über eine
\textbf{Rotation} erhöht werden. Wenn die Rotation nicht ausgeschlossen
wird (mit dem Argument \texttt{rotate="none"}), dann ist die
Voreinstellung eine \texttt{Varimax-Rotation}.

Mit \texttt{h2} (Kommunalität) ist der Anteil eines Items bezeichnet,
der durch die Komponenten insgesamt erklärt wird. Hier haben die Anzahl
der Komponenten auf 5 beschränkt. Daher wird nicht die ganze Varianz des
Items erklärt.

Es gibt keine Items die auf mehr als einer Komponente hoch laden. Die
Ladungen sind Korrelationskoeffizienten zwischen den Items und den
Hauptkomponenten. In der Zeile \emph{SS loadings} finden wir die
Eigenwerte der fünf Hauptkomponenten (berechnet als Summe der
quadrierten Ladungen). Den Anteil an der Gesamtvarianz, den sie
erklären, findet man in der Zeile \emph{Proportion Var}. Aufsummiert
sind die Anteile in der Zeile \emph{Cumlative Var}. Insgesamt werden
durch die fünf Hauptkomponenten 64\% der Gesamtvarianz erklärt. Die
stärke Hauptkomponente hat einen Eigenwert von 2.08 und erklärt 14\% der
Varianz.

Einzig das Item ``Unabhängigkeit'' lädt auf keine der Hauptkomponenten
hoch.

Um die inhaltliche Bedeutung der Komponenten zu interpretieren, schauen
wir uns die Inhalte der jeweiligen Items an und versuchen hierfür einen
inhaltlichen Gesamtbegriff zu finden. Die Erste Komponenten könnte mit
\textbf{Genuss}, die zweite mit \textbf{Sicherheit}, die dritte mit
\textbf{Bewusstsein}, die vierte mit \textbf{Konformismus} und die
fünfte mit \textbf{Anerkennung} bezeichnet werden.

Mit der Funktion \texttt{fa.diagram} kann das Ergebnis auch grafisch
dargestellt werden: \texttt{fa.diagram(Werte.pca)}.

\section{Exploratorische Faktorenanalyse
(EFA)}\label{exploratorische-faktorenanalyse-efa}

Genau genommen ist der Begriff \emph{Faktorenanalyse
(FA)}\index{Faktorenanalyse} ein Überbegriff für mehrere Arten von
ähnlichen Verfahren der Dimensionsreduktion. Ein Beispiel für eine Art
von Faktorenanalyse wäre dann die PCA. Aber der Begriff Faktorenanalyse
wird auch verwendet, um eine bestimmte Art von Faktorenanalyse -
sozusagen eine Faktorenanalyse im engeren Sinne - zu bezeichnen. Wir
halten uns hier an letztere Begriffskonvention.

In diesem Sinne ist die \emph{Exploratorische Faktorenanalyse
(EFA)}\index{Exploratorische Faktorenanalyse} ist eine Methode, um die
Beziehung von Konstrukten (Konzepten), d. h. Faktoren zu Variablen zu
beurteilen. Dabei werden die Faktoren als \emph{latente Variablen}
betrachtet, die nicht direkt beobachtet werden können. Stattdessen
werden sie empirisch durch mehrere Variablen beobachtet, von denen jede
ein Indikator der zugrunde liegenden Faktoren ist. Diese beobachteten
Werte werden als \emph{manifeste Variablen} bezeichnet und umfassen
Indikatoren. Die EFA versucht den Grad zu bestimmen, in dem Faktoren die
beobachtete Streuung der manifesten Variablen berücksichtigen.

Das Ergebnis der EFA ist ähnlich zur PCA: eine Matrix von Faktoren
(ähnlich zu den PCA-Komponenten) und ihre Beziehung zu den
ursprünglichen Variablen (Ladung der Faktoren auf die Variablen). Im
Gegensatz zur PCA versucht die EFA, Lösungen zu finden, die in den
\emph{manifesten variablen maximal interpretierbar} sind. Im Allgemeinen
versucht sie, Lösungen zu finden, bei denen eine kleine Anzahl von
Ladungen für jeden Faktor sehr hoch ist, während andere Ladungen für
diesen Faktor gering sind. Wenn dies möglich ist, kann dieser Faktor mit
diesem Variablen-Set interpretiert werden.

\subsection{Finden einer EFA Lösung}\label{finden-einer-efa-losung}

Als erstes muss die Anzahl der zu schätzenden Faktoren bestimmt werden.
Hierzu verwenden wir wieder das Ellbow-Kriterium und das
Eigenwert-Kriterium. Beide Kriterien haben wir schon bei der PCA
verwendet, dabei kommen wir auf 5 Faktoren.

Durch das Paket \texttt{nFactors} bekommen wir eine ausgefuchstere
Berechnung der Scree-Plot Lösung mit dem Befehl \texttt{nScree()} - es
werden noch weitere, sophistiziertere Methoden zur Berechnung der
`richtigen' Anzahl von Faktoren eingesetzt. Wir sparen uns hier die
Details.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{nScree}\NormalTok{(Werte)}
\end{Highlighting}
\end{Shaded}

\texttt{nScree} gibt vier methodische Schätzungen für die Anzahl an
Faktoren durch den Scree-Plot aus. Wir sehen, dass drei von vier
Methoden fünf Faktoren vorschlagen. Nach kurzer Überlegung und Blick aus
dem Fenster entscheiden wir uns für 5 Faktoren.

\subsection{Schätzung der EFA}\label{schatzung-der-efa}

Eine EFA wird geschätzt mit dem Befehl
\texttt{factanal(x,factors\ =\ k)}, wobei \texttt{k} die Anzahl Faktoren
angibt und \texttt{x} den Datensatz.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Werte.fa<-}\KeywordTok{factanal}\NormalTok{(Werte, }\DataTypeTok{factors =} \DecValTok{5}\NormalTok{)}
\NormalTok{Werte.fa}
\end{Highlighting}
\end{Shaded}

Eine übersichtlichere Ausgabe bekommen wir mit dem \texttt{print}
Befehl, in dem wir zusätzlich noch die Dezimalstellen kürzen mit
\texttt{digits\ =\ 2}, alle Ladungen kleiner als 0,5 ausblenden mit
\texttt{cutoff\ =\ .4} und die Ladungen mit \texttt{sort\ =\ TRUE} so
sortieren, dass die Ladungen, die auf einen Faktor laden, untereinander
stehen.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(Werte.fa, }\DataTypeTok{digits =} \DecValTok{2}\NormalTok{, }\DataTypeTok{cutoff =}\NormalTok{ .}\DecValTok{4}\NormalTok{, }\DataTypeTok{sort =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Standardmäßig wird bei \texttt{factanal()} eine \emph{Varimax-Rotation}
durchgeführt (das Koordinatensystem der Faktoren wird so rotiert, das
eine optimale Zuordnung zu den Variablen erfolgt). Bei Varimax gibt es
keine Korrelationen zwischen den Faktoren. Sollen Korrelationen zwischen
den Faktoren zugelassen werden, empfiehlt sich die Oblimin-Rotation mit
dem Argument \texttt{rotation="oblimin"} aus dem Paket
\texttt{GPArotation}.

Das eine Rotation sinnvoll ist, kann man sich am einfachsten an einem
Diagramm verdeutlichen (s. Abbildung \ref{fig:rotation}, (Fjalnes
\protect\hyperlink{ref-fjalnes_orthogonale_2014}{2014})).

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/dimred/rotation} 

}

\caption{Beispiel für eine rechtwinklige Rotation}\label{fig:rotation}
\end{figure}

Das Rotieren kann man sich als Drehen des Koordinatensystems vorstellen.
Durch die Rotation sind die Items `näher' an den Faktoren: Die
Faktorladung zu einem Faktor wurde größer, zum anderen Faktor hingegen
geringer. Damit wurde die Ladung, also die Zuordnung der Items zu den
Faktoren, insgesamt klarer, besser. Das wollen wir. Übrigens: Der Winkel
der Achsen ist beim Rotieren gleich (rechtwinklig, orthogonal)
geblieben. Daher spricht man von einer rechtwinkligen oder orthogonalen
Rotation. Man kann auch die Achsen unterschiedlich rotieren, so dass sie
nicht mehr rechtwinklig sind. Das könnte die Ladung noch klarer machen,
führt aber dazu, dass die Faktoren dann korreliert sind. Korrelierte
Faktoren sind oft nicht wünschenswert, weil ähnlich.

\subsection{Vertiefung: Heatmap mit
Ladungen}\label{vertiefung-heatmap-mit-ladungen}

In der obigen Ausgabe werden die Item-to-Faktor-Ladungen angezeigt. Im
zurückgegebenen Objekt \texttt{Werte.fa} sind diese als
\texttt{\$loadings} vorhanden. Wir können die Item-Faktor-Beziehungen
mit einer Heatmap von \texttt{\$loadings} visualisieren aus dem Paket
\texttt{gplots}\footnote{bereits automatisch geladen}, s. Abb.
\ref{fig:efa-heatmap}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{heatmap.2}\NormalTok{(Werte.fa}\OperatorTok{$}\NormalTok{loadings,}
          \DataTypeTok{dendrogram =} \StringTok{"both"}\NormalTok{,}
          \DataTypeTok{labRow =} \OtherTok{NULL}\NormalTok{,}
          \DataTypeTok{labCol =} \OtherTok{NULL}\NormalTok{,}
          \DataTypeTok{cexRow=}\DecValTok{1}\NormalTok{,}
          \DataTypeTok{cexCol=}\DecValTok{1}\NormalTok{,}
          \DataTypeTok{margins =} \KeywordTok{c}\NormalTok{(}\DecValTok{7}\NormalTok{,}\DecValTok{7}\NormalTok{),}
          \DataTypeTok{trace =} \StringTok{"none"}\NormalTok{,}
          \CommentTok{#lmat = rbind(c(0,0),c(0,1)),}
          \DataTypeTok{lhei =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{),}
          \DataTypeTok{keysize=}\FloatTok{0.75}\NormalTok{, }
          \DataTypeTok{key.par =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{cex=}\FloatTok{0.5}\NormalTok{)}
\NormalTok{          )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{083_Dimensionsreduktion_files/figure-latex/efa-heatmap-1} 

}

\caption{Heatmap einer EFA}\label{fig:efa-heatmap}
\end{figure}

Die Heatmap stellt ähnliche Objekte - hier: Variablen, die hoch auf
einer Hauptkomponenten laden - räumlich nahe (nebeneinander) dar. Im
Ergebnis zeigt die Heatmap eine deutliche Trennung der Items in 5
Faktoren, die interpretierbar sind als \emph{Anerkennung},
\emph{Genuss}, \emph{Sicherheit}, \emph{Bewusstsein} und
\emph{Konformismus}.

\subsection{Berechnung der
Faktor-Scores}\label{berechnung-der-faktor-scores}

Zusätzlich zur Schätzung der Faktorstruktur kann die EFA auch die
latenten Faktorwerte für jede Beobachtung schätzen. Die gängige
Extraktionsmethode ist die Bartlett-Methode, worauf wir hier nicht
weiter eingehen. Kurz gesagt: Jeder Fall (jede Zeile im Datensatz, jede
Person) bekommt einen Wert pro Komponente bzw. Faktor, man spricht von
Faktor-Scores oder Faktorwerten der Beobachtungen.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Werte.ob <-}\StringTok{ }\KeywordTok{factanal}\NormalTok{(Werte, }\DataTypeTok{factors =} \DecValTok{5}\NormalTok{, }\DataTypeTok{scores =} \StringTok{"Bartlett"}\NormalTok{)}
\NormalTok{Werte.skaliertores <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(Werte.ob}\OperatorTok{$}\NormalTok{scores)}
\KeywordTok{names}\NormalTok{(Werte.skaliertores) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Anerkennung"}\NormalTok{, }\StringTok{"Genuss"}\NormalTok{, }\StringTok{"Sicherheit"}\NormalTok{, }\StringTok{"Bewusstsein"}\NormalTok{, }\StringTok{"Konformismus"}\NormalTok{) }
\KeywordTok{head}\NormalTok{(Werte.skaliertores)}
\CommentTok{#>   Anerkennung Genuss Sicherheit Bewusstsein Konformismus}
\CommentTok{#> 1       1.380  0.985      0.563       0.173       -0.106}
\CommentTok{#> 2      -1.404 -0.721      1.597       1.166        0.695}
\CommentTok{#> 3       1.532 -0.657      1.672      -2.003        0.239}
\CommentTok{#> 4      -0.579  2.344     -1.056      -1.120       -0.118}
\CommentTok{#> 5       0.234 -1.652      1.189      -1.701        0.437}
\CommentTok{#> 6      -0.130  0.111     -1.053       0.791       -1.003}
\end{Highlighting}
\end{Shaded}

Wir haben nun anstatt der 15 Variablen 5 Faktoren mit Scores. Die
Dimensionen wurden um ein Drittel reduziert.

\section{Interne Konsistenz der
Skalen}\label{interne-konsistenz-der-skalen}

Das einfachste Maß für die \emph{interne
Konsistenz}\index{interne Konsistenz} ist die
\emph{Split-Half-Reliabilität}\index{Split-Half-Reliabilität}. Die Items
werden in zwei Hälften unterteilt und die resultierenden Scores sollten
in ihren Kenngrößen ähnlich sein. Hohe Korrelationen zwischen den
Hälften deuten auf eine hohe interne Konsistenz hin. Das Problem ist,
dass die Ergebnisse davon abhängen, wie die Items aufgeteilt werden. Ein
üblicher Ansatz zur Lösung dieses Problems besteht darin, den
Koeffizienten \emph{Alpha (Cronbachs Alpha)}\index{Cronbachs Alpha} zu
verwenden.

Der Koeffizient \emph{Alpha} ist der Mittelwert aller möglichen
Split-Half-Koeffizienten, die sich aus verschiedenen Arten der
Aufteilung der Items ergeben. Dieser Koeffizient variiert von 0 bis 1.
Inhaltlich ist Alpha eine Art mittlere Korrelation, die sich ergibt wenn
man alle Items (paarweise) miteinander korreliert: I1-I2, I1-I3,\ldots{}

Zufriedenstellende Reliabilität wird bei einem Alpha-Wert von 0.7
erreicht. Werte unter 0.5 gelten als nicht akzeptabel, Werte ab 0.8 als
gut.

Wir bewerten nun die interne Konsistent der Items Beispielhaft für das
Konstrukt \texttt{Sicherheit} und nehmen zur Demonstration das Item
\texttt{Unabhängigkeit} mit in die Analyse auf.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Werte }\OperatorTok{%>%}
\StringTok{  }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(Unabhaengigkeit, Zuhoeren, Umweltbewusstsein, Interesse) ->}\StringTok{ }\NormalTok{df}

\NormalTok{psych}\OperatorTok{::}\KeywordTok{alpha}\NormalTok{(df, }\DataTypeTok{check.keys =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Bei dem Konstrukt \texttt{Sicherheit} können wir durch Elimination von
\texttt{Unabhängigkeit} das Cronbachs Alpha von 0,63 auf einen fast
akzeptablen Wert von 0,69 erhöhen.

Das Argument \texttt{check.keys=TRUE} gibt uns eine Warnung aus, sollte
die Ladung eines oder mehrerer Items negativ sein. Dies ist hier nicht
der Fall, somit müssen auch keine Items recodiert werden.

\section[Aufgaben]{\texorpdfstring{Aufgaben\footnote{F, F, R, R, R, R, R}}{Aufgaben}}\label{aufgaben-17}

\BeginKnitrBlock{rmdexercises}
Richtig oder Falsch!?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Addiert man Antwortpunkte einer Reihe von Items zu Aggression, so hat
  (sicher) man Aggression gemessen.
\item
  Die Hauptkomponentenanalyse ist eine Methode zur Verringerung der
  Anzahl der Fälle eines Datensatzes.
\item
  Hauptkomponenten sind stets orthogonal zueinander (in einem Datesatz).
\item
  Ein Screeplot ist ein Diagramm, welches die Eigenwerte darstellt.
\item
  Längere Eigenvektoren sind durch größere Eigenwerte gekennzeichnet.
\item
  Bei einer rechtwinkligen Rotation bleiben die Faktoren rechtwinklig.
\item
  Die interne Konsistenz einer Skala ist ein Maß dafür, wie stark die
  Items miteinander korrelieren.
\end{enumerate}
\EndKnitrBlock{rmdexercises}

\section{Befehlsübersicht}\label{befehlsubersicht-10}

Tabelle \ref{tab:befehle-dimred} fasst die R-Funktionen dieses Kapitels
zusammen.

\begin{table}

\caption{\label{tab:befehle-dimred}Befehle des Kapitels 'Dimensionsreduktion'}
\centering
\begin{tabular}[t]{l|l}
\hline
Paket::Funktion & "Beschreibung"\\
\hline
cor & "Berechnet eine Korrelationsmatrix."\\
\hline
read.csv2 & "Liest eine 'deutsche' CSV-Datei ein."\\
\hline
glimpse & "Wirft einen Blick (to glimpse) in den Datensatz."\\
\hline
scale & "führt eine z-Transformation durch"\\
\hline
corrplot::corrplot & "Plottet einen Korrelationsplot."\\
\hline
na.omit & "Schließt Zeilen mit fehlenden Werten von Datensatz aus."\\
\hline
pr.comp & "Berechnet Hauptkomponentenanalyse."\\
\hline
eigen & "Berechnet Eigenwerte."\\
\hline
psych::VSS.scree & "Plottet einen Screeplot."\\
\hline
biplot & "Plottet einen Biplot."\\
\hline
psych::principal & "Berechnet die Statistiken für eine Hauptkomponentenanalyse"\\
\hline
psych::fa.diagram & "Plottet ein Pfaddiagramm für eine Faktorenanalyse"\\
\hline
nFactors::nscree & "Gibt verschiedenen Vorschläge für die Anzahl der 'richtigen' Faktoren"\\
\hline
factanal & "Berechnet eine Faktorenanalyse"\\
\hline
gplots::heatmap.2 & "Plottet ein Heatmap"\\
\hline
factanal & "Berechnet Faktor-Scores"\\
\hline
psych::alpha & "Berechnet Cronbachs Alpha und weitere Statistiken"\\
\hline
\end{tabular}
\end{table}

\chapter{Vertiefung: Grundlagen des
Textmining}\label{vertiefung-grundlagen-des-textmining}

\begin{center}\includegraphics[width=0.3\linewidth]{images/FOM} \end{center}

\begin{center}\includegraphics[width=0.1\linewidth]{images/licence} \end{center}

\BeginKnitrBlock{rmdcaution}
Lernziele:

\begin{itemize}
\tightlist
\item
  Sie kennen zentrale Ziele und Begriffe des Textminings.
\item
  Sie wissen, was ein `tidy text dataframe' ist.
\item
  Sie können Worthäufigkeiten auszählen.
\item
  Sie können Worthäufigkeiten anhand einer Wordcloud visualisieren.
\end{itemize}
\EndKnitrBlock{rmdcaution}

In diesem Kapitel benötigte R-Pakete:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)  }\CommentTok{# Datenjudo}
\KeywordTok{library}\NormalTok{(stringr)  }\CommentTok{# Textverarbeitung}
\KeywordTok{library}\NormalTok{(tidytext)  }\CommentTok{# Textmining}
\KeywordTok{library}\NormalTok{(pdftools)  }\CommentTok{# PDF einlesen}
\KeywordTok{library}\NormalTok{(downloader)  }\CommentTok{# Daten herunterladen}
\KeywordTok{library}\NormalTok{(lsa)  }\CommentTok{# Stopwörter }
\KeywordTok{library}\NormalTok{(SnowballC)  }\CommentTok{# Wörter trunkieren}
\KeywordTok{library}\NormalTok{(wordcloud)  }\CommentTok{# Wordcloud anzeigen}
\end{Highlighting}
\end{Shaded}

Ein großer Teil der zur Verfügung stehenden Daten liegt nicht als braves
Zahlenmaterial vor, sondern in ``unstrukturierter'' Form, z.B. in Form
von Texten. Im Gegensatz zur Analyse von numerischen Daten ist die
Analyse von Texten weniger verbreitet bisher. In Anbetracht der Menge
und der Informationsreichhaltigkeit von Text erscheint die Analyse von
Text als vielversprechend.

In gewisser Weise ist das Textmining ein alternative zu klassischen
qualitativen Verfahren der Sozialforschung. Geht es in der qualitativen
Sozialforschung primär um das Verstehen eines Textes, so kann man für
das Textmining ähnliche Ziele formulieren. Allerdings: Das Textmining
ist wesentlich schwächer und beschränkter in der Tiefe des Verstehens.
Der Computer ist einfach noch (?) wesentlich \emph{dümmer} als ein
Mensch, zumindest in dieser Hinsicht. Allerdings ist er auch wesentlich
\emph{schneller} als ein Mensch, was das Lesen betrifft. Daher bietet
sich das Textmining für das Lesen großer Textmengen an, in denen eine
geringe Informationsdichte vermutet wird. Sozusagen maschinelles Sieben
im großen Stil. Da fällt viel durch die Maschen, aber es werden Tonnen
von Sand bewegt.

In der Regel wird das Textmining als \emph{gemischte} Methode verwendet:
sowohl qualitative als auch qualitative Aspekte spielen eine Rolle.
Damit vermittelt das Textmining auf konstruktive Art und Weise zwischen
den manchmal antagonierenden Schulen der qualitativ-idiographischen und
der quantitativ-nomothetischen Sichtweise auf die Welt. Man könnte es
auch als qualitative Forschung mit moderner Technik bezeichnen - mit den
skizzierten Einschränkungen wohlgemerkt.

\section{Zentrale Begriffe}\label{zentrale-begriffe}

Die computergestützte Analyse von Texten speiste (und speist) sich
reichhaltig aus Quellen der Linguistik; entsprechende Fachtermini finden
Verwendung:

\begin{itemize}
\item
  Ein \emph{Corpus} bezeichnet die Menge der zu analysierenden
  Dokumente; das könnten z.B. alle Reden der Bundeskanzlerin Angela
  Merkel sein oder alle Tweets von ``@realDonaldTrump''.
\item
  Ein \emph{Token} (\emph{Term}) ist ein elementarer Baustein eines
  Texts, die kleinste Analyseeinheit, häufig ein Wort.
\item
  Unter \emph{tidy text} versteht man einen Dataframe, in dem pro Zeile
  nur \emph{ein} Token (z.B. Wort) steht (Silge und Robinson
  \protect\hyperlink{ref-Silge2016}{2016}). Synonym könnte man von einem
  ``langen'' Dataframe sprechen, so wie wir in Kapitel \ref{Datenjudo}
  kennen gelernt haben.
\end{itemize}

\section{Grundlegende Analyse}\label{grundlegende-analyse}

\subsection{Tidy Text Dataframes}\label{tidy-text-dataframes}

Wozu ist es nützlich, einen Text-Dataframe in einen langen Dataframe
umzuwandeln? Der Grund ist, dass immer wenn nur ein Wort (allgemeiner:
Term) pro Zelle steht, dann können wir die Spalte einfach auszählen. Wir
können z.B. \texttt{count} nutzen, um zu zählen, wie häufig ein Wort
vorkommt. Sprich: Sobald wir einen langen (Text-)Dataframe haben, können
wir unsere bekannte Methoden einsetzen.

Basteln wir uns einen \emph{tidy text} Dataframe. Wir gehen dabei von
einem Vektor mit mehreren Text-Elementen aus, das ist ein realistischer
Startpunkt. Unser Text-Vektor\footnote{Nach dem Gedicht ``Jahrgang
  1899'' von Erich Kästner} besteht aus 4 Elementen.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{text <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Wir haben die Frauen zu Bett gebracht,"}\NormalTok{,}
          \StringTok{"als die Männer in Frankreich standen."}\NormalTok{,}
          \StringTok{"Wir hatten uns das viel schöner gedacht."}\NormalTok{,}
          \StringTok{"Wir waren nur Konfirmanden."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Als nächstes machen wir daraus einen Dataframe.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{text_df <-}\StringTok{ }\KeywordTok{data_frame}\NormalTok{(}\DataTypeTok{Zeile =} \DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{,}
                      \DataTypeTok{text =}\NormalTok{ text)}
\end{Highlighting}
\end{Shaded}

Diesen Mini-Datensatz finden Sie auch im Ordner \texttt{data} als
\texttt{Brecht.csv}; nach bekannter Manier können Sie die CSV-Datei
importieren:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{text_df <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"data/Brecht.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|l}
\hline
Zeile & text\\
\hline
1 & Wir haben die Frauen zu Bett gebracht,\\
\hline
2 & als die Männer in Frankreich standen.\\
\hline
3 & Wir hatten uns das viel schöner gedacht.\\
\hline
4 & Wir waren nur Konfirmanden.\\
\hline
\end{tabular}

Übrigens, falls Sie eine beliebige Textdatei einlesen möchten, können
Sie das so tun:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{text <-}\StringTok{ }\KeywordTok{read_lines}\NormalTok{(}\StringTok{"data/Brecht.txt"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Der Befehl \texttt{read\_lines} (aus \texttt{readr}\footnote{Teil der
  Tidyverse-Familie}) liest Zeilen (Zeile für Zeile) aus einer
Textdatei.

Dann ``dehnen'' wir den Dataframe zu einem \emph{tidy text} Dataframe
(s. Abb. \ref{fig:tidytextdf}); das besorgt die Funktion
\texttt{unnest\_tokens}. `unnest' heißt dabei so viel wie
`Entschachteln', also von breit auf lang dehnen. Mit `tokens' sind hier
einfach die Wörter gemeint (es könnten aber auch andere Analyseeinheiten
sein, Sätze zum Beispiel).

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/textmining/tidytext-crop} 

}

\caption{Illustration eines Tidy Text Dataframe}\label{fig:tidytextdf}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{text_df }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{unnest_tokens}\NormalTok{(}\DataTypeTok{output =}\NormalTok{ wort, }\DataTypeTok{input =}\NormalTok{ text) ->}\StringTok{ }\NormalTok{tidytext_df}

\NormalTok{tidytext_df }\OperatorTok{%>%}\StringTok{ }\NormalTok{head}
\CommentTok{#> # A tibble: 6 x 2}
\CommentTok{#>   Zeile   wort}
\CommentTok{#>   <int>  <chr>}
\CommentTok{#> 1     1    wir}
\CommentTok{#> 2     1  haben}
\CommentTok{#> 3     1    die}
\CommentTok{#> 4     1 frauen}
\CommentTok{#> 5     1     zu}
\CommentTok{#> 6     1   bett}
\end{Highlighting}
\end{Shaded}

Der Parameter \texttt{output} sagt, wie neue `saubere' (lange) Spalte
heißen soll; \texttt{input} sagt der Funktion, welche Spalte sie als ihr
Futter (Input) betrachten soll (welche Spalte in tidy text umgewandelt
werden soll).

\begin{quote}
In einem `tidy text Dataframe' steht in jeder Zeile ein Wort (token) und
die Häufigkeit des Worts im Dokument.
\end{quote}

Überprüfen Sie, ob das stimmt: Betrachten Sie den Dataframe
\texttt{tidytext\_df}.

Das \texttt{unnest\_tokens} kann übersetzt werden als ``entschachtele''
oder ``dehne'' die Tokens - so dass in \emph{jeder Zeile} nur noch
\emph{ein Wort} (genauer: Token) steht. Die Syntax ist
\texttt{unnest\_tokens(Ausgabespalte,\ Eingabespalte)}. Nebenbei werden
übrigens alle Buchstaben auf Kleinschreibung getrimmt.

Als nächstes filtern wir die Satzzeichen heraus, da die Wörter für die
Analyse wichtiger (oder zumindest einfacher) sind.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{text_df }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{unnest_tokens}\NormalTok{(wort, text) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\KeywordTok{str_detect}\NormalTok{(wort, }\StringTok{"[a-z]"}\NormalTok{))}
\CommentTok{#> # A tibble: 24 x 2}
\CommentTok{#>    Zeile     wort}
\CommentTok{#>    <int>    <chr>}
\CommentTok{#>  1     1      wir}
\CommentTok{#>  2     1    haben}
\CommentTok{#>  3     1      die}
\CommentTok{#>  4     1   frauen}
\CommentTok{#>  5     1       zu}
\CommentTok{#>  6     1     bett}
\CommentTok{#>  7     1 gebracht}
\CommentTok{#>  8     2      als}
\CommentTok{#>  9     2      die}
\CommentTok{#> 10     2   männer}
\CommentTok{#> # ... with 14 more rows}
\end{Highlighting}
\end{Shaded}

Das \texttt{"{[}a-z{]}"} steht für ``alle Buchstaben von a-z''. In
Pseudo-Code heißt dieser Abschnitt:

\BeginKnitrBlock{rmdpseudocode}
Nehme den Datensatz ``text\_df'' UND DANN\\
dehne die einzelnen Elemente der Spalte ``text'', so dass jedes Element
seine eigene Spalte bekommt.\\
Ach ja: Diese ``gedehnte'' Spalte soll ``Wort'' heißen (weil nur
einzelne Wörter drinnen stehen).\\
Ach ja 2: Diesees ``dehnen'' wandelt automatisch Groß- in
Kleinbuchstaben um. UND DANN\\
filtere die Spalte ``wort'', so dass nur noch Kleinbuchstaben übrig
bleiben. FERTIG.
\EndKnitrBlock{rmdpseudocode}

\subsection{Text-Daten einlesen}\label{text-daten-einlesen}

Nun lesen wir Text-Daten ein; das können beliebige Daten sein\footnote{Ggf.
  benötigen Sie Administrator-Rechte, um Dateien auf Ihre Festplatte zu
  speichern.}. Eine gewisse Reichhaltigkeit ist von Vorteil. Nehmen wir
das Parteiprogramm der Partei AfD\footnote{}. Vor dem Hintergrund des
Erstarkens des Populismus weltweit und der großen Gefahr, die davon
ausgeht - man blicke auf die Geschichte Europas in der ersten Hälfte des
20. Jahrhunderts - \sout{verdient}erfordert der politische Prozess und
speziell Neuentwicklungen darin unsere besondere Beachtung.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{afd_pfad <-}\StringTok{ "data/afd_programm.pdf"}

\NormalTok{afd_raw <-}\StringTok{ }\KeywordTok{pdf_text}\NormalTok{(afd_pfad)}
\end{Highlighting}
\end{Shaded}

Mit \texttt{head(afd\_raw)} können Sie sich den Beginn dieses Textvektor
anzeigen lassen.

Für uns ist \texttt{pdf\_text} sehr praktisch, da diese Funktion Text
aus einer beliebige PDF-Datei in einen Text-Vektor einliest.
\texttt{head(afd\_raw,\ 1)} liest das 1. Element (und nur das erste) aus
\texttt{afd\_raw} aus.

Der Vektor \texttt{afd\_raw} hat 96 Elemente (entsprechend der
Seitenzahl des Dokuments); zählen wir die Gesamtzahl an Wörtern. Dazu
wandeln wir den Vektor in einen tidy text Dataframe um. Auch die
Stopwörter entfernen wir wieder wie gehabt.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{afd_df <-}\StringTok{ }\KeywordTok{data_frame}\NormalTok{(}\DataTypeTok{Zeile =} \DecValTok{1}\OperatorTok{:}\DecValTok{96}\NormalTok{, }
\NormalTok{                     afd_raw)}
\NormalTok{afd_df }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{unnest_tokens}\NormalTok{(}\DataTypeTok{output =}\NormalTok{ token, }\DataTypeTok{input =}\NormalTok{ afd_raw) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{filter}\NormalTok{(}\KeywordTok{str_detect}\NormalTok{(token, }\StringTok{"[a-z]"}\NormalTok{)) ->}\StringTok{ }\NormalTok{afd_df}


\KeywordTok{count}\NormalTok{(afd_df) }
\CommentTok{#> # A tibble: 1 x 1}
\CommentTok{#>       n}
\CommentTok{#>   <int>}
\CommentTok{#> 1 26396}
\end{Highlighting}
\end{Shaded}

Eine substanzielle Menge von Text. Was wohl die häufigsten Wörter sind?

\subsection{Worthäufigkeiten
auszählen}\label{worthaufigkeiten-auszahlen}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{afd_df }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{na.omit}\NormalTok{() }\OperatorTok{%>%}\StringTok{  }\CommentTok{# fehlende Werte löschen}
\StringTok{  }\KeywordTok{count}\NormalTok{(token, }\DataTypeTok{sort =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{#> # A tibble: 7,087 x 2}
\CommentTok{#>    token     n}
\CommentTok{#>    <chr> <int>}
\CommentTok{#>  1   die  1151}
\CommentTok{#>  2   und  1147}
\CommentTok{#>  3   der   870}
\CommentTok{#>  4    zu   435}
\CommentTok{#>  5   für   392}
\CommentTok{#>  6    in   392}
\CommentTok{#>  7   den   271}
\CommentTok{#>  8   von   257}
\CommentTok{#>  9   ist   251}
\CommentTok{#> 10   das   225}
\CommentTok{#> # ... with 7,077 more rows}
\end{Highlighting}
\end{Shaded}

Die häufigsten Wörter sind inhaltsleere Partikel, Präpositionen,
Artikel\ldots{} Solche sogenannten ``Stopwörter'' sollten wir besser
herausfischen, um zu den inhaltlich tragenden Wörtern zu kommen.
Praktischerweise gibt es frei verfügbare Listen von Stopwörtern, z.B. im
Paket \texttt{lsa}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(stopwords_de, }\DataTypeTok{package =} \StringTok{"lsa"}\NormalTok{)}

\NormalTok{stopwords_de <-}\StringTok{ }\KeywordTok{data_frame}\NormalTok{(}\DataTypeTok{word =}\NormalTok{ stopwords_de)}

\NormalTok{stopwords_de <-}\StringTok{ }\NormalTok{stopwords_de }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{rename}\NormalTok{(}\DataTypeTok{token =}\NormalTok{ word)  }
\CommentTok{# Für das Joinen werden gleiche Spaltennamen benötigt}

\NormalTok{afd_df }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{anti_join}\NormalTok{(stopwords_de) ->}\StringTok{ }\NormalTok{afd_df}
\end{Highlighting}
\end{Shaded}

Unser Datensatz hat jetzt viel weniger Zeilen; wir haben also durch
\texttt{anti\_join} Zeilen gelöscht (herausgefiltert). Das ist die
Funktion von \texttt{anti\_join}: Die Zeilen, die in beiden Dataframes
vorkommen, werden herausgefiltert. Es verbleiben also nicht
``Nicht-Stopwörter'' in unserem Dataframe. Damit wird es schon
interessanter, welche Wörter häufig sind.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{afd_df }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{(token, }\DataTypeTok{sort =} \OtherTok{TRUE}\NormalTok{) ->}\StringTok{ }\NormalTok{afd_count}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-19}Die häufigsten Wörter im AfD-Parteiprogramm}
\centering
\begin{tabular}[t]{l|r}
\hline
token & n\\
\hline
deutschland & 190\\
\hline
afd & 171\\
\hline
programm & 80\\
\hline
wollen & 67\\
\hline
bürger & 57\\
\hline
euro & 55\\
\hline
dafür & 53\\
\hline
eu & 53\\
\hline
deutsche & 47\\
\hline
deutschen & 47\\
\hline
\end{tabular}
\end{table}

Ganz interessant; aber es gibt mehrere Varianten des Themas ``deutsch''.
Es ist wohl sinnvoller, diese auf den gemeinsamen Wortstamm
zurückzuführen und diesen nur einmal zu zählen. Dieses Verfahren nennt
man ``stemming'' oder ``trunkieren''.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{afd_df }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{token_stem =} \KeywordTok{wordStem}\NormalTok{(.}\OperatorTok{$}\NormalTok{token, }\DataTypeTok{language =} \StringTok{"german"}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{(token_stem, }\DataTypeTok{sort =} \OtherTok{TRUE}\NormalTok{) ->}\StringTok{ }\NormalTok{afd_count}

\NormalTok{afd_count }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{top_n}\NormalTok{(}\DecValTok{10}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(}\DataTypeTok{caption =} \StringTok{"Die häufigsten Wörter im AfD-Parteiprogramm mit 'stemming'"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-20}Die häufigsten Wörter im AfD-Parteiprogramm mit 'stemming'}
\centering
\begin{tabular}[t]{l|r}
\hline
token\_stem & n\\
\hline
deutschland & 219\\
\hline
afd & 171\\
\hline
deutsch & 119\\
\hline
polit & 88\\
\hline
staat & 85\\
\hline
programm & 81\\
\hline
europa & 80\\
\hline
woll & 67\\
\hline
burg & 66\\
\hline
soll & 63\\
\hline
\end{tabular}
\end{table}

Das ist schon informativer. Dem Befehl \texttt{SnowballC::wordStem}
füttert man einen Vektor an Wörtern ein und gibt die Sprache an (Default
ist Englisch). Denken Sie daran, dass \texttt{.} bei \texttt{dplyr} nur
den Datensatz meint, wie er im letzten Schritt definiert war. Mit
\texttt{.\$token} wählen wir also die Variable \texttt{token} aus
\texttt{afd\_raw} aus.

\subsection{Visualisierung}\label{visualisierung}

Zum Abschluss noch eine Visualisierung mit einer ``Wordcloud'' dazu.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{myword <-}\KeywordTok{na.omit}\NormalTok{(afd_count}\OperatorTok{$}\NormalTok{token_stem)}
\KeywordTok{head}\NormalTok{(myword, }\DecValTok{10}\NormalTok{)}
\NormalTok{myfreq <-}\StringTok{ }\KeywordTok{na.omit}\NormalTok{(afd_count}\OperatorTok{$}\NormalTok{n)}
\KeywordTok{head}\NormalTok{(myfreq, }\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{wordcloud}\NormalTok{(}\DataTypeTok{words =}\NormalTok{ afd_count}\OperatorTok{$}\NormalTok{token_stem, }
          \DataTypeTok{freq =}\NormalTok{ afd_count}\OperatorTok{$}\NormalTok{n, }
          \DataTypeTok{max.words =} \DecValTok{100}\NormalTok{, }
          \DataTypeTok{scale =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,.}\DecValTok{5}\NormalTok{), }
          \DataTypeTok{colors=}\KeywordTok{brewer.pal}\NormalTok{(}\DecValTok{6}\NormalTok{, }\StringTok{"Dark2"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{090_Textmining_files/figure-latex/show-wordcloud-1} \end{center}

Man kann die Anzahl der Wörter, Farben und einige weitere Formatierungen
der Wortwolke beeinflussen\footnote{\url{https://cran.r-project.org/web/packages/wordcloud/index.html}}.

Weniger verspielt ist eine schlichte visualisierte Häufigkeitsauszählung
dieser Art, z.B. mit Balkendiagrammen (gedreht).

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{afd_count }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{top_n}\NormalTok{(}\DecValTok{30}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{reorder}\NormalTok{(token_stem, n), }\DataTypeTok{y =}\NormalTok{ n) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_col}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =} \StringTok{"mit Trunkierung"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_flip}\NormalTok{() ->}\StringTok{ }\NormalTok{p1}

\NormalTok{afd_df }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{(token, }\DataTypeTok{sort =} \OtherTok{TRUE}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{top_n}\NormalTok{(}\DecValTok{30}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{reorder}\NormalTok{(token, n), }\DataTypeTok{y =}\NormalTok{ n) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_col}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =} \StringTok{"ohne Trunkierung"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_flip}\NormalTok{() ->}\StringTok{ }\NormalTok{p2}


\KeywordTok{library}\NormalTok{(gridExtra)}
\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{090_Textmining_files/figure-latex/unnamed-chunk-21-1} \end{center}

Die beiden Diagramme vergleichen die trunkierten Wörter mit den nicht
trunkierten Wörtern. Mit \texttt{reorder} ordnen wir die Spalte
\texttt{token} nach der Spalte \texttt{n}. \texttt{coord\_flip} dreht
die Abbildung um 90°, d.h. die Achsen sind vertauscht.
\texttt{grid.arrange} packt beide Plots in eine Abbildung, welche 2
Spalten (\texttt{ncol}) hat.

\section[Aufgaben]{\texorpdfstring{Aufgaben\footnote{F, R, F, F, R, R,
  F, F}}{Aufgaben}}\label{aufgaben-18}

\BeginKnitrBlock{rmdexercises}
Richtig oder Falsch!?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Unter einem Token versteht man die größte Analyseeinheit in einem
  Text.
\item
  In einem tidytext Dataframe steht jedes Wort in einer (eigenen) Zeile.
\item
  Eine hinreichende Bedingung für einen tidytext Dataframe ist es, dass
  in jeder Zeile ein Wort steht (beziehen Sie sich auf den tidytext
  Dataframe wie in diesem Kapitel erörtert).
\item
  Gibt es `Stop-Wörter' in einem Dataframe, dessen Text analysiert wird,
  so kommt es - per definitionem - zu einem Stop.
\item
  Mit dem Befehl \texttt{unnest\_tokens} kann man einen tidytext
  Dataframe erstellen.
\item
  Balkendiagramme sind sinnvolle und auch häufige Diagrammtypen, um die
  häufigsten Wörter (oder auch Tokens) in einem Corpus darzustellen.
\item
  In einem `tidy text Dataframe' steht in jeder Zeile ein Wort (token)
  \emph{aber nicht} die Häufigkeit des Worts im Dokument.
\item
  Unter `Stemming' versteht man (bei der Textanalyse), die Etymologie
  eines Wort (Herkunft) zu erkunden.
\end{enumerate}
\EndKnitrBlock{rmdexercises}

\section{Befehlsübersicht}\label{befehlsubersicht-11}

Tabelle \ref{tab:befehle-text} fasst die R-Funktionen dieses Kapitels
zusammen.

\begin{table}

\caption{\label{tab:befehle-text}Befehle des Kapitels 'Textmining'}
\centering
\begin{tabular}[t]{l|l}
\hline
Paket..Befehl & Beschreibung\\
\hline
tidytext::unnest\_tokens & Jedes Token (Wort) einer Spalte bekommt eine eigene Zeile in einem Dataframe\\
\hline
stringr::str\_detect & Sucht nach einem String (Text)\\
\hline
downloader:: download & lädt eine Datei aus dem Internet herunter\\
\hline
dplyr::rename & Benennt Spalten um\\
\hline
anti\_join & Führt Dataframes zusammen, so dass nicht matchende Einträge übernommen werden\\
\hline
wordcloud::wordcloud & Erstellt eine Wordcloud\\
\hline
ggplot2::labs & Fügt Titel oder andere Hinweise einem ggplot2-Objekt hinzu\\
\hline
ggplot2::coord\_flip & Dreht die Achsen um 90 Grad\\
\hline
\end{tabular}
\end{table}

\section{Verweise}\label{verweise-6}

\begin{itemize}
\tightlist
\item
  Das Buch \emph{Tidy Text Minig} (Julia und David
  \protect\hyperlink{ref-tidytextminig}{2017}) ist eine hervorragende
  Quelle vertieftem Wissens zum Textmining mit R.
\end{itemize}

\appendix

\renewcommand{\section}{\chapter}




\chapter{Probeklausur}\label{probeklausur}

\begin{center}\includegraphics[width=0.3\linewidth]{images/FOM} \end{center}

\begin{center}\includegraphics[width=0.1\linewidth]{images/licence} \end{center}

Aussagen sind entweder als ``richtig'' oder als ``falsch'' zu
beantworten. Offene Fragen verlangen einen ``Text'' als Antwort.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Bei \texttt{install.packages} spielt der Parameter
  \texttt{dependencies\ =\ TRUE} in der Praxis keine Rolle.
\item
  Dateien mit der Endung \texttt{.R} sind keine Textdateien.
\item
  Der Befehl \texttt{read.csv} kann auch Dateien einlesen, die nicht
  lokal, sondern auf einem Server im Internet gespeichert sind.
\item
  Fehlende Werte werden in R durch \texttt{NA} kodiert.
\item
  Um Variablen einen Wert zuzuweisen, kann man in R den Zuweisungspfeil
  \texttt{\textless{}-} verwenden.
\item
  Die deutsche Version von R verwendet im Standard das Komma als
  Dezimaltrennzeichen.
\item
  Statistisches Modellieren verwendet die Abduktion als zentrale
  Denkfigur.
\item
  Eine Abduktion führt zu sicheren Schlüssen.
\item
  Das CSV-Format ist identisch zum Excel-Format, was sich auch darin
  zeigt, dass Excel CSV-Dateien oft problemlos öffnet.
\item
  Das Arbeitsverzeichnis (engl. \emph{working directory}) ist der Ort,
  in dem R eine Datei, die Sie aufrufen, vermutet - sofern kein anderer
  Pfad angegeben ist.
\item
  In einer Tabelle in Normalform steht in jeder Zeile eine Variable und
  in jeder Spalte eine Beobachtung.
\item
  Die Funktion \texttt{filter} filtert Spalten aus einer Tabelle.
\item
  Die Funktion \texttt{select} lässt Spalten sowohl anhand ihres Namens
  als auch ihrer Nummer (Position in der Tabelle) auswählen.
\item
  Die Funktion \texttt{group\_by} gruppiert eine Tabelle anhand der
  Werte einer diskreten Variablen.
\item
  Die Funktion \texttt{group\_by} akzeptiert nur Faktorvariablen als
  Gruppierungsvariablen.
\item
  Die Funktion \texttt{summarise} darf nur für Funktionen verwendet
  werden, welche genau \emph{einen} Wert zurückliefern.
\item
  Was sind drei häufige Operationen der Datenaufbereitung?
\item
  Um Korrelationen mit R zu berechnen, kann man die Funktion
  \texttt{corrr::correlate} verwenden.
\item
  \texttt{corrr::correlate} liefert stets einen Dataframe zurück.
\item
  Tibbles sind eine spezielle Art von Dataframes.
\item
  Was zeigt uns ``Anscombes Quartett''?
\item
  \texttt{ggplot} unterscheidet drei Bestandteile eines Diagramms:
  Daten, Geome und Transformationen.
\item
  Um eine kontinuierliche Variable zu plotten, wird häufig ein
  Histogramm verwendet.
\item
  Das Geom \texttt{tile} zeigt drei Variablen.
\item
  Geleitetes Modellieren kann unterteilt werden in prädiktives und
  explikatives Modellieren.
\item
  Der Befehl \texttt{scale} verschiebt den Mittelwert einer Verteilung
  auf 0 und skaliert die sd auf 1.
\item
  Mit ``binnen'' im Sinne der Datenanalyse ist gemeint, eine kategoriale
  Variable in eine kontinuierliche zu überführen.
\item
  Die Gleichung \texttt{y\ =\ ax\ +\ b} lässt sich in R darstellen als
  \texttt{y\ \textasciitilde{}\ ax\ +\ b}.
\item
  \(R^2\), auch Bestimmtheitsmaß oder Determinationskoeffizient genannt,
  gibt die Vorhersagegüte im Verhältnis zu einem ``Nullmodell'' an.
\item
  Bei der logistischen Regression gilt: Bei \(\beta_0>0\) ist die
  Wahrscheinlichkeit \emph{kleiner} als 50\% gibt, dass das modellierte
  Ereignis eintritt, wenn alle anderen Prädiktoren Null sind.
\item
  Die logistische Regression sollte \emph{nicht} verwendet werden, wenn
  die abhängige Variable dichotom ist.
\end{enumerate}

32.Die logistische Regression stellt den Zusammenhang zwischen Prädiktor
und Kriterium nicht mit einer Geraden, sondern mit einer ``s-förmigen''
Kurve dar.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{32}
\item
  Bevor die Koeffizienten der logistischen Regression als Odds Ration
  interpretiert werden können, müssen sie ``delogarithmiert'' werden.
\item
  Unter ``delogarithmieren'' versteht man, die Umkehrfunktion der
  e-Funktion auf eine Gleichung anzuwenden.
\item
  Wendet man die ``normale'' Regression an, um eine dichotome Variable
  als Kriterium zu modellieren, so kann man Wahrscheinlichkeiten größer
  als 1 und kleiner als 0 bekommen.
\item
  Eine typische Idee der Clusteranalyse lautet, die Varianz innerhalb
  der Cluster jeweils zu maximieren.
\item
  Bei einer k-means-Clusteranalyse darf man nicht die Anzahl der Cluster
  vorab festlegen; vielmehr ermittelt der Algorithmus die richtige
  Anzahl der Cluster.
\item
  Für die Wahl der ``richtigen'' Anzahl der Cluster kann das
  ``Ellbogen-Kriterium'' als Entscheidungsgrundlage herangezogen werden.
\item
  Ein ``Screeplot'' stellt die Varianz innerhalb der Cluster als
  Funktion der Anzahl der Cluster dar (im Rahmen der Clusteranalyse).
\item
  Die euklidische Distanz zwischen zwei Objekten in der Ebene lässt sich
  mit dem Satz des Pythagoras berechnen.
\end{enumerate}

\section{Lösungen}\label{losungen-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Falsch
\item
  Falsch
\item
  Richtig
\item
  Richtig
\item
  Richtig
\item
  Falsch
\item
  Richtig
\item
  Falsch
\item
  Falsch
\item
  Richtig
\item
  Falsch
\item
  Falsch
\item
  Falsch
\item
  Richtig
\item
  Richtig
\item
  Falsch
\item
  Richtig
\item
  Auf fehlende Werte prüfen, Fälle mit fehlenden Werte löschen, Fehlende
  Werte ggf. ersetzen,Nach Fehlern suche, Ausreiser identifizieren,
  Hochkorrelierte Variablen finden, z-Standardisieren, Quasi-Konstante
  finden, Auf Normalverteilung prüfen, Werte umkodieren und ``binnen''.
\item
  Richtig
\item
  Richtig
\item
  Richtig
\item
  Es geht hier um vier Datensätze mit zwei Variablen (Spalten; X und Y).
  Offenbar sind die Datensätze praktisch identisch: Alle X haben den
  gleichen Mittelwert und die gleiche Varianz; dasselbe gilt für die Y.
  Die Korrelation zwischen X und Y ist in allen vier Datensätzen gleich.
  Allerdings erzählt eine Visualisierung der vier Datensätze eine ganz
  andere Geschichte.
\item
  Falsch
\item
  Richtig
\item
  Richtig
\item
  Falsch
\item
  Richtig
\item
  Falsch
\item
  Richtig
\item
  Richtig
\item
  Falsch
\item
  Falsch
\item
  Richtig
\item
  Richtig
\item
  Falsch. Richtig wäre: Die Umkehrfunktion des Logarithmus, also die
  e-Funktion, auf eine Gleichung anzuwenden.
\item
  Richtig
\item
  Falsch
\item
  Falsch. Richtig wäre: Man gibt die Anzahl der Cluster vor. Dann
  vergleicht man die Varianz within der verschiedenen Lösungen.
\item
  Richtig
\item
  Richtig
\item
  Richtig
\end{enumerate}

\chapter{Hinweise}\label{hinweise-2}

\section{Icons}\label{icons}

R spricht zu Ihnen; sie versucht es jedenfalls, mit einigen Items
(Icon-Pond \protect\hyperlink{ref-edu_icons}{2016}).

\emph{R-Pseudo-Syntax}: R ist (momentan) die führende Umgebung für
Datenanalyse. Entsprechend zentral ist R in diesem Kurs. Zugebenermaßen
braucht es etwas Zeit, bis man ein paar Brocken ``Errisch'' spricht. Um
den Einstieg zu erleichtern, ist Errisch auf Deutsch übersetzt an
einigen Stellen, wo mir dies besonders hilfreich erschien. Diese Stellen
sind mit diesem Symbol
\includegraphics[width=0.05000\textwidth]{images/icons/pseudocode.png}
gekennzeichnet (für R-Pseudo-Syntax).

\emph{Achtung, Falle}: Schwierige oder fehlerträchtige Stellen sind mit
diesem Symbol
\includegraphics[width=0.05000\textwidth]{images/icons/caution.png}
markiert.

\emph{Übungsaufgaben}: Das Skript beinhaltet in jedem Kapitel
Übungsaufgaben oder/und Testfragen. Auf diese wird mit diesem Icon
\includegraphics[width=0.05000\textwidth]{images/icons/exercises.png}
verwiesen oder die Übungen sind in einem Abschnitt mit einsichtigem
Titel zu finden.

\section{Voraussetzungen}\label{voraussetzungen}

Dieses Skript hat einige \emph{Voraussetzungen}, was das Vorwissen der
Leser angeht; folgende Themengebiete werden vorausgesetzt:

\begin{itemize}
\tightlist
\item
  Deskriptive Statistik
\item
  Grundlagen der Inferenzstatistik
\item
  Grundlagen der Regressionsanalyse
\item
  Skalenniveaus
\item
  Grundlagen von R
\end{itemize}

\section{Zitationen}\label{zitationen}

Kunstwerke (Bilder) sind genau wie Standard-Literatur im Text zitiert.
Alle Werke (auch Daten und Software) finden sich im
Literaturverzeichnis.

\section{Lizenz}\label{lizenz}

Dieses Skript ist publiziert unter
\href{https://creativecommons.org/licenses/by-nc-sa/3.0/de/}{CC-BY-NC-SA
3.0 DE}.

\begin{center}\includegraphics[width=0.1\linewidth]{images/licence} \end{center}

\section{Autoren}\label{autoren}

\emph{Sebastian Sauer} schrieb den Hauptteil dieses Skripts.
\emph{Oliver Gansser} schrieb das Kapitel zur Dimensionsreduktion.
\emph{Karsten Lübke} schrieb den Großteil des Kapitels zur Regression
und zur Clusteranalyse sowie Teile des Kapitels `Rahmen'. \emph{Matthias
Gehrke} schrieb den Großteil des Kapitels zur logistischen Regression.

\section{Danke}\label{danke}

Norman Markgraf hat umfangreich Fehler gejagt und Verbesserungen
\sout{angemahnt} vorgenommen. Der Austausch mit den ifes-Mitgliedern
hielt die Flamme am Köcheln. Eine Reihe weiterer Kollegen standen mit
Rat und Tat zur Seite. Die Hochschulleitung sowie das Dekanat für
Wirtschaftspsychologie hat dieses Projekt unterstützt. Die Abteilung
Medienentwicklung der FOM hat bei Fragen rund um die Veröffentlichung
geholfen. Last but not least: Viele Studierenden wiesen auf
Inkonsistenzen, Fehler und Unklarheiten hin. Ihnen allen: Vielen Dank!

\section{Zitation dieses Skripts}\label{zitation-dieses-skripts}

Bitte zitieren Sie das Skript so:

Sauer, S. (2017). \emph{Praxis der Datenanalyse}. Skript zum Modul im
MSc.-Studiengang ``Wirtschaftspsychologie \& Consulting'' an der FOM.
FOM Nürnberg. DOI: 10.5281/zenodo.580649.

Mehr Infos zum DOI hier:
\url{https://zenodo.org/badge/latestdoi/81811975}

Ein Bib-File um dieses Skript zu zitieren finden Sie hier:
\url{https://raw.githubusercontent.com/sebastiansauer/Praxis_der_Datenanalyse/master/Praxis_der_Datenanalyse.bib}.

\section{Kontakt}\label{kontakt}

Wenn Sie einen Fehler oder Verbesserungshinweise berichten möchten,
können Sie unter
\url{https://github.com/sebastiansauer/Praxis_der_Datenanalyse/issues}
einen ``Issue'' einreichen (Button ``New Issue''). Alternativ können Sie
Sebastian Sauer und die anderen Autoren über den Online Campus der FOM
kontaktieren (eine Nachricht schreiben). Sebastian Sauer können Sie via
Twitter folgen (\url{https://twitter.com/sauer_sebastian}) oder seinen
Blog lesen (\url{https://sebastiansauer.github.io}).

\section{Technische Details}\label{technische-details}

Dieses Skript wurde mit dem Paket \texttt{bookdown} (Xie
\protect\hyperlink{ref-xie2015}{2015}) erstellt, welches wiederum stark
auf den Paketen \texttt{knitr} (Xie
\protect\hyperlink{ref-xie2015}{2015}) und \texttt{rmarkdown} (Allaire
u.~a.
\protect\hyperlink{ref-rmarkdown}{2016}\protect\hyperlink{ref-rmarkdown}{a})
beruht. Diese Pakete stellen verblüffende Funktionalität zur Verfügung
als freie Software (frei wie in Bier und frei wie in Freiheit).

Informationen zu den verwendeten Paketen etc. (\texttt{sessionInfo()})
finden Sie hier:
\url{https://raw.githubusercontent.com/sebastiansauer/Praxis_der_Datenanalyse/master/includes/sessionInfo_PraDa.html}.

\section{Sonstiges}\label{sonstiges}

Aus Gründen der Lesbarkeit wird das männliche Generikum verwendet,
welches Frauen und Männer in gleichen Maßen ansprechen soll.

\chapter{Literaturverzeichnis}\label{literaturverzeichnis}

\hypertarget{refs}{}
\hypertarget{ref-rmarkdown}{}
Allaire, JJ, Joe Cheng, Yihui Xie, Jonathan McPherson, Winston Chang,
Jeff Allen, Hadley Wickham, Aron Atkins, und Rob Hyndman. 2016a.
\emph{rmarkdown: Dynamic Documents for R}.
\url{https://CRAN.R-project.org/package=rmarkdown}.

\hypertarget{ref-R-rmarkdown}{}
---------. 2016b. \emph{rmarkdown: Dynamic Documents for R}.
\url{https://CRAN.R-project.org/package=rmarkdown}.

\hypertarget{ref-R-gridExtra}{}
Auguie, Baptiste. 2016. \emph{gridExtra: Miscellaneous Functions for
„Grid`` Graphics}. \url{https://CRAN.R-project.org/package=gridExtra}.

\hypertarget{ref-R-BaylorEdPsych}{}
Beaujean, A. Alexander. 2012. \emph{BaylorEdPsych: R Package for Baylor
University Educational Psychology Quantitative Courses}.
\url{https://CRAN.R-project.org/package=BaylorEdPsych}.

\hypertarget{ref-R-quanteda}{}
Benoit, Kenneth, und Paul Nulty. 2016. \emph{quanteda: Quantitative
Analysis of Textual Data}.
\url{https://CRAN.R-project.org/package=quanteda}.

\hypertarget{ref-R-SnowballC}{}
Bouchet-Valat, Milan. 2014. \emph{SnowballC: Snowball stemmers based on
the C libstemmer UTF-8 library}.
\url{https://CRAN.R-project.org/package=SnowballC}.

\hypertarget{ref-breaking}{}
Briggs, William M. 2008a. \emph{Breaking the Law of Averages: Real-Life
Probability and Statistics in Plain English}. Lulu.com.

\hypertarget{ref-Breaking}{}
---------. 2008b. \emph{Breaking the Law of Averages: Real-Life
Probability and Statistics in Plain English}. Lulu.com.
\url{https://www.amazon.com/Breaking-Law-Averages-Probability-Statistics/dp/0557019907?SubscriptionId=0JYN1NVW651KCA56C102\&tag=techkie-20\&linkCode=xm2\&camp=2025\&creative=165953\&creativeASIN=0557019907}.

\hypertarget{ref-uncertainty}{}
---------. 2016. \emph{Uncertainty: The Soul of Modeling, Probability \&
Statistics}. Springer.

\hypertarget{ref-bryant1995practical}{}
Bryant, PG, und MA Smith. 1995. „Practical Data Analysis: Case Studies
in Business Statistics, Homewood, IL: Richard D``. Irwin Publishing.

\hypertarget{ref-R-downloader}{}
Chang, Winston. 2015. \emph{downloader: Download Files over HTTP and
HTTPS}. \url{https://CRAN.R-project.org/package=downloader}.

\hypertarget{ref-Chapman2015}{}
Chapman, Chris, und Elea McDonnell Feit. 2015. \emph{R for Marketing
Research and Analytics}. New York City: Springer.
doi:\href{https://doi.org/10.1007/978-3-319-14436-8}{10.1007/978-3-319-14436-8}.

\hypertarget{ref-Cleveland}{}
Cleveland, William S. 1993. \emph{Visualizing Data}. Hobart Press.

\hypertarget{ref-clopper1934use}{}
Clopper, Charles J, und Egon S Pearson. 1934. „The use of confidence or
fiducial limits illustrated in the case of the binomial``.
\emph{Biometrika} 26 (4). JSTOR: 404--13.

\hypertarget{ref-cobb2007introductory}{}
Cobb, George W. 2007. „The introductory statistics course: a Ptolemaic
curriculum?`` \emph{Technology Innovations in Statistics Education} 1
(1).

\hypertarget{ref-Cohen1992}{}
Cohen, J. 1992. „A power primer``. \emph{Psychological Bulletin} 112
(1): 155--59.

\hypertarget{ref-cohen_statistical_1988}{}
Cohen, Jacob. 1988. \emph{Statistical Power Analysis for the Behavioral
Sciences}. Routledge. \url{http://dx.doi.org/10.4324/9780203771587}.

\hypertarget{ref-cortez2009modeling}{}
Cortez, Paulo, António Cerdeira, Fernando Almeida, Telmo Matos, und José
Reis. 2009. „Modeling wine preferences by data mining from
physicochemical properties``. \emph{Decision Support Systems} 47 (4).
Elsevier: 547--53.

\hypertarget{ref-R-ggdendro}{}
de Vries, Andrie, und Brian D. Ripley. 2016. \emph{ggdendro: Create
Dendrograms and Tree Diagrams Using 'ggplot2'}.
\url{https://CRAN.R-project.org/package=ggdendro}.

\hypertarget{ref-introstats}{}
Diez, David M, Christopher D Barr, und Mine Cetinkaya-Rundel. 2014.
\emph{Introductory Statistics with Randomization and Simulation}. North
Charleston, South Carolina: CreateSpace Independent Publishing Platform.

\hypertarget{ref-eid2010statistik}{}
Eid, Michael, Mario Gollwitzer, und Manfred Schmitt. 2010.
\emph{Statistik und Forschungsmethoden}. Göttingen: Hogrefe.

\hypertarget{ref-etz2016become}{}
Etz, Alexander, Quentin Frederik Gronau, Fabian Dablander, Peter
Edelsbrunner, und Beth Baribault. 2016. „How to become a Bayesian in
eight easy steps: An annotated reading list``. PsyArXiv.

\hypertarget{ref-fair1978theory}{}
Fair, Ray C. 1978. „A theory of extramarital affairs``. \emph{Journal of
Political Economy} 86 (1). The University of Chicago Press: 45--61.

\hypertarget{ref-R-tm}{}
Feinerer, Ingo, und Kurt Hornik. 2015. \emph{tm: Text Mining Package}.
\url{https://CRAN.R-project.org/package=tm}.

\hypertarget{ref-R-wordcloud}{}
Fellows, Ian. 2014. \emph{wordcloud: Word Clouds}.
\url{https://CRAN.R-project.org/package=wordcloud}.

\hypertarget{ref-fjalnes_orthogonale_2014}{}
Fjalnes. 2014. „Orthogonale Faktorrotation``.
\url{https://de.wikipedia.org/wiki/Rotationsverfahren_(Statistik)\#/media/File:Orthogonale_faktorrotation.svg}.

\hypertarget{ref-R-car}{}
Fox, John, und Sanford Weisberg. 2016. \emph{car: Companion to Applied
Regression}. \url{https://CRAN.R-project.org/package=car}.

\hypertarget{ref-Gansser_2017}{}
Gansser, Oliver. 2017. „Data for Principal Component Analysis and Common
Factor Analysis``. Open Science Framework. \url{osf.io/zg89r}.

\hypertarget{ref-gigerenzer1980}{}
Gigerenzer, Gerd. 1980. \emph{Messung und Modellbildung in der
Psychologie (Uni-Taschenbucher. Psychologie, Padagogik, Soziologie,
Psychiatrie) (German Edition)}. E. Reinhardt.

\hypertarget{ref-Gigerenzer2004}{}
---------. 2004. „Mindless statistics``. \emph{The Journal of
Socio-Economics} 33 (5). Elsevier BV: 587--606.
doi:\href{https://doi.org/10.1016/j.socec.2004.09.033}{10.1016/j.socec.2004.09.033}.

\hypertarget{ref-god_i_2016}{}
God. 2016. „I don't care about you. Please share this with friends.``
Twitter Tweet. \emph{TheTweetOfGod}.
\url{https://twitter.com/TheTweetOfGod/status/688035049187454976}.

\hypertarget{ref-grolemund2014cognitive}{}
Grolemund, Garrett, und Hadley Wickham. 2014. „A cognitive
interpretation of data analysis``. \emph{International Statistical
Review} 82 (2). Wiley Online Library: 184--204.

\hypertarget{ref-R-arules}{}
Hahsler, Michael, Christian Buchta, Bettina Gruen, und Kurt Hornik.
2016. \emph{arules: Mining Association Rules and Frequent Itemsets}.
\url{https://CRAN.R-project.org/package=arules}.

\hypertarget{ref-R-arulesViz}{}
Hahsler, Michael, und Sudheer Chelluboina. 2016. \emph{arulesViz:
Visualizing Association Rules and Frequent Itemsets}.
\url{https://CRAN.R-project.org/package=arulesViz}.

\hypertarget{ref-hamermesh2005beauty}{}
Hamermesh, Daniel S, und Amy Parker. 2005. „Beauty in the classroom:
Instructors' pulchritude and putative pedagogical productivity``.
\emph{Economics of Education Review} 24 (4). Elsevier: 369--76.

\hypertarget{ref-hardin2015data}{}
Hardin, Johanna, Roger Hoerl, Nicholas J Horton, Deborah Nolan, Ben
Baumer, Olaf Hall-Holt, Paul Murrell, u.~a. 2015. „Data science in
statistics curricula: Preparing students to 'Think with Data'``.
\emph{The American Statistician} 69 (4). Taylor \& Francis: 343--53.

\hypertarget{ref-Hatzinger}{}
Hatzinger, Reinhold, Kurt Hornik, und Herbert Nagel. 2014. \emph{R-
Einfuehrung durch angewandte Statistik}. Pearson Studium.

\hypertarget{ref-Head2015}{}
Head, Megan L., Luke Holman, Rob Lanfear, Andrew T. Kahn, und Michael D.
Jennions. 2015. „The Extent and Consequences of P-Hacking in Science``.
\emph{PLOS Biology} 13 (3). Public Library of Science (PLoS): e1002106.
doi:\href{https://doi.org/10.1371/journal.pbio.1002106}{10.1371/journal.pbio.1002106}.

\hypertarget{ref-R-titanic}{}
Hendricks, Paul. 2015. \emph{titanic: Titanic Passenger Survival Data
Set}. \url{https://CRAN.R-project.org/package=titanic}.

\hypertarget{ref-hoekstra2014robust}{}
Hoekstra, Rink, Richard D Morey, Jeffrey N Rouder, und Eric-Jan
Wagenmakers. 2014. „Robust misinterpretation of confidence intervals``.
\emph{Psychonomic bulletin \& review} 21 (5). Springer: 1157--64.

\hypertarget{ref-hyndman2014forecasting}{}
Hyndman, R.J., und G. Athanasopoulos. 2014. \emph{Forecasting:
principles and practice:} OTexts.
\url{https://books.google.de/books?id=gDuRBAAAQBAJ}.

\hypertarget{ref-edu_icons}{}
Icon-Pond. 2016. „Education. 35 Icons.`` Flaticon.
\url{http://www.flaticon.com/authors/popcorns-arts}.

\hypertarget{ref-tm}{}
Ingo Feinerer, Kurt Hornik, und David Meyer. 2008. „Text Mining
Infrastructure in R``. \emph{Journal of Statistical Software} 25 (5):
1--54. \url{http://www.jstatsoft.org/v25/i05/}.

\hypertarget{ref-R-corrr}{}
Jackson, Simon. 2016. \emph{corrr: Correlations in R}.
\url{https://CRAN.R-project.org/package=corrr}.

\hypertarget{ref-R-ISLR}{}
James, Gareth, Daniela Witten, Trevor Hastie, und Rob Tibshirani. 2013a.
\emph{ISLR: Data for An Introduction to Statistical Learning with
Applications in R}. \url{https://CRAN.R-project.org/package=ISLR}.

\hypertarget{ref-introstatlearning}{}
James, Gareth, Daniela Witten, Trevor Hastie, und Robert Tibshirani.
2013b. \emph{An introduction to statistical learning}. Bd. 6. Springer.

\hypertarget{ref-james2013introduction}{}
---------. 2013c. \emph{An introduction to statistical learning}. Bd. 6.
Springer.

\hypertarget{ref-tidytextminig}{}
Julia, PhD Silge, und PhD Robinson David. 2017. \emph{Text Mining with
R: A tidy approach}. O'Reilly Media.

\hypertarget{ref-Kerby2014}{}
Kerby, Dave S. 2014. „The Simple Difference Formula: An Approach to
Teaching Nonparametric Correlation``. \emph{Comprehensive Psychology} 3:
11.IT.3.1.
doi:\href{https://doi.org/10.2466/11.IT.3.1}{10.2466/11.IT.3.1}.

\hypertarget{ref-kim2015okcupid}{}
Kim, Albert Y, und Adriana Escobedo-Land. 2015. „OkCupid Data for
Introductory Statistics and Data Science Courses``. \emph{Journal of
Statistics Education} 23 (2). Citeseer: n2.

\hypertarget{ref-R-okcupiddata}{}
Kim, Albert Y., und Adriana Escobedo-Land. 2016. \emph{okcupiddata:
OkCupid Profile Data for Introductory Statistics and Data Science
Courses}. \url{https://CRAN.R-project.org/package=okcupiddata}.

\hypertarget{ref-kraemer2011wir}{}
Krämer, W. 2011. \emph{Wie wir uns von falschen Theorien täuschen
lassen}. Berlin University Press.
\url{https://books.google.de/books?id=HWUKaAEACAAJ}.

\hypertarget{ref-kruschke2010bayesian}{}
Kruschke, John K. 2010. „Bayesian data analysis``. \emph{Wiley
Interdisciplinary Reviews: Cognitive Science} 1 (5). Burlington, MA:
Academic Press: 658--76.

\hypertarget{ref-kuhn2013applied}{}
Kuhn, Max, und Kjell Johnson. 2013. \emph{Applied predictive modeling}.
Bd. 26. Springer.

\hypertarget{ref-R-scatterplot3d}{}
Ligges, Uwe, Martin Maechler, und Sarah Schnackenberg. 2017.
\emph{scatterplot3d: 3D Scatter Plot}.
\url{https://CRAN.R-project.org/package=scatterplot3d}.

\hypertarget{ref-lubke2014angewandte}{}
Lübke, Karsten, und Martin Vogt. 2014. \emph{Angewandte
Wirtschaftsstatistik: Daten und Zufall}. Berlin: Springer.

\hypertarget{ref-m7_savinellis_2004}{}
M7. 2004. „Savinelli's Italian smoking pipe``.
\url{https://commons.wikimedia.org/wiki/File:Pipa_savinelli.jpg}.

\hypertarget{ref-matejka2017same}{}
Matejka, Justin, und George Fitzmaurice. 2017. „Same stats, different
graphs: Generating datasets with varied appearance and identical
statistics through simulated annealing``. In \emph{Proceedings of the
2017 CHI Conference on Human Factors in Computing Systems}, 1290--4.
ACM.

\hypertarget{ref-Micceri1989}{}
Micceri, Theodore. 1989. „The unicorn, the normal curve, and other
improbable creatures.`` \emph{Psychological Bulletin} 105 (1): 156--66.
doi:\href{https://doi.org/10.1037/0033-2909.105.1.156}{10.1037/0033-2909.105.1.156}.

\hypertarget{ref-Michell2000}{}
Michell, Joel. 2000. „Normal Science, Pathological Science and
Psychometrics``. \emph{Theory \& Psychology} 10 (5): 639--67.

\hypertarget{ref-R-rpart.plot}{}
Milborrow, Stephen. 2017. \emph{rpart.plot: Plot 'rpart' Models: An
Enhanced Version of 'plot.rpart'}.
\url{https://CRAN.R-project.org/package=rpart.plot}.

\hypertarget{ref-moore1990uncertainty}{}
Moore, David S. 1990. „Uncertainty``. \emph{On the shoulders of giants:
New approaches to numeracy}. ERIC, 95--137.

\hypertarget{ref-R-tokenizers}{}
Mullen, Lincoln. 2016. \emph{tokenizers: A Consistent Interface to
Tokenize Natural Language Text}.
\url{https://CRAN.R-project.org/package=tokenizers}.

\hypertarget{ref-R-RColorBrewer}{}
Neuwirth, Erich. 2014. \emph{RColorBrewer: ColorBrewer Palettes}.
\url{https://CRAN.R-project.org/package=RColorBrewer}.

\hypertarget{ref-Neyman1933}{}
Neyman, J., und E. S. Pearson. 1933. „On the Problem of the Most
Efficient Tests of Statistical Hypotheses``. \emph{Philosophical
Transactions of the Royal Society A: Mathematical, Physical and
Engineering Sciences} 231 (694-706): 289--337.
doi:\href{https://doi.org/10.1098/rsta.1933.0009}{10.1098/rsta.1933.0009}.

\hypertarget{ref-neyman1935problem}{}
Neyman, Jerzy. 1935. „On the problem of confidence intervals``.
\emph{The annals of mathematical statistics} 6 (3). JSTOR: 111--16.

\hypertarget{ref-neyman1992problem}{}
Neyman, Jerzy, und Egon S Pearson. 1992. „On the problem of the most
efficient tests of statistical hypotheses``. In \emph{Breakthroughs in
statistics}, 73--108. New York City: Springer.

\hypertarget{ref-R-pdftools}{}
Ooms, Jeroen. 2016. \emph{pdftools: Text Extraction and Rendering of PDF
Documents}. \url{https://CRAN.R-project.org/package=pdftools}.

\hypertarget{ref-peirce1955abduction}{}
Peirce, Charles S. 1955. „Abduction and induction``. \emph{Philosophical
writings of Peirce} 11. New York.

\hypertarget{ref-peng2015art}{}
Peng, Roger D, und Elizabeth Matsui. 2015. „The Art of Data Science``.
\emph{A Guide for Anyone Who Works with Data. Skybrude Consulting} 200:
162.

\hypertarget{ref-welchertest}{}
Prel, Jean-Baptist du, Bernd Roehrig, Gerhard Hommel, und Maria
Blettner. 2010. \emph{Deutsches Aerzteblatt Online}, Mai. Deutscher
Aerzte-Verlag.
doi:\href{https://doi.org/10.3238/arztebl.2010.0343}{10.3238/arztebl.2010.0343}.

\hypertarget{ref-ligges}{}
\emph{Programmieren mit R}. 2009. Springer Berlin Heidelberg.
doi:\href{https://doi.org/10.1007/978-3-540-79998-6}{10.1007/978-3-540-79998-6}.

\hypertarget{ref-R-nFactors}{}
Raiche, Gilles, und David Magis. 2011. \emph{nFactors: Parallel Analysis
and Non Graphical Solutions to the Cattell Scree Test}.
\url{https://CRAN.R-project.org/package=nFactors}.

\hypertarget{ref-R-wesanderson}{}
Ram, Karthik, und Hadley Wickham. 2015. \emph{wesanderson: A Wes
Anderson Palette Generator}.
\url{https://CRAN.R-project.org/package=wesanderson}.

\hypertarget{ref-R-compute.es}{}
Re, AC Del. 2014. \emph{compute.es: Compute Effect Sizes}.
\url{https://CRAN.R-project.org/package=compute.es}.

\hypertarget{ref-remquahey2010}{}
Remus, R., U. Quasthoff, und G. Heyer. 2010. „SentiWS -- a Publicly
Available German-language Resource for Sentiment Analysis``. In
\emph{Proceedings of the 7th International Language Resources and
Evaluation (LREC'10)}, 1168--71.

\hypertarget{ref-R-MASS}{}
Ripley, Brian. 2016. \emph{MASS: Support Functions and Datasets for
Venables and Ripley's MASS}.
\url{https://CRAN.R-project.org/package=MASS}.

\hypertarget{ref-nycflights13}{}
RITA, Bureau of transportation statistics. 2013. „nycflights13``.
\url{http://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236}.

\hypertarget{ref-R-gutenbergr}{}
Robinson, David. 2016. \emph{gutenbergr: Download and Process Public
Domain Works from Project Gutenberg}.
\url{https://cran.rstudio.com/package=gutenbergr}.

\hypertarget{ref-R-broom}{}
Robinson, David, Matthieu Gomez, Boris Demeshev, Dieter Menne, Benjamin
Nutter, Luke Johnston, Ben Bolker, Francois Briatte, und Hadley Wickham.
2015. \emph{broom: Convert Statistical Analysis Objects into Tidy Data
Frames}. \url{https://CRAN.R-project.org/package=broom}.

\hypertarget{ref-R-tidytext}{}
Robinson, David, und Julia Silge. 2016. \emph{tidytext: Text Mining
using 'dplyr', 'ggplot2', and Other Tidy Tools}.
\url{https://CRAN.R-project.org/package=tidytext}.

\hypertarget{ref-sep-statistics}{}
Romeijn, Jan-Willem. 2016. „Philosophy of Statistics``. In \emph{The
Stanford Encyclopedia of Philosophy}, herausgegeben von Edward N. Zalta,
Winter 2016.
\url{http://plato.stanford.edu/archives/win2016/entries/statistics/}.

\hypertarget{ref-ruckerinfinity}{}
Rucker, Rudy. 2004. \emph{Infinity and the Mind}. Princeton: Princeton
University Press. \url{https://books.google.de/books?id=MD0UAwAAQBAJ}.

\hypertarget{ref-Sauer_2016}{}
Sauer, Sebastian. 2016. „Extraversion Dataset``. Open Science Framework.
doi:\href{https://doi.org/10.17605/OSF.IO/4KGZH}{10.17605/OSF.IO/4KGZH}.

\hypertarget{ref-Sauer_2017}{}
---------. 2017a. „Dataset 'predictors of performance in stats test'``.
Open Science Framework.
doi:\href{https://doi.org/10.17605/OSF.IO/SJHUY}{10.17605/OSF.IO/SJHUY}.

\hypertarget{ref-Sauer_2017a}{}
---------. 2017b. „Dataset 'Height and shoe size'``. Open Science
Framework.
doi:\href{https://doi.org/10.17605/OSF.IO/JA9DW}{10.17605/OSF.IO/JA9DW}.

\hypertarget{ref-sauer_wolff}{}
Sauer, Sebastian, und Alexander Wolff. 2016. „The effect of a status
symbol on success in online dating: an experimental study (data
paper)``. \emph{The Winnower}, August.
doi:\href{https://doi.org/10.15200/winn.147241.13309}{10.15200/winn.147241.13309}.

\hypertarget{ref-sauer2010gray}{}
Sauer, Sebastian, Harald Walach, und Niko Kohls. 2010. „Gray's
Behavioural Inhibition System as a mediator of mindfulness towards
well-being``. \emph{Personality and Individual Differences} 50 (4).
Pergamon: 506--51.
doi:\href{https://doi.org/10.1016/j.paid.2010.11.019}{10.1016/j.paid.2010.11.019}.

\hypertarget{ref-R-GGally}{}
Schloerke, Barret, Jason Crowley, Di Cook, Francois Briatte, Moritz
Marbach, Edwin Thoen, Amos Elberg, und Joseph Larmarange. 2016.
\emph{GGally: Extension to 'ggplot2'}.
\url{https://CRAN.R-project.org/package=GGally}.

\hypertarget{ref-Schmidt2007}{}
Schmidt, Peter, Sebastian Bamberg, Eldad Davidov, Johannes Herrmann, und
Shalom H. Schwartz. 2007. „Die Messung von Werten mit dem Portraits
Value Questionnaire``. \emph{Zeitschrift für Sozialpsychologie} 38 (4).
Hogrefe Publishing Group: 261--75.
doi:\href{https://doi.org/10.1024/0044-3514.38.4.261}{10.1024/0044-3514.38.4.261}.

\hypertarget{ref-Shmueli2010}{}
Shmueli, Galit. 2010. „To Explain or to Predict?`` \emph{Statistical
Science} 25 (3): 289--310.
doi:\href{https://doi.org/10.1214/10-STS330}{10.1214/10-STS330}.

\hypertarget{ref-R-janeaustenr}{}
Silge, Julia. 2016. \emph{janeaustenr: Jane Austen's Complete Novels}.
\url{https://CRAN.R-project.org/package=janeaustenr}.

\hypertarget{ref-tidytext-archive}{}
Silge, Julia, David Robinson, und Jim Hester. 2016. „tidytext: Text
mining using dplyr, ggplot2, and other tidy tools``.
doi:\href{https://doi.org/10.5281/zenodo.56714}{10.5281/zenodo.56714}.

\hypertarget{ref-Silge2016}{}
Silge, Julia, und David Robinson. 2016. „tidytext: Text Mining and
Analysis Using Tidy Data Principles in R``. \emph{The Journal of Open
Source Software} 1 (3). The Open Journal.
doi:\href{https://doi.org/10.21105/joss.00037}{10.21105/joss.00037}.

\hypertarget{ref-spurzem_vw_2017}{}
Spurzem, Lothar. 2017. „VW 1303 von Wiking in 1:87``.
\url{https://de.wikipedia.org/wiki/Modellautomobil\#/media/File:Wiking-Modell_VW_1303_(um_1975).JPG}.

\hypertarget{ref-suppes1962basic}{}
Suppes, Patrick, und Joseph L Zinnes. 1962. \emph{Basic measurement
theory}. Institute for mathematical studies in the social sciences.

\hypertarget{ref-oxford}{}
\emph{The Oxford Dictionary of Statistical Terms}. 2006. Oxford
University Press.

\hypertarget{ref-R-rpart}{}
Therneau, Terry, Beth Atkinson, und Brian Ripley. 2015. \emph{rpart:
Recursive Partitioning and Regression Trees}.
\url{https://CRAN.R-project.org/package=rpart}.

\hypertarget{ref-1930824149}{}
Tufte, Edward R. 1990. \emph{Envisioning Information}. Graphics Press.

\hypertarget{ref-1930824130}{}
---------. 2001. \emph{The Visual Display of Quantitative Information}.
Graphics Press.

\hypertarget{ref-1930824165}{}
---------. 2006. \emph{Beautiful Evidence}. Graphics Press.

\hypertarget{ref-unrau1}{}
Unrau, Sebastian. 2017. „No Title``.
\url{https://unsplash.com/photos/CoD2Q92UaEg}.

\hypertarget{ref-R-SDMTools}{}
VanDerWal, Jeremy, Lorena Falconi, Stephanie Januchowski, Luke Shoo, und
Collin Storlie. 2014. \emph{SDMTools: Species Distribution Modelling
Tools: Tools for processing data associated with species distribution
modelling exercises}. \url{https://CRAN.R-project.org/package=SDMTools}.

\hypertarget{ref-Wagenmakers2007}{}
Wagenmakers, Eric-Jan. 2007. „A practical solution to the pervasive
problems ofp values``. \emph{Psychonomic Bulletin \& Review} 14 (5).
Springer Nature: 779--804.
doi:\href{https://doi.org/10.3758/bf03194105}{10.3758/bf03194105}.

\hypertarget{ref-R-gplots}{}
Warnes, Gregory R., Ben Bolker, Lodewijk Bonebakker, Robert Gentleman,
Wolfgang Huber Andy Liaw, Thomas Lumley, Martin Maechler, u.~a. 2016.
\emph{gplots: Various R Programming Tools for Plotting Data}.
\url{https://CRAN.R-project.org/package=gplots}.

\hypertarget{ref-R-corrplot}{}
Wei, Taiyun, und Viliam Simko. 2016. \emph{corrplot: Visualization of a
Correlation Matrix}. \url{https://CRAN.R-project.org/package=corrplot}.

\hypertarget{ref-Wicherts2016}{}
Wicherts, Jelte M., Coosje L. S. Veldkamp, Hilde E. M. Augusteijn,
Marjan Bakker, Robbie C. M. van Aert, und Marcel A. L. M. van Assen.
2016. „Degrees of Freedom in Planning, Running, Analyzing, and Reporting
Psychological Studies: A Checklist to Avoid p-Hacking``. \emph{Frontiers
in Psychology} 7 (November). Frontiers Media SA.
doi:\href{https://doi.org/10.3389/fpsyg.2016.01832}{10.3389/fpsyg.2016.01832}.

\hypertarget{ref-R-ggplot2}{}
Wickham, Hadley. 2009. \emph{ggplot2: Elegant Graphics for Data
Analysis}. Springer-Verlag New York. \url{http://ggplot2.org}.

\hypertarget{ref-wickham2014advanced}{}
---------. 2014a. \emph{Advanced R}. Boca Raton, Florida: CRC Press.

\hypertarget{ref-tidydata}{}
---------. 2014b. „Tidy Data``. \emph{Journal of Statistical Software}
59 (1): 1--23.
doi:\href{https://doi.org/10.18637/jss.v059.i10}{10.18637/jss.v059.i10}.

\hypertarget{ref-R-reshape2}{}
---------. 2016a. \emph{reshape2: Flexibly Reshape Data: A Reboot of the
Reshape Package}. \url{https://CRAN.R-project.org/package=reshape2}.

\hypertarget{ref-R-tidyr}{}
---------. 2016b. \emph{tidyr: Easily Tidy Data with `spread()` and
`gather()` Functions}. \url{https://CRAN.R-project.org/package=tidyr}.

\hypertarget{ref-R-nycflights13}{}
---------. 2017a. \emph{nycflights13: Flights that Departed NYC in
2013}. \url{https://CRAN.R-project.org/package=nycflights13}.

\hypertarget{ref-R-stringr}{}
---------. 2017b. \emph{stringr: Simple, Consistent Wrappers for Common
String Operations}. \url{https://CRAN.R-project.org/package=stringr}.

\hypertarget{ref-R-tidyverse}{}
---------. 2017c. \emph{tidyverse: Easily Install and Load 'Tidyverse'
Packages}. \url{https://CRAN.R-project.org/package=tidyverse}.

\hypertarget{ref-readr}{}
Wickham, Hadley, Jim Hester, und Romain Francois. 2016a. \emph{readr:
Read Tabular Data}. \url{https://CRAN.R-project.org/package=readr}.

\hypertarget{ref-R-readr}{}
---------. 2016b. \emph{readr: Read Tabular Data}.
\url{https://CRAN.R-project.org/package=readr}.

\hypertarget{ref-R-dplyr}{}
Wickham, Hadley, und Romain Francois. 2016. \emph{dplyr: A Grammar of
Data Manipulation}. \url{https://CRAN.R-project.org/package=dplyr}.

\hypertarget{ref-r4ds}{}
Wickham, Hadley, und Garrett Grolemund. 2016. \emph{R for Data Science:
Visualize, Model, Transform, Tidy, and Import Data}. O'Reilly Media.

\hypertarget{ref-wiki:groesse}{}
Wikipedia. 2017. „Körpergröße --- Wikipedia, Die freie Enzyklopädie``.
\url{https://de.wikipedia.org/w/index.php?title=K\%C3\%B6rpergr\%C3\%B6\%C3\%9Fe\&oldid=165047921}.

\hypertarget{ref-wild1999statistical}{}
Wild, Chris J, und Maxine Pfannkuch. 1999. „Statistical thinking in
empirical enquiry``. \emph{International Statistical Review} 67 (3).
Wiley Online Library: 223--48.

\hypertarget{ref-R-lsa}{}
Wild, Fridolin. 2015. \emph{lsa: Latent Semantic Analysis}.
\url{https://CRAN.R-project.org/package=lsa}.

\hypertarget{ref-wilkinson2006grammar}{}
Wilkinson, Leland. 2006. \emph{The grammar of graphics}. Springer
Science \& Business Media.

\hypertarget{ref-xie2015}{}
Xie, Yihui. 2015. \emph{Dynamic Documents with R and knitr}. 2nd Aufl.
Boca Raton, Florida: Chapman; Hall/CRC. \url{http://yihui.name/knitr/}.

\hypertarget{ref-R-knitr}{}
---------. 2016. \emph{knitr: A General-Purpose Package for Dynamic
Report Generation in R}. \url{https://CRAN.R-project.org/package=knitr}.

\hypertarget{ref-zumel2014practical}{}
Zumel, Nina, John Mount, und Jim Porzak. 2014. \emph{Practical data
science with R}. Manning.

\printindex

\backmatter


\end{document}
